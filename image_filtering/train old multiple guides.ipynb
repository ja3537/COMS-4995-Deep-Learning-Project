{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9bc56dc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\adams\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\numpy\\_distributor_init.py:30: UserWarning: loaded more than 1 DLL from .libs:\n",
      "c:\\users\\adams\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\numpy\\.libs\\libopenblas.EL2C6PLE4ZYW3ECEVIV3OXXGRN2NRFM2.gfortran-win_amd64.dll\n",
      "c:\\users\\adams\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\numpy\\.libs\\libopenblas.GK7GX5KEQ4F6UYO3P26ULGBQYHGQO7J4.gfortran-win_amd64.dll\n",
      "  warnings.warn(\"loaded more than 1 DLL from .libs:\"\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function, division\n",
    "import os\n",
    "import torchvision\n",
    "import torch\n",
    "from skimage import io, transform\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "import torch.nn as nn\n",
    "from math import log10, pi\n",
    "import time\n",
    "\n",
    "import utils\n",
    "from datasetsMultiple import DatasetMultiple\n",
    "from vgg import Vgg16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "addc7164",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MFFNet(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MFFNet, self).__init__()\n",
    "        \n",
    "        self.conv1 = ConvLayer(3, 32, kernel_size=9, stride=1)\n",
    "        self.in1 = torch.nn.InstanceNorm2d(32, affine=True)\n",
    "        self.conv2 = ConvLayer(32, 64, kernel_size=3, stride=2)\n",
    "        self.in2 = torch.nn.InstanceNorm2d(64, affine=True)\n",
    "        self.conv3 = ConvLayer(64, 128, kernel_size=3, stride=2)\n",
    "        self.in3 = torch.nn.InstanceNorm2d(128, affine=True)\n",
    "        # Residual layers\n",
    "        self.res1 = ResidualBlock(128)\n",
    "        self.res2 = ResidualBlock(128)\n",
    "        self.res3 = ResidualBlock(128)\n",
    "        self.res4 = ResidualBlock(128)\n",
    "        self.res5 = ResidualBlock(128)\n",
    "        self.res6 = ResidualBlock(128)\n",
    "        self.res7 = ResidualBlock(128)\n",
    "        self.res8 = ResidualBlock(128)\n",
    "        self.res9 = ResidualBlock(128)\n",
    "        self.res10 = ResidualBlock(128)\n",
    "        self.res11 = ResidualBlock(128)\n",
    "        self.res12 = ResidualBlock(128)\n",
    "        self.res13 = ResidualBlock(128)\n",
    "        self.res14 = ResidualBlock(128)\n",
    "        self.res15 = ResidualBlock(128)\n",
    "        self.res16 = ResidualBlock(128)\n",
    "        \n",
    "        self.deconv1 = UpsampleConvLayer(128*2, 64, kernel_size=3, stride=1, upsample=2)\n",
    "        self.in4 = torch.nn.InstanceNorm2d(64, affine=True)\n",
    "        self.deconv2 = UpsampleConvLayer(64*2, 32, kernel_size=3, stride=1, upsample=2)\n",
    "        self.in5 = torch.nn.InstanceNorm2d(32, affine=True)\n",
    "        self.deconv3 = ConvLayer(32*2, 3, kernel_size=9, stride=1)\n",
    "\n",
    "        self.relu = torch.nn.ReLU()\n",
    "    \n",
    "    def forward(self, X):\n",
    "        o1 = self.relu(self.conv1(X))\n",
    "        o2 = self.relu(self.conv2(o1))\n",
    "        o3 = self.relu(self.conv3(o2))\n",
    "\n",
    "        y = self.res1(o3)\n",
    "        y = self.res2(y)\n",
    "        y = self.res3(y)\n",
    "        y = self.res4(y)\n",
    "        y = self.res5(y)\n",
    "        y = self.res6(y)\n",
    "        y = self.res7(y)\n",
    "        y = self.res8(y)\n",
    "        y = self.res9(y)\n",
    "        y = self.res10(y)\n",
    "        y = self.res11(y)\n",
    "        y = self.res12(y)\n",
    "        y = self.res13(y)\n",
    "        y = self.res14(y)\n",
    "        y = self.res15(y)\n",
    "        y = self.res16(y)\n",
    "        \n",
    "        in1 = torch.cat( (y, o3), 1 )\n",
    "        y = self.relu(self.deconv1(in1))\n",
    "        in2 = torch.cat( (y, o2), 1 )\n",
    "        y = self.relu(self.deconv2(in2))\n",
    "        in3 = torch.cat( (y, o1), 1 )\n",
    "        y = self.deconv3(in3)\n",
    "        \n",
    "        return y\n",
    "\n",
    "class ConvLayer(torch.nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride):\n",
    "        super(ConvLayer, self).__init__()\n",
    "        reflection_padding = kernel_size // 2\n",
    "        self.reflection_pad = torch.nn.ReflectionPad2d(reflection_padding)\n",
    "        self.conv2d = torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.reflection_pad(x)\n",
    "        out = self.conv2d(out)\n",
    "        return out\n",
    "\n",
    "class ResidualBlock(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, channels):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.conv1 = ConvLayer(channels, channels, kernel_size=3, stride=1)\n",
    "        self.in1 = torch.nn.InstanceNorm2d(channels, affine=True)\n",
    "        self.conv2 = ConvLayer(channels, channels, kernel_size=3, stride=1)\n",
    "        self.in2 = torch.nn.InstanceNorm2d(channels, affine=True)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        out = self.relu(self.conv1(x))\n",
    "        out = self.conv2(out)\n",
    "        out = out + residual\n",
    "        return out\n",
    "\n",
    "\n",
    "class UpsampleConvLayer(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride, upsample=None):\n",
    "        super(UpsampleConvLayer, self).__init__()\n",
    "        self.upsample = upsample\n",
    "        reflection_padding = kernel_size // 2\n",
    "        self.reflection_pad = torch.nn.ReflectionPad2d(reflection_padding)\n",
    "        self.conv2d = torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x_in = x\n",
    "        if self.upsample:\n",
    "            x_in = torch.nn.functional.interpolate(x_in, mode='nearest', scale_factor=self.upsample)\n",
    "        out = self.reflection_pad(x_in)\n",
    "        out = self.conv2d(out)\n",
    "        return out\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2f01aae2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/600], Training Loss: 793.1590, vgg Loss: 66963.1406, L2 Loss: 123.5277, time: 45.6465\n",
      "Epoch [2/600], Training Loss: 169.8015, vgg Loss: 14634.4258, L2 Loss: 23.4573, time: 45.9326\n",
      "Epoch [3/600], Training Loss: 104.7223, vgg Loss: 8924.1572, L2 Loss: 15.4807, time: 46.6451\n",
      "Epoch [4/600], Training Loss: 85.5938, vgg Loss: 7259.2783, L2 Loss: 13.0010, time: 47.6569\n",
      "Epoch [5/600], Training Loss: 73.8493, vgg Loss: 6196.3823, L2 Loss: 11.8855, time: 48.6194\n",
      "Epoch [6/600], Training Loss: 61.4747, vgg Loss: 5118.2417, L2 Loss: 10.2923, time: 47.6413\n",
      "Epoch [7/600], Training Loss: 56.7786, vgg Loss: 4707.0068, L2 Loss: 9.7086, time: 47.7698\n",
      "Epoch [8/600], Training Loss: 48.4824, vgg Loss: 4063.5142, L2 Loss: 7.8473, time: 56.5676\n",
      "Epoch [9/600], Training Loss: 42.1711, vgg Loss: 3557.1360, L2 Loss: 6.5997, time: 60.6503\n",
      "Epoch [10/600], Training Loss: 42.1526, vgg Loss: 3509.6594, L2 Loss: 7.0560, time: 46.6941\n",
      "Epoch [11/600], Training Loss: 76.9300, vgg Loss: 5867.4961, L2 Loss: 18.2551, time: 47.3894\n",
      "Epoch [12/600], Training Loss: 38.5022, vgg Loss: 3257.7319, L2 Loss: 5.9248, time: 47.2605\n",
      "Epoch [13/600], Training Loss: 34.8982, vgg Loss: 2947.5186, L2 Loss: 5.4230, time: 47.3248\n",
      "Epoch [14/600], Training Loss: 34.4202, vgg Loss: 2891.6714, L2 Loss: 5.5034, time: 47.1061\n",
      "Epoch [15/600], Training Loss: 32.4790, vgg Loss: 2702.2256, L2 Loss: 5.4568, time: 47.7071\n",
      "Epoch [16/600], Training Loss: 33.4195, vgg Loss: 2753.5015, L2 Loss: 5.8845, time: 47.2728\n",
      "Epoch [17/600], Training Loss: 76.1994, vgg Loss: 5812.5161, L2 Loss: 18.0743, time: 47.5595\n",
      "Epoch [18/600], Training Loss: 29.8153, vgg Loss: 2512.0930, L2 Loss: 4.6944, time: 47.5921\n",
      "Epoch [19/600], Training Loss: 27.6605, vgg Loss: 2342.4692, L2 Loss: 4.2358, time: 47.7407\n",
      "Epoch [20/600], Training Loss: 26.3811, vgg Loss: 2234.6230, L2 Loss: 4.0348, time: 48.2485\n",
      "Epoch [21/600], Training Loss: 25.5695, vgg Loss: 2164.1890, L2 Loss: 3.9276, time: 47.7865\n",
      "Epoch [22/600], Training Loss: 29.1182, vgg Loss: 2367.7668, L2 Loss: 5.4406, time: 48.2986\n",
      "Epoch [23/600], Training Loss: 24.0846, vgg Loss: 2035.9409, L2 Loss: 3.7252, time: 47.4735\n",
      "Epoch [24/600], Training Loss: 24.8297, vgg Loss: 2082.2932, L2 Loss: 4.0068, time: 47.6561\n",
      "Epoch [25/600], Training Loss: 25.4985, vgg Loss: 2112.0623, L2 Loss: 4.3779, time: 47.4612\n",
      "Epoch [26/600], Training Loss: 24.1820, vgg Loss: 2008.7582, L2 Loss: 4.0944, time: 47.4364\n",
      "Epoch [27/600], Training Loss: 25.4267, vgg Loss: 2076.7068, L2 Loss: 4.6596, time: 47.9092\n",
      "Epoch [28/600], Training Loss: 21.4441, vgg Loss: 1803.7542, L2 Loss: 3.4065, time: 47.4003\n",
      "Epoch [29/600], Training Loss: 22.4190, vgg Loss: 1860.1365, L2 Loss: 3.8176, time: 47.4966\n",
      "Epoch [30/600], Training Loss: 23.1285, vgg Loss: 1842.4946, L2 Loss: 4.7035, time: 48.0097\n",
      "Epoch [31/600], Training Loss: 25.5295, vgg Loss: 2055.6201, L2 Loss: 4.9733, time: 47.7530\n",
      "Epoch [32/600], Training Loss: 20.7039, vgg Loss: 1730.9126, L2 Loss: 3.3948, time: 47.7113\n",
      "Epoch [33/600], Training Loss: 28.3916, vgg Loss: 2259.5813, L2 Loss: 5.7958, time: 47.5017\n",
      "Epoch [34/600], Training Loss: 27.1245, vgg Loss: 2123.4768, L2 Loss: 5.8897, time: 47.4102\n",
      "Epoch [35/600], Training Loss: 17.9727, vgg Loss: 1531.7917, L2 Loss: 2.6548, time: 47.7750\n",
      "Epoch [36/600], Training Loss: 20.6943, vgg Loss: 1709.4630, L2 Loss: 3.5997, time: 47.7768\n",
      "Epoch [37/600], Training Loss: 18.6167, vgg Loss: 1572.8168, L2 Loss: 2.8885, time: 48.1663\n",
      "Epoch [38/600], Training Loss: 19.3419, vgg Loss: 1598.4531, L2 Loss: 3.3574, time: 47.8756\n",
      "Epoch [39/600], Training Loss: 19.3244, vgg Loss: 1564.4545, L2 Loss: 3.6798, time: 48.3500\n",
      "Epoch [40/600], Training Loss: 16.6064, vgg Loss: 1406.8596, L2 Loss: 2.5378, time: 47.6470\n",
      "Epoch [41/600], Training Loss: 17.7767, vgg Loss: 1483.5863, L2 Loss: 2.9408, time: 47.3793\n",
      "Epoch [42/600], Training Loss: 16.4698, vgg Loss: 1379.8663, L2 Loss: 2.6711, time: 48.9356\n",
      "Epoch [43/600], Training Loss: 17.8582, vgg Loss: 1478.5068, L2 Loss: 3.0731, time: 47.7714\n",
      "Epoch [44/600], Training Loss: 16.7178, vgg Loss: 1391.8168, L2 Loss: 2.7996, time: 47.7063\n",
      "Epoch [45/600], Training Loss: 18.7442, vgg Loss: 1543.6079, L2 Loss: 3.3082, time: 47.8166\n",
      "Epoch [46/600], Training Loss: 15.4401, vgg Loss: 1303.7773, L2 Loss: 2.4023, time: 47.9742\n",
      "Epoch [47/600], Training Loss: 15.3427, vgg Loss: 1282.6476, L2 Loss: 2.5162, time: 47.5537\n",
      "Epoch [48/600], Training Loss: 22.9072, vgg Loss: 1788.0199, L2 Loss: 5.0270, time: 47.8281\n",
      "Epoch [49/600], Training Loss: 14.9295, vgg Loss: 1261.9904, L2 Loss: 2.3096, time: 48.6896\n",
      "Epoch [50/600], Training Loss: 14.6992, vgg Loss: 1222.7262, L2 Loss: 2.4719, time: 47.7584\n",
      "Epoch [51/600], Training Loss: 16.6604, vgg Loss: 1369.9528, L2 Loss: 2.9609, time: 47.8946\n",
      "Epoch [52/600], Training Loss: 14.6311, vgg Loss: 1215.6312, L2 Loss: 2.4748, time: 47.8399\n",
      "Epoch [53/600], Training Loss: 13.7783, vgg Loss: 1165.2886, L2 Loss: 2.1254, time: 47.8317\n",
      "Epoch [54/600], Training Loss: 14.5622, vgg Loss: 1198.3684, L2 Loss: 2.5785, time: 47.8565\n",
      "Epoch [55/600], Training Loss: 13.7245, vgg Loss: 1146.9634, L2 Loss: 2.2548, time: 48.3473\n",
      "Epoch [56/600], Training Loss: 14.9743, vgg Loss: 1244.6613, L2 Loss: 2.5277, time: 48.1396\n",
      "Epoch [57/600], Training Loss: 13.9842, vgg Loss: 1172.8033, L2 Loss: 2.2562, time: 48.1435\n",
      "Epoch [58/600], Training Loss: 12.6657, vgg Loss: 1068.8461, L2 Loss: 1.9773, time: 49.2132\n",
      "Epoch [59/600], Training Loss: 12.5598, vgg Loss: 1060.1005, L2 Loss: 1.9588, time: 47.8255\n",
      "Epoch [60/600], Training Loss: 13.8118, vgg Loss: 1153.1465, L2 Loss: 2.2803, time: 48.0069\n",
      "Epoch [61/600], Training Loss: 12.9095, vgg Loss: 1052.9187, L2 Loss: 2.3803, time: 47.8946\n",
      "Epoch [62/600], Training Loss: 16.5894, vgg Loss: 1337.8090, L2 Loss: 3.2113, time: 48.0605\n",
      "Epoch [63/600], Training Loss: 14.5306, vgg Loss: 1214.0284, L2 Loss: 2.3903, time: 47.6613\n",
      "Epoch [64/600], Training Loss: 11.0960, vgg Loss: 951.5749, L2 Loss: 1.5802, time: 48.0241\n",
      "Epoch [65/600], Training Loss: 10.1246, vgg Loss: 869.0193, L2 Loss: 1.4344, time: 47.6322\n",
      "Epoch [66/600], Training Loss: 12.2215, vgg Loss: 1018.0664, L2 Loss: 2.0409, time: 47.7376\n",
      "Epoch [67/600], Training Loss: 11.2103, vgg Loss: 941.4011, L2 Loss: 1.7963, time: 47.8844\n",
      "Epoch [68/600], Training Loss: 12.2178, vgg Loss: 1018.3408, L2 Loss: 2.0344, time: 47.9373\n",
      "Epoch [69/600], Training Loss: 11.3173, vgg Loss: 943.3158, L2 Loss: 1.8841, time: 47.7900\n",
      "Epoch [70/600], Training Loss: 11.3045, vgg Loss: 952.0333, L2 Loss: 1.7841, time: 48.2770\n",
      "Epoch [71/600], Training Loss: 10.5953, vgg Loss: 886.8348, L2 Loss: 1.7269, time: 48.1163\n",
      "Epoch [72/600], Training Loss: 10.1876, vgg Loss: 867.8641, L2 Loss: 1.5090, time: 48.7705\n",
      "Epoch [73/600], Training Loss: 9.7069, vgg Loss: 832.3747, L2 Loss: 1.3832, time: 47.7996\n",
      "Epoch [74/600], Training Loss: 11.8240, vgg Loss: 977.5421, L2 Loss: 2.0485, time: 48.0883\n",
      "Epoch [75/600], Training Loss: 466.9512, vgg Loss: 34317.9258, L2 Loss: 123.7720, time: 48.1117\n",
      "Epoch [76/600], Training Loss: 24.8597, vgg Loss: 2183.4414, L2 Loss: 3.0253, time: 47.9513\n",
      "Epoch [77/600], Training Loss: 20.9780, vgg Loss: 1853.3546, L2 Loss: 2.4444, time: 49.3618\n",
      "Epoch [78/600], Training Loss: 18.5843, vgg Loss: 1647.7500, L2 Loss: 2.1068, time: 47.7604\n",
      "Epoch [79/600], Training Loss: 17.3297, vgg Loss: 1537.1411, L2 Loss: 1.9582, time: 47.9042\n",
      "Epoch [80/600], Training Loss: 15.9100, vgg Loss: 1416.8184, L2 Loss: 1.7418, time: 48.1778\n",
      "Epoch [81/600], Training Loss: 15.0353, vgg Loss: 1340.0720, L2 Loss: 1.6345, time: 48.0439\n",
      "Epoch [82/600], Training Loss: 14.5453, vgg Loss: 1293.7959, L2 Loss: 1.6073, time: 47.4534\n",
      "Epoch [83/600], Training Loss: 13.3396, vgg Loss: 1189.0948, L2 Loss: 1.4487, time: 47.6605\n",
      "Epoch [84/600], Training Loss: 12.8250, vgg Loss: 1144.0117, L2 Loss: 1.3849, time: 47.8375\n",
      "Epoch [85/600], Training Loss: 12.0920, vgg Loss: 1080.8151, L2 Loss: 1.2839, time: 47.7666\n",
      "Epoch [86/600], Training Loss: 12.0834, vgg Loss: 1081.7445, L2 Loss: 1.2659, time: 47.8692\n",
      "Epoch [87/600], Training Loss: 11.2869, vgg Loss: 1010.7636, L2 Loss: 1.1792, time: 47.8671\n",
      "Epoch [88/600], Training Loss: 10.1747, vgg Loss: 907.1104, L2 Loss: 1.1036, time: 47.8885\n",
      "Epoch [89/600], Training Loss: 9.3757, vgg Loss: 837.9742, L2 Loss: 0.9960, time: 48.0811\n",
      "Epoch [90/600], Training Loss: 8.7523, vgg Loss: 783.6520, L2 Loss: 0.9157, time: 47.9590\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [91/600], Training Loss: 8.2050, vgg Loss: 741.4686, L2 Loss: 0.7903, time: 47.8773\n",
      "Epoch [92/600], Training Loss: 7.8288, vgg Loss: 711.1799, L2 Loss: 0.7170, time: 47.8338\n",
      "Epoch [93/600], Training Loss: 7.3973, vgg Loss: 676.1514, L2 Loss: 0.6358, time: 48.2539\n",
      "Epoch [94/600], Training Loss: 7.4018, vgg Loss: 673.0460, L2 Loss: 0.6713, time: 47.8021\n",
      "Epoch [95/600], Training Loss: 6.9356, vgg Loss: 633.1016, L2 Loss: 0.6046, time: 47.7496\n",
      "Epoch [96/600], Training Loss: 6.8190, vgg Loss: 624.4165, L2 Loss: 0.5748, time: 48.7054\n",
      "Epoch [97/600], Training Loss: 6.6821, vgg Loss: 611.2844, L2 Loss: 0.5693, time: 47.8222\n",
      "Epoch [98/600], Training Loss: 6.4609, vgg Loss: 593.4432, L2 Loss: 0.5264, time: 49.6294\n",
      "Epoch [99/600], Training Loss: 6.4360, vgg Loss: 587.1775, L2 Loss: 0.5642, time: 48.1080\n",
      "Epoch [100/600], Training Loss: 6.4755, vgg Loss: 580.3886, L2 Loss: 0.6716, time: 47.8432\n",
      "Epoch [101/600], Training Loss: 6.0634, vgg Loss: 557.4421, L2 Loss: 0.4890, time: 47.9391\n",
      "Epoch [102/600], Training Loss: 5.9233, vgg Loss: 542.8938, L2 Loss: 0.4944, time: 48.1722\n",
      "Epoch [103/600], Training Loss: 5.7875, vgg Loss: 529.0380, L2 Loss: 0.4971, time: 47.9552\n",
      "Epoch [104/600], Training Loss: 5.6237, vgg Loss: 514.0593, L2 Loss: 0.4831, time: 48.1609\n",
      "Epoch [105/600], Training Loss: 5.6188, vgg Loss: 511.5204, L2 Loss: 0.5036, time: 48.0238\n",
      "Epoch [106/600], Training Loss: 5.6691, vgg Loss: 509.9869, L2 Loss: 0.5692, time: 47.9462\n",
      "Epoch [107/600], Training Loss: 5.3405, vgg Loss: 486.1949, L2 Loss: 0.4786, time: 48.0388\n",
      "Epoch [108/600], Training Loss: 5.2809, vgg Loss: 477.0858, L2 Loss: 0.5100, time: 49.0189\n",
      "Epoch [109/600], Training Loss: 5.1242, vgg Loss: 467.9362, L2 Loss: 0.4449, time: 48.0098\n",
      "Epoch [110/600], Training Loss: 5.0672, vgg Loss: 463.0386, L2 Loss: 0.4368, time: 48.1000\n",
      "Epoch [111/600], Training Loss: 4.9882, vgg Loss: 453.0799, L2 Loss: 0.4574, time: 47.9445\n",
      "Epoch [112/600], Training Loss: 5.1610, vgg Loss: 470.0805, L2 Loss: 0.4602, time: 48.3012\n",
      "Epoch [113/600], Training Loss: 4.9198, vgg Loss: 446.0532, L2 Loss: 0.4593, time: 47.9234\n",
      "Epoch [114/600], Training Loss: 4.7549, vgg Loss: 430.1869, L2 Loss: 0.4530, time: 47.9699\n",
      "Epoch [115/600], Training Loss: 4.8427, vgg Loss: 436.8174, L2 Loss: 0.4745, time: 48.7023\n",
      "Epoch [116/600], Training Loss: 4.5970, vgg Loss: 419.0734, L2 Loss: 0.4063, time: 48.1322\n",
      "Epoch [117/600], Training Loss: 4.5824, vgg Loss: 414.5302, L2 Loss: 0.4371, time: 48.2190\n",
      "Epoch [118/600], Training Loss: 4.4950, vgg Loss: 410.4591, L2 Loss: 0.3904, time: 48.2950\n",
      "Epoch [119/600], Training Loss: 4.6077, vgg Loss: 417.2525, L2 Loss: 0.4352, time: 48.2884\n",
      "Epoch [120/600], Training Loss: 4.4064, vgg Loss: 401.3087, L2 Loss: 0.3933, time: 48.0884\n",
      "Epoch [121/600], Training Loss: 4.7031, vgg Loss: 420.9302, L2 Loss: 0.4938, time: 48.8701\n",
      "Epoch [122/600], Training Loss: 4.3660, vgg Loss: 393.7918, L2 Loss: 0.4281, time: 48.2597\n",
      "Epoch [123/600], Training Loss: 4.7040, vgg Loss: 422.9662, L2 Loss: 0.4744, time: 47.8714\n",
      "Epoch [124/600], Training Loss: 4.6131, vgg Loss: 416.2997, L2 Loss: 0.4501, time: 48.4729\n",
      "Epoch [125/600], Training Loss: 4.3994, vgg Loss: 396.2728, L2 Loss: 0.4367, time: 48.1847\n",
      "Epoch [126/600], Training Loss: 4.2670, vgg Loss: 387.4203, L2 Loss: 0.3928, time: 48.1171\n",
      "Epoch [127/600], Training Loss: 4.0259, vgg Loss: 366.4951, L2 Loss: 0.3609, time: 48.2778\n",
      "Epoch [128/600], Training Loss: 4.2710, vgg Loss: 388.3882, L2 Loss: 0.3871, time: 48.6962\n",
      "Epoch [129/600], Training Loss: 4.4260, vgg Loss: 396.8232, L2 Loss: 0.4578, time: 47.9962\n",
      "Epoch [130/600], Training Loss: 3.9336, vgg Loss: 359.4750, L2 Loss: 0.3388, time: 48.2217\n",
      "Epoch [131/600], Training Loss: 4.1493, vgg Loss: 374.5760, L2 Loss: 0.4035, time: 48.0690\n",
      "Epoch [132/600], Training Loss: 4.0300, vgg Loss: 365.2207, L2 Loss: 0.3778, time: 47.9469\n",
      "Epoch [133/600], Training Loss: 3.9500, vgg Loss: 357.8193, L2 Loss: 0.3719, time: 47.9862\n",
      "Epoch [134/600], Training Loss: 3.9462, vgg Loss: 356.8906, L2 Loss: 0.3773, time: 48.9935\n",
      "Epoch [135/600], Training Loss: 4.0070, vgg Loss: 360.7039, L2 Loss: 0.3999, time: 48.3271\n",
      "Epoch [136/600], Training Loss: 3.5970, vgg Loss: 324.9528, L2 Loss: 0.3475, time: 48.4610\n",
      "Epoch [137/600], Training Loss: 3.8962, vgg Loss: 352.1573, L2 Loss: 0.3747, time: 48.9995\n",
      "Epoch [138/600], Training Loss: 3.9156, vgg Loss: 351.4947, L2 Loss: 0.4007, time: 48.5689\n",
      "Epoch [139/600], Training Loss: 3.6968, vgg Loss: 335.9254, L2 Loss: 0.3375, time: 48.4349\n",
      "Epoch [140/600], Training Loss: 3.7899, vgg Loss: 344.1747, L2 Loss: 0.3481, time: 48.5601\n",
      "Epoch [141/600], Training Loss: 3.7952, vgg Loss: 343.0680, L2 Loss: 0.3646, time: 48.1984\n",
      "Epoch [142/600], Training Loss: 3.6792, vgg Loss: 332.4657, L2 Loss: 0.3545, time: 48.1615\n",
      "Epoch [143/600], Training Loss: 3.7640, vgg Loss: 338.9606, L2 Loss: 0.3744, time: 104.1574\n",
      "Epoch [144/600], Training Loss: 3.6556, vgg Loss: 331.4907, L2 Loss: 0.3407, time: 106.9945\n",
      "Epoch [145/600], Training Loss: 3.5239, vgg Loss: 320.2442, L2 Loss: 0.3215, time: 106.8045\n",
      "Epoch [146/600], Training Loss: 3.8183, vgg Loss: 342.7689, L2 Loss: 0.3906, time: 107.0943\n",
      "Epoch [147/600], Training Loss: 4.0096, vgg Loss: 359.0845, L2 Loss: 0.4187, time: 113.1334\n",
      "Epoch [148/600], Training Loss: 3.5503, vgg Loss: 323.1600, L2 Loss: 0.3187, time: 106.7667\n",
      "Epoch [149/600], Training Loss: 3.6154, vgg Loss: 323.2972, L2 Loss: 0.3824, time: 106.6022\n",
      "Epoch [150/600], Training Loss: 3.4105, vgg Loss: 310.2251, L2 Loss: 0.3083, time: 106.8271\n",
      "Epoch [151/600], Training Loss: 3.5221, vgg Loss: 318.3516, L2 Loss: 0.3385, time: 106.7971\n",
      "Epoch [152/600], Training Loss: 3.4884, vgg Loss: 316.9295, L2 Loss: 0.3191, time: 106.8579\n",
      "Epoch [153/600], Training Loss: 3.6102, vgg Loss: 322.4300, L2 Loss: 0.3859, time: 106.8490\n",
      "Epoch [154/600], Training Loss: 3.5351, vgg Loss: 322.3615, L2 Loss: 0.3115, time: 106.7473\n",
      "Epoch [155/600], Training Loss: 3.3643, vgg Loss: 304.2031, L2 Loss: 0.3223, time: 107.4764\n",
      "Epoch [156/600], Training Loss: 3.4002, vgg Loss: 306.7611, L2 Loss: 0.3326, time: 106.9071\n",
      "Epoch [157/600], Training Loss: 3.3356, vgg Loss: 299.8961, L2 Loss: 0.3367, time: 106.6687\n",
      "Epoch [158/600], Training Loss: 3.3151, vgg Loss: 299.4229, L2 Loss: 0.3208, time: 106.7602\n",
      "Epoch [159/600], Training Loss: 3.3472, vgg Loss: 303.3741, L2 Loss: 0.3134, time: 106.7603\n",
      "Epoch [160/600], Training Loss: 3.2634, vgg Loss: 296.5818, L2 Loss: 0.2976, time: 106.7949\n",
      "Epoch [161/600], Training Loss: 3.1970, vgg Loss: 290.2915, L2 Loss: 0.2941, time: 106.6398\n",
      "Epoch [162/600], Training Loss: 3.5534, vgg Loss: 320.8376, L2 Loss: 0.3450, time: 106.8251\n",
      "Epoch [163/600], Training Loss: 3.2942, vgg Loss: 297.8707, L2 Loss: 0.3155, time: 106.4414\n",
      "Epoch [164/600], Training Loss: 3.1125, vgg Loss: 285.6605, L2 Loss: 0.2558, time: 112.4885\n",
      "Epoch [165/600], Training Loss: 3.1749, vgg Loss: 288.4653, L2 Loss: 0.2903, time: 106.7039\n",
      "Epoch [166/600], Training Loss: 3.3672, vgg Loss: 304.7340, L2 Loss: 0.3198, time: 106.7019\n",
      "Epoch [167/600], Training Loss: 3.1524, vgg Loss: 288.6089, L2 Loss: 0.2663, time: 106.9818\n",
      "Epoch [168/600], Training Loss: 3.2872, vgg Loss: 293.8506, L2 Loss: 0.3486, time: 106.5954\n",
      "Epoch [169/600], Training Loss: 3.2275, vgg Loss: 292.1650, L2 Loss: 0.3059, time: 106.5982\n",
      "Epoch [170/600], Training Loss: 3.2924, vgg Loss: 298.9225, L2 Loss: 0.3031, time: 106.7078\n",
      "Epoch [171/600], Training Loss: 3.1858, vgg Loss: 287.9619, L2 Loss: 0.3062, time: 106.8152\n",
      "Epoch [172/600], Training Loss: 3.0926, vgg Loss: 282.0586, L2 Loss: 0.2720, time: 112.9210\n",
      "Epoch [173/600], Training Loss: 3.0662, vgg Loss: 280.1967, L2 Loss: 0.2642, time: 106.7408\n",
      "Epoch [174/600], Training Loss: 2.9674, vgg Loss: 271.6680, L2 Loss: 0.2507, time: 106.4714\n",
      "Epoch [175/600], Training Loss: 3.1707, vgg Loss: 282.6341, L2 Loss: 0.3443, time: 106.7666\n",
      "Epoch [176/600], Training Loss: 3.0129, vgg Loss: 272.8194, L2 Loss: 0.2847, time: 106.7065\n",
      "Epoch [177/600], Training Loss: 3.0738, vgg Loss: 280.5120, L2 Loss: 0.2687, time: 106.7279\n",
      "Epoch [178/600], Training Loss: 2.8104, vgg Loss: 256.8972, L2 Loss: 0.2414, time: 106.7832\n",
      "Epoch [179/600], Training Loss: 3.1616, vgg Loss: 283.7026, L2 Loss: 0.3246, time: 106.8201\n",
      "Epoch [180/600], Training Loss: 2.9208, vgg Loss: 267.4034, L2 Loss: 0.2468, time: 106.6265\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [181/600], Training Loss: 2.9265, vgg Loss: 268.6066, L2 Loss: 0.2404, time: 112.8723\n",
      "Epoch [182/600], Training Loss: 2.9150, vgg Loss: 264.5136, L2 Loss: 0.2699, time: 106.6943\n",
      "Epoch [183/600], Training Loss: 2.8244, vgg Loss: 258.2632, L2 Loss: 0.2418, time: 106.6279\n",
      "Epoch [184/600], Training Loss: 2.8554, vgg Loss: 261.3473, L2 Loss: 0.2420, time: 106.6895\n",
      "Epoch [185/600], Training Loss: 2.9187, vgg Loss: 266.8355, L2 Loss: 0.2503, time: 106.8366\n",
      "Epoch [186/600], Training Loss: 3.0960, vgg Loss: 272.2135, L2 Loss: 0.3739, time: 106.7011\n",
      "Epoch [187/600], Training Loss: 2.7673, vgg Loss: 254.5007, L2 Loss: 0.2223, time: 106.7686\n",
      "Epoch [188/600], Training Loss: 2.9143, vgg Loss: 261.4978, L2 Loss: 0.2993, time: 106.7588\n",
      "Epoch [189/600], Training Loss: 2.8086, vgg Loss: 257.0572, L2 Loss: 0.2381, time: 112.6536\n",
      "Epoch [190/600], Training Loss: 2.8452, vgg Loss: 261.3557, L2 Loss: 0.2316, time: 106.7472\n",
      "Epoch [191/600], Training Loss: 2.8396, vgg Loss: 259.5612, L2 Loss: 0.2440, time: 106.7989\n",
      "Epoch [192/600], Training Loss: 2.7471, vgg Loss: 251.2816, L2 Loss: 0.2342, time: 106.5913\n",
      "Epoch [193/600], Training Loss: 2.8001, vgg Loss: 255.2671, L2 Loss: 0.2474, time: 106.7168\n",
      "Epoch [194/600], Training Loss: 2.9026, vgg Loss: 264.3630, L2 Loss: 0.2590, time: 106.7519\n",
      "Epoch [195/600], Training Loss: 2.7549, vgg Loss: 252.1448, L2 Loss: 0.2334, time: 106.7486\n",
      "Epoch [196/600], Training Loss: 2.7498, vgg Loss: 252.3437, L2 Loss: 0.2264, time: 106.7000\n",
      "Epoch [197/600], Training Loss: 2.8184, vgg Loss: 254.7770, L2 Loss: 0.2706, time: 106.5513\n",
      "Epoch [198/600], Training Loss: 2.7267, vgg Loss: 249.0612, L2 Loss: 0.2361, time: 112.6602\n",
      "Epoch [199/600], Training Loss: 2.8103, vgg Loss: 254.4535, L2 Loss: 0.2658, time: 106.5977\n",
      "Epoch [200/600], Training Loss: 2.7058, vgg Loss: 248.1272, L2 Loss: 0.2245, time: 106.7358\n",
      "Epoch [201/600], Training Loss: 2.6971, vgg Loss: 248.2911, L2 Loss: 0.2142, time: 106.5322\n",
      "Epoch [202/600], Training Loss: 2.7904, vgg Loss: 254.2908, L2 Loss: 0.2475, time: 106.3905\n",
      "Epoch [203/600], Training Loss: 2.7298, vgg Loss: 249.7985, L2 Loss: 0.2318, time: 106.8141\n",
      "Epoch [204/600], Training Loss: 2.8247, vgg Loss: 256.2582, L2 Loss: 0.2621, time: 106.8026\n",
      "Epoch [205/600], Training Loss: 2.7483, vgg Loss: 251.1559, L2 Loss: 0.2367, time: 106.7355\n",
      "Epoch [206/600], Training Loss: 2.6757, vgg Loss: 245.7667, L2 Loss: 0.2180, time: 112.7149\n",
      "Epoch [207/600], Training Loss: 2.6937, vgg Loss: 247.3712, L2 Loss: 0.2200, time: 106.8003\n",
      "Epoch [208/600], Training Loss: 2.7966, vgg Loss: 254.9272, L2 Loss: 0.2473, time: 106.5911\n",
      "Epoch [209/600], Training Loss: 2.6675, vgg Loss: 243.4274, L2 Loss: 0.2332, time: 106.7955\n",
      "Epoch [210/600], Training Loss: 2.9005, vgg Loss: 257.8577, L2 Loss: 0.3219, time: 106.4595\n",
      "Epoch [211/600], Training Loss: 2.6217, vgg Loss: 240.6582, L2 Loss: 0.2151, time: 106.6734\n",
      "Epoch [212/600], Training Loss: 2.8694, vgg Loss: 257.6676, L2 Loss: 0.2927, time: 106.7400\n",
      "Epoch [213/600], Training Loss: 2.5955, vgg Loss: 239.3808, L2 Loss: 0.2017, time: 106.7023\n",
      "Epoch [214/600], Training Loss: 2.7034, vgg Loss: 246.6581, L2 Loss: 0.2368, time: 106.7254\n",
      "Epoch [215/600], Training Loss: 2.5047, vgg Loss: 231.3122, L2 Loss: 0.1916, time: 112.6152\n",
      "Epoch [216/600], Training Loss: 2.7527, vgg Loss: 249.0035, L2 Loss: 0.2627, time: 106.8237\n",
      "Epoch [217/600], Training Loss: 2.6410, vgg Loss: 239.0653, L2 Loss: 0.2503, time: 106.6566\n",
      "Epoch [218/600], Training Loss: 2.6130, vgg Loss: 239.0795, L2 Loss: 0.2222, time: 106.6927\n",
      "Epoch [219/600], Training Loss: 2.5800, vgg Loss: 238.1508, L2 Loss: 0.1985, time: 106.8259\n",
      "Epoch [220/600], Training Loss: 2.4512, vgg Loss: 226.1083, L2 Loss: 0.1902, time: 106.6594\n",
      "Epoch [221/600], Training Loss: 2.5679, vgg Loss: 235.5005, L2 Loss: 0.2129, time: 106.5931\n",
      "Epoch [222/600], Training Loss: 2.5777, vgg Loss: 235.6265, L2 Loss: 0.2214, time: 106.8316\n",
      "Epoch [223/600], Training Loss: 2.5542, vgg Loss: 234.4428, L2 Loss: 0.2097, time: 112.6322\n",
      "Epoch [224/600], Training Loss: 2.4956, vgg Loss: 230.2143, L2 Loss: 0.1934, time: 106.7635\n",
      "Epoch [225/600], Training Loss: 2.6197, vgg Loss: 239.4330, L2 Loss: 0.2254, time: 106.9666\n",
      "Epoch [226/600], Training Loss: 2.6632, vgg Loss: 244.2943, L2 Loss: 0.2203, time: 106.8066\n",
      "Epoch [227/600], Training Loss: 2.4482, vgg Loss: 225.4364, L2 Loss: 0.1938, time: 106.8876\n",
      "Epoch [228/600], Training Loss: 2.5584, vgg Loss: 233.9797, L2 Loss: 0.2186, time: 108.1732\n",
      "Epoch [229/600], Training Loss: 2.6233, vgg Loss: 239.2590, L2 Loss: 0.2307, time: 108.1409\n",
      "Epoch [230/600], Training Loss: 2.5657, vgg Loss: 235.3246, L2 Loss: 0.2125, time: 108.0368\n",
      "Epoch [231/600], Training Loss: 2.5027, vgg Loss: 231.4557, L2 Loss: 0.1881, time: 106.9139\n",
      "Epoch [232/600], Training Loss: 2.6541, vgg Loss: 241.0657, L2 Loss: 0.2435, time: 112.9160\n",
      "Epoch [233/600], Training Loss: 2.4924, vgg Loss: 229.7969, L2 Loss: 0.1945, time: 106.8562\n",
      "Epoch [234/600], Training Loss: 2.5101, vgg Loss: 231.7318, L2 Loss: 0.1928, time: 106.8789\n",
      "Epoch [235/600], Training Loss: 2.4082, vgg Loss: 223.1215, L2 Loss: 0.1770, time: 106.5473\n",
      "Epoch [236/600], Training Loss: 2.5040, vgg Loss: 231.2525, L2 Loss: 0.1915, time: 106.8111\n",
      "Epoch [237/600], Training Loss: 2.5131, vgg Loss: 230.3580, L2 Loss: 0.2096, time: 106.8580\n",
      "Epoch [238/600], Training Loss: 2.3697, vgg Loss: 220.2748, L2 Loss: 0.1669, time: 106.8407\n",
      "Epoch [239/600], Training Loss: 2.3748, vgg Loss: 218.8229, L2 Loss: 0.1866, time: 106.5601\n",
      "Epoch [240/600], Training Loss: 2.3945, vgg Loss: 221.8314, L2 Loss: 0.1762, time: 112.5772\n",
      "Epoch [241/600], Training Loss: 2.5120, vgg Loss: 227.6113, L2 Loss: 0.2359, time: 106.6153\n",
      "Epoch [242/600], Training Loss: 2.5875, vgg Loss: 234.1638, L2 Loss: 0.2459, time: 106.9212\n",
      "Epoch [243/600], Training Loss: 2.3796, vgg Loss: 219.2511, L2 Loss: 0.1871, time: 106.9354\n",
      "Epoch [244/600], Training Loss: 2.4169, vgg Loss: 223.5257, L2 Loss: 0.1816, time: 106.7899\n",
      "Epoch [245/600], Training Loss: 2.3476, vgg Loss: 217.2989, L2 Loss: 0.1746, time: 106.8707\n",
      "Epoch [246/600], Training Loss: 2.3872, vgg Loss: 220.2810, L2 Loss: 0.1844, time: 106.9142\n",
      "Epoch [247/600], Training Loss: 2.6136, vgg Loss: 229.2996, L2 Loss: 0.3206, time: 106.7707\n",
      "Epoch [248/600], Training Loss: 2.3602, vgg Loss: 217.6843, L2 Loss: 0.1833, time: 106.9371\n",
      "Epoch [249/600], Training Loss: 2.4846, vgg Loss: 227.5906, L2 Loss: 0.2087, time: 111.0278\n",
      "Epoch [250/600], Training Loss: 2.3037, vgg Loss: 213.7904, L2 Loss: 0.1658, time: 106.5764\n",
      "Epoch [251/600], Training Loss: 2.3167, vgg Loss: 213.7806, L2 Loss: 0.1789, time: 106.6042\n",
      "Epoch [252/600], Training Loss: 2.5198, vgg Loss: 230.2147, L2 Loss: 0.2177, time: 106.7264\n",
      "Epoch [253/600], Training Loss: 2.3350, vgg Loss: 215.8417, L2 Loss: 0.1766, time: 106.7247\n",
      "Epoch [254/600], Training Loss: 2.3330, vgg Loss: 217.3615, L2 Loss: 0.1594, time: 106.7932\n",
      "Epoch [255/600], Training Loss: 2.3274, vgg Loss: 215.6936, L2 Loss: 0.1704, time: 106.6875\n",
      "Epoch [256/600], Training Loss: 2.4157, vgg Loss: 222.2709, L2 Loss: 0.1929, time: 106.6671\n",
      "Epoch [257/600], Training Loss: 2.4028, vgg Loss: 221.6383, L2 Loss: 0.1864, time: 112.1969\n",
      "Epoch [258/600], Training Loss: 2.3504, vgg Loss: 217.7897, L2 Loss: 0.1725, time: 106.8742\n",
      "Epoch [259/600], Training Loss: 2.2285, vgg Loss: 207.8238, L2 Loss: 0.1502, time: 106.4618\n",
      "Epoch [260/600], Training Loss: 2.3307, vgg Loss: 215.1767, L2 Loss: 0.1789, time: 106.5464\n",
      "Epoch [261/600], Training Loss: 2.2350, vgg Loss: 208.3580, L2 Loss: 0.1514, time: 106.7077\n",
      "Epoch [262/600], Training Loss: 2.3357, vgg Loss: 215.8989, L2 Loss: 0.1767, time: 106.8077\n",
      "Epoch [263/600], Training Loss: 2.3978, vgg Loss: 220.5098, L2 Loss: 0.1927, time: 106.7334\n",
      "Epoch [264/600], Training Loss: 2.3077, vgg Loss: 213.7512, L2 Loss: 0.1702, time: 106.7576\n",
      "Epoch [265/600], Training Loss: 2.3071, vgg Loss: 213.5391, L2 Loss: 0.1717, time: 106.6360\n",
      "Epoch [266/600], Training Loss: 2.3153, vgg Loss: 214.2758, L2 Loss: 0.1725, time: 109.9691\n",
      "Epoch [267/600], Training Loss: 2.2125, vgg Loss: 205.5759, L2 Loss: 0.1567, time: 106.7281\n",
      "Epoch [268/600], Training Loss: 2.2825, vgg Loss: 211.0582, L2 Loss: 0.1719, time: 106.5103\n",
      "Epoch [269/600], Training Loss: 2.2654, vgg Loss: 210.1505, L2 Loss: 0.1639, time: 106.5240\n",
      "Epoch [270/600], Training Loss: 2.3224, vgg Loss: 214.3924, L2 Loss: 0.1784, time: 106.8524\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [271/600], Training Loss: 2.2171, vgg Loss: 205.9955, L2 Loss: 0.1572, time: 106.8413\n",
      "Epoch [272/600], Training Loss: 2.2415, vgg Loss: 207.2918, L2 Loss: 0.1686, time: 106.7240\n",
      "Epoch [273/600], Training Loss: 2.1787, vgg Loss: 203.9000, L2 Loss: 0.1397, time: 106.6999\n",
      "Epoch [274/600], Training Loss: 2.1950, vgg Loss: 205.7212, L2 Loss: 0.1378, time: 107.0441\n",
      "Epoch [275/600], Training Loss: 2.2601, vgg Loss: 209.9336, L2 Loss: 0.1607, time: 106.5435\n",
      "Epoch [276/600], Training Loss: 2.2418, vgg Loss: 210.0637, L2 Loss: 0.1412, time: 106.8367\n",
      "Epoch [277/600], Training Loss: 2.3378, vgg Loss: 214.5841, L2 Loss: 0.1919, time: 106.6328\n",
      "Epoch [278/600], Training Loss: 2.2712, vgg Loss: 211.3019, L2 Loss: 0.1582, time: 106.6286\n",
      "Epoch [279/600], Training Loss: 2.2523, vgg Loss: 208.5473, L2 Loss: 0.1668, time: 106.7937\n",
      "Epoch [280/600], Training Loss: 2.2473, vgg Loss: 208.6768, L2 Loss: 0.1605, time: 107.1040\n",
      "Epoch [281/600], Training Loss: 2.1788, vgg Loss: 203.4180, L2 Loss: 0.1446, time: 106.7228\n",
      "Epoch [282/600], Training Loss: 2.1776, vgg Loss: 203.0982, L2 Loss: 0.1466, time: 107.0435\n",
      "Epoch [283/600], Training Loss: 2.1635, vgg Loss: 202.0848, L2 Loss: 0.1427, time: 106.8515\n",
      "Epoch [284/600], Training Loss: 2.2138, vgg Loss: 205.4379, L2 Loss: 0.1594, time: 106.6792\n",
      "Epoch [285/600], Training Loss: 2.0937, vgg Loss: 195.9514, L2 Loss: 0.1342, time: 106.6693\n",
      "Epoch [286/600], Training Loss: 2.1940, vgg Loss: 204.3864, L2 Loss: 0.1502, time: 106.5979\n",
      "Epoch [287/600], Training Loss: 2.3078, vgg Loss: 209.8516, L2 Loss: 0.2092, time: 106.6132\n",
      "Epoch [288/600], Training Loss: 2.2230, vgg Loss: 204.7988, L2 Loss: 0.1750, time: 106.6521\n",
      "Epoch [289/600], Training Loss: 2.1371, vgg Loss: 199.0764, L2 Loss: 0.1464, time: 106.7628\n",
      "Epoch [290/600], Training Loss: 2.1633, vgg Loss: 202.1842, L2 Loss: 0.1415, time: 106.7339\n",
      "Epoch [291/600], Training Loss: 2.1348, vgg Loss: 198.7166, L2 Loss: 0.1476, time: 106.8088\n",
      "Epoch [292/600], Training Loss: 2.1391, vgg Loss: 200.3475, L2 Loss: 0.1356, time: 106.6946\n",
      "Epoch [293/600], Training Loss: 2.2513, vgg Loss: 207.4984, L2 Loss: 0.1764, time: 106.8616\n",
      "Epoch [294/600], Training Loss: 2.1416, vgg Loss: 198.9816, L2 Loss: 0.1518, time: 106.6065\n",
      "Epoch [295/600], Training Loss: 2.2090, vgg Loss: 206.0939, L2 Loss: 0.1481, time: 106.6716\n",
      "Epoch [296/600], Training Loss: 2.1736, vgg Loss: 199.8570, L2 Loss: 0.1750, time: 106.7953\n",
      "Epoch [297/600], Training Loss: 2.1772, vgg Loss: 202.7766, L2 Loss: 0.1495, time: 107.1051\n",
      "Epoch [298/600], Training Loss: 2.5618, vgg Loss: 229.3924, L2 Loss: 0.2679, time: 106.8649\n",
      "Epoch [299/600], Training Loss: 2.2661, vgg Loss: 211.4713, L2 Loss: 0.1514, time: 106.4834\n",
      "Epoch [300/600], Training Loss: 2.2993, vgg Loss: 212.0584, L2 Loss: 0.1788, time: 106.8832\n",
      "Epoch [301/600], Training Loss: 2.1197, vgg Loss: 198.6988, L2 Loss: 0.1327, time: 106.5822\n",
      "Epoch [302/600], Training Loss: 1.9768, vgg Loss: 187.0999, L2 Loss: 0.1058, time: 106.6017\n",
      "Epoch [303/600], Training Loss: 1.9985, vgg Loss: 188.3250, L2 Loss: 0.1152, time: 106.5027\n",
      "Epoch [304/600], Training Loss: 2.0225, vgg Loss: 190.5458, L2 Loss: 0.1170, time: 106.3358\n",
      "Epoch [305/600], Training Loss: 1.9962, vgg Loss: 188.7310, L2 Loss: 0.1089, time: 106.5965\n",
      "Epoch [306/600], Training Loss: 1.9174, vgg Loss: 181.7044, L2 Loss: 0.1003, time: 112.7755\n",
      "Epoch [307/600], Training Loss: 1.9839, vgg Loss: 187.2021, L2 Loss: 0.1119, time: 112.1979\n",
      "Epoch [308/600], Training Loss: 1.9467, vgg Loss: 184.0953, L2 Loss: 0.1058, time: 106.8937\n",
      "Epoch [309/600], Training Loss: 1.9503, vgg Loss: 184.5862, L2 Loss: 0.1044, time: 106.8088\n",
      "Epoch [310/600], Training Loss: 1.8866, vgg Loss: 178.4913, L2 Loss: 0.1017, time: 106.8352\n",
      "Epoch [311/600], Training Loss: 1.9176, vgg Loss: 181.5231, L2 Loss: 0.1023, time: 106.8298\n",
      "Epoch [312/600], Training Loss: 1.9488, vgg Loss: 184.3249, L2 Loss: 0.1055, time: 105.8382\n",
      "Epoch [313/600], Training Loss: 1.9038, vgg Loss: 180.0261, L2 Loss: 0.1035, time: 131.3860\n",
      "Epoch [314/600], Training Loss: 1.9433, vgg Loss: 183.9887, L2 Loss: 0.1034, time: 154.8851\n",
      "Epoch [315/600], Training Loss: 1.9250, vgg Loss: 182.1949, L2 Loss: 0.1030, time: 151.6271\n",
      "Epoch [316/600], Training Loss: 1.9145, vgg Loss: 181.5174, L2 Loss: 0.0993, time: 115.2675\n",
      "Epoch [317/600], Training Loss: 1.8603, vgg Loss: 175.6442, L2 Loss: 0.1038, time: 132.8610\n",
      "Epoch [318/600], Training Loss: 1.8808, vgg Loss: 178.4052, L2 Loss: 0.0968, time: 135.5727\n",
      "Epoch [319/600], Training Loss: 1.9257, vgg Loss: 182.0830, L2 Loss: 0.1049, time: 130.9513\n",
      "Epoch [320/600], Training Loss: 1.9080, vgg Loss: 180.5558, L2 Loss: 0.1025, time: 135.8632\n",
      "Epoch [321/600], Training Loss: 1.8556, vgg Loss: 176.0427, L2 Loss: 0.0952, time: 129.8693\n",
      "Epoch [322/600], Training Loss: 1.9141, vgg Loss: 181.3029, L2 Loss: 0.1011, time: 130.1600\n",
      "Epoch [323/600], Training Loss: 1.9159, vgg Loss: 181.2933, L2 Loss: 0.1030, time: 125.9777\n",
      "Epoch [324/600], Training Loss: 1.8926, vgg Loss: 179.2653, L2 Loss: 0.1000, time: 126.1824\n",
      "Epoch [325/600], Training Loss: 1.9064, vgg Loss: 181.1378, L2 Loss: 0.0951, time: 126.0196\n",
      "Epoch [326/600], Training Loss: 1.9058, vgg Loss: 180.2744, L2 Loss: 0.1031, time: 126.3355\n",
      "Epoch [327/600], Training Loss: 1.8460, vgg Loss: 174.9642, L2 Loss: 0.0964, time: 126.3083\n",
      "Epoch [328/600], Training Loss: 1.9121, vgg Loss: 181.1976, L2 Loss: 0.1001, time: 126.3922\n",
      "Epoch [329/600], Training Loss: 1.9486, vgg Loss: 184.3001, L2 Loss: 0.1056, time: 126.4826\n",
      "Epoch [330/600], Training Loss: 1.8739, vgg Loss: 177.2623, L2 Loss: 0.1013, time: 126.3220\n",
      "Epoch [331/600], Training Loss: 1.9177, vgg Loss: 180.9038, L2 Loss: 0.1086, time: 126.4964\n",
      "Epoch [332/600], Training Loss: 1.8976, vgg Loss: 179.7163, L2 Loss: 0.1005, time: 127.0600\n",
      "Epoch [333/600], Training Loss: 1.8750, vgg Loss: 177.4702, L2 Loss: 0.1003, time: 126.4249\n",
      "Epoch [334/600], Training Loss: 1.9399, vgg Loss: 183.0260, L2 Loss: 0.1096, time: 126.4022\n",
      "Epoch [335/600], Training Loss: 1.8751, vgg Loss: 177.8870, L2 Loss: 0.0963, time: 126.3849\n",
      "Epoch [336/600], Training Loss: 1.8948, vgg Loss: 179.1469, L2 Loss: 0.1034, time: 126.3854\n",
      "Epoch [337/600], Training Loss: 1.9084, vgg Loss: 180.7629, L2 Loss: 0.1007, time: 126.4612\n",
      "Epoch [338/600], Training Loss: 1.8974, vgg Loss: 179.4409, L2 Loss: 0.1029, time: 126.3874\n",
      "Epoch [339/600], Training Loss: 1.8990, vgg Loss: 179.4932, L2 Loss: 0.1041, time: 126.2866\n",
      "Epoch [340/600], Training Loss: 1.8819, vgg Loss: 178.3502, L2 Loss: 0.0984, time: 126.2360\n",
      "Epoch [341/600], Training Loss: 1.9237, vgg Loss: 181.8042, L2 Loss: 0.1057, time: 126.3880\n",
      "Epoch [342/600], Training Loss: 1.8373, vgg Loss: 174.2394, L2 Loss: 0.0950, time: 126.3912\n",
      "Epoch [343/600], Training Loss: 1.8819, vgg Loss: 178.0426, L2 Loss: 0.1014, time: 126.4553\n",
      "Epoch [344/600], Training Loss: 1.9630, vgg Loss: 186.0185, L2 Loss: 0.1028, time: 126.5909\n",
      "Epoch [345/600], Training Loss: 1.8907, vgg Loss: 178.8857, L2 Loss: 0.1018, time: 126.5330\n",
      "Epoch [346/600], Training Loss: 1.8756, vgg Loss: 177.8195, L2 Loss: 0.0974, time: 126.5651\n",
      "Epoch [347/600], Training Loss: 1.8946, vgg Loss: 179.5313, L2 Loss: 0.0993, time: 126.2444\n",
      "Epoch [348/600], Training Loss: 1.8882, vgg Loss: 179.2602, L2 Loss: 0.0956, time: 126.3023\n",
      "Epoch [349/600], Training Loss: 1.9228, vgg Loss: 181.8378, L2 Loss: 0.1045, time: 126.3656\n",
      "Epoch [350/600], Training Loss: 1.8331, vgg Loss: 174.2007, L2 Loss: 0.0911, time: 126.5496\n",
      "Epoch [351/600], Training Loss: 1.8944, vgg Loss: 179.1974, L2 Loss: 0.1024, time: 126.4936\n",
      "Epoch [352/600], Training Loss: 1.9105, vgg Loss: 181.0847, L2 Loss: 0.0997, time: 126.4709\n",
      "Epoch [353/600], Training Loss: 1.8917, vgg Loss: 179.1517, L2 Loss: 0.1002, time: 126.5087\n",
      "Epoch [354/600], Training Loss: 1.9039, vgg Loss: 179.8330, L2 Loss: 0.1056, time: 126.5690\n",
      "Epoch [355/600], Training Loss: 1.8495, vgg Loss: 174.6746, L2 Loss: 0.1027, time: 126.6155\n",
      "Epoch [356/600], Training Loss: 1.8630, vgg Loss: 176.5002, L2 Loss: 0.0980, time: 126.5356\n",
      "Epoch [357/600], Training Loss: 1.8154, vgg Loss: 171.8415, L2 Loss: 0.0970, time: 126.5412\n",
      "Epoch [358/600], Training Loss: 1.8981, vgg Loss: 180.3001, L2 Loss: 0.0951, time: 126.6664\n",
      "Epoch [359/600], Training Loss: 1.8705, vgg Loss: 176.6938, L2 Loss: 0.1035, time: 126.5570\n",
      "Epoch [360/600], Training Loss: 1.8747, vgg Loss: 177.5590, L2 Loss: 0.0991, time: 126.5367\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [361/600], Training Loss: 1.9056, vgg Loss: 180.3900, L2 Loss: 0.1017, time: 126.5091\n",
      "Epoch [362/600], Training Loss: 1.8824, vgg Loss: 178.4774, L2 Loss: 0.0976, time: 126.5623\n",
      "Epoch [363/600], Training Loss: 1.8357, vgg Loss: 174.2763, L2 Loss: 0.0929, time: 126.4609\n",
      "Epoch [364/600], Training Loss: 1.8669, vgg Loss: 176.8188, L2 Loss: 0.0987, time: 126.8896\n",
      "Epoch [365/600], Training Loss: 1.8509, vgg Loss: 175.5612, L2 Loss: 0.0953, time: 126.5665\n",
      "Epoch [366/600], Training Loss: 1.8774, vgg Loss: 177.9319, L2 Loss: 0.0981, time: 126.6266\n",
      "Epoch [367/600], Training Loss: 1.8524, vgg Loss: 175.8039, L2 Loss: 0.0944, time: 126.5787\n",
      "Epoch [368/600], Training Loss: 1.8834, vgg Loss: 178.5221, L2 Loss: 0.0982, time: 126.5863\n",
      "Epoch [369/600], Training Loss: 1.8017, vgg Loss: 170.8948, L2 Loss: 0.0928, time: 126.4530\n",
      "Epoch [370/600], Training Loss: 1.8503, vgg Loss: 175.7437, L2 Loss: 0.0928, time: 126.3449\n",
      "Epoch [371/600], Training Loss: 1.8532, vgg Loss: 175.8519, L2 Loss: 0.0947, time: 126.5446\n",
      "Epoch [372/600], Training Loss: 1.8656, vgg Loss: 176.8612, L2 Loss: 0.0970, time: 126.5897\n",
      "Epoch [373/600], Training Loss: 1.8401, vgg Loss: 174.4145, L2 Loss: 0.0959, time: 126.5110\n",
      "Epoch [374/600], Training Loss: 1.8480, vgg Loss: 175.4658, L2 Loss: 0.0933, time: 126.5850\n",
      "Epoch [375/600], Training Loss: 1.8441, vgg Loss: 174.5559, L2 Loss: 0.0985, time: 126.5094\n",
      "Epoch [376/600], Training Loss: 1.8286, vgg Loss: 173.0732, L2 Loss: 0.0979, time: 126.2919\n",
      "Epoch [377/600], Training Loss: 1.8774, vgg Loss: 177.5745, L2 Loss: 0.1017, time: 126.5488\n",
      "Epoch [378/600], Training Loss: 1.8693, vgg Loss: 176.2237, L2 Loss: 0.1071, time: 126.4819\n",
      "Epoch [379/600], Training Loss: 1.7919, vgg Loss: 170.5704, L2 Loss: 0.0861, time: 126.8415\n",
      "Epoch [380/600], Training Loss: 1.8630, vgg Loss: 176.5742, L2 Loss: 0.0972, time: 127.8081\n",
      "Epoch [381/600], Training Loss: 1.8261, vgg Loss: 173.4574, L2 Loss: 0.0916, time: 126.5374\n",
      "Epoch [382/600], Training Loss: 1.8626, vgg Loss: 176.7074, L2 Loss: 0.0955, time: 126.9793\n",
      "Epoch [383/600], Training Loss: 1.8639, vgg Loss: 176.8015, L2 Loss: 0.0958, time: 126.6550\n",
      "Epoch [384/600], Training Loss: 1.8690, vgg Loss: 177.4259, L2 Loss: 0.0947, time: 126.6338\n",
      "Epoch [385/600], Training Loss: 1.8444, vgg Loss: 175.1811, L2 Loss: 0.0926, time: 126.5655\n",
      "Epoch [386/600], Training Loss: 1.8709, vgg Loss: 176.9255, L2 Loss: 0.1016, time: 126.6201\n",
      "Epoch [387/600], Training Loss: 1.7677, vgg Loss: 167.8362, L2 Loss: 0.0894, time: 126.6908\n",
      "Epoch [388/600], Training Loss: 1.8894, vgg Loss: 178.6013, L2 Loss: 0.1034, time: 126.6361\n",
      "Epoch [389/600], Training Loss: 1.7960, vgg Loss: 170.8549, L2 Loss: 0.0875, time: 126.5822\n",
      "Epoch [390/600], Training Loss: 1.8471, vgg Loss: 175.4014, L2 Loss: 0.0931, time: 126.6072\n",
      "Epoch [391/600], Training Loss: 1.7974, vgg Loss: 171.3791, L2 Loss: 0.0836, time: 126.4606\n",
      "Epoch [392/600], Training Loss: 1.8325, vgg Loss: 173.7018, L2 Loss: 0.0955, time: 126.5456\n",
      "Epoch [393/600], Training Loss: 1.8542, vgg Loss: 175.8111, L2 Loss: 0.0960, time: 126.5564\n",
      "Epoch [394/600], Training Loss: 1.9067, vgg Loss: 180.6365, L2 Loss: 0.1004, time: 126.5848\n",
      "Epoch [395/600], Training Loss: 1.8316, vgg Loss: 173.7557, L2 Loss: 0.0940, time: 126.5328\n",
      "Epoch [396/600], Training Loss: 1.8388, vgg Loss: 174.7039, L2 Loss: 0.0917, time: 126.4738\n",
      "Epoch [397/600], Training Loss: 1.8480, vgg Loss: 175.4610, L2 Loss: 0.0934, time: 126.5035\n",
      "Epoch [398/600], Training Loss: 1.7691, vgg Loss: 167.9387, L2 Loss: 0.0897, time: 126.5306\n",
      "Epoch [399/600], Training Loss: 1.8625, vgg Loss: 176.6813, L2 Loss: 0.0957, time: 126.5573\n",
      "Epoch [400/600], Training Loss: 1.7773, vgg Loss: 168.9232, L2 Loss: 0.0881, time: 126.5666\n",
      "Epoch [401/600], Training Loss: 1.7930, vgg Loss: 169.9424, L2 Loss: 0.0936, time: 126.6036\n",
      "Epoch [402/600], Training Loss: 1.8047, vgg Loss: 171.3705, L2 Loss: 0.0910, time: 126.6760\n",
      "Epoch [403/600], Training Loss: 1.8016, vgg Loss: 171.0557, L2 Loss: 0.0910, time: 126.6358\n",
      "Epoch [404/600], Training Loss: 1.8529, vgg Loss: 176.0796, L2 Loss: 0.0921, time: 126.4315\n",
      "Epoch [405/600], Training Loss: 1.8180, vgg Loss: 172.8201, L2 Loss: 0.0898, time: 126.6135\n",
      "Epoch [406/600], Training Loss: 1.8589, vgg Loss: 175.9829, L2 Loss: 0.0991, time: 126.5904\n",
      "Epoch [407/600], Training Loss: 1.8461, vgg Loss: 174.7858, L2 Loss: 0.0982, time: 126.6496\n",
      "Epoch [408/600], Training Loss: 1.8372, vgg Loss: 174.7190, L2 Loss: 0.0900, time: 126.6095\n",
      "Epoch [409/600], Training Loss: 1.8208, vgg Loss: 172.4692, L2 Loss: 0.0961, time: 126.7563\n",
      "Epoch [410/600], Training Loss: 1.8064, vgg Loss: 171.4921, L2 Loss: 0.0915, time: 126.5998\n",
      "Epoch [411/600], Training Loss: 1.8060, vgg Loss: 171.7642, L2 Loss: 0.0884, time: 126.6293\n",
      "Epoch [412/600], Training Loss: 1.8654, vgg Loss: 176.7448, L2 Loss: 0.0980, time: 126.5348\n",
      "Epoch [413/600], Training Loss: 1.8387, vgg Loss: 174.6613, L2 Loss: 0.0921, time: 126.6783\n",
      "Epoch [414/600], Training Loss: 1.8155, vgg Loss: 172.7101, L2 Loss: 0.0884, time: 127.4563\n",
      "Epoch [415/600], Training Loss: 1.8722, vgg Loss: 177.3871, L2 Loss: 0.0984, time: 127.6527\n",
      "Epoch [416/600], Training Loss: 1.7780, vgg Loss: 168.9531, L2 Loss: 0.0884, time: 126.9835\n",
      "Epoch [417/600], Training Loss: 1.8631, vgg Loss: 176.4461, L2 Loss: 0.0987, time: 126.7085\n",
      "Epoch [418/600], Training Loss: 1.8072, vgg Loss: 171.8870, L2 Loss: 0.0883, time: 126.7979\n",
      "Epoch [419/600], Training Loss: 1.7712, vgg Loss: 168.4027, L2 Loss: 0.0871, time: 126.5478\n",
      "Epoch [420/600], Training Loss: 1.8361, vgg Loss: 174.6253, L2 Loss: 0.0898, time: 126.5849\n",
      "Epoch [421/600], Training Loss: 1.7528, vgg Loss: 167.0933, L2 Loss: 0.0819, time: 126.4951\n",
      "Epoch [422/600], Training Loss: 1.7709, vgg Loss: 167.7030, L2 Loss: 0.0939, time: 126.4207\n",
      "Epoch [423/600], Training Loss: 1.8118, vgg Loss: 171.7014, L2 Loss: 0.0948, time: 126.6055\n",
      "Epoch [424/600], Training Loss: 1.7973, vgg Loss: 170.8679, L2 Loss: 0.0886, time: 126.6078\n",
      "Epoch [425/600], Training Loss: 1.7826, vgg Loss: 169.5107, L2 Loss: 0.0875, time: 126.3742\n",
      "Epoch [426/600], Training Loss: 1.8142, vgg Loss: 172.9295, L2 Loss: 0.0849, time: 126.3922\n",
      "Epoch [427/600], Training Loss: 1.8524, vgg Loss: 176.1057, L2 Loss: 0.0914, time: 126.3875\n",
      "Epoch [428/600], Training Loss: 1.8337, vgg Loss: 173.8902, L2 Loss: 0.0948, time: 132.1993\n",
      "Epoch [429/600], Training Loss: 1.8160, vgg Loss: 172.4884, L2 Loss: 0.0911, time: 126.7349\n",
      "Epoch [430/600], Training Loss: 1.7996, vgg Loss: 171.2303, L2 Loss: 0.0873, time: 128.4772\n",
      "Epoch [431/600], Training Loss: 1.8328, vgg Loss: 174.0144, L2 Loss: 0.0926, time: 141.8468\n",
      "Epoch [432/600], Training Loss: 1.8255, vgg Loss: 173.1088, L2 Loss: 0.0944, time: 129.1328\n",
      "Epoch [433/600], Training Loss: 1.7811, vgg Loss: 169.2102, L2 Loss: 0.0890, time: 134.0875\n",
      "Epoch [434/600], Training Loss: 1.7787, vgg Loss: 169.0670, L2 Loss: 0.0880, time: 106.2019\n",
      "Epoch [435/600], Training Loss: 1.8183, vgg Loss: 172.3748, L2 Loss: 0.0946, time: 102.8455\n",
      "Epoch [436/600], Training Loss: 1.8431, vgg Loss: 175.1804, L2 Loss: 0.0913, time: 105.7194\n",
      "Epoch [437/600], Training Loss: 1.7302, vgg Loss: 164.7272, L2 Loss: 0.0829, time: 107.9262\n",
      "Epoch [438/600], Training Loss: 1.7933, vgg Loss: 170.5636, L2 Loss: 0.0877, time: 104.9599\n",
      "Epoch [439/600], Training Loss: 1.8211, vgg Loss: 172.8722, L2 Loss: 0.0924, time: 106.7288\n",
      "Epoch [440/600], Training Loss: 1.8479, vgg Loss: 175.6406, L2 Loss: 0.0914, time: 105.0868\n",
      "Epoch [441/600], Training Loss: 1.8222, vgg Loss: 173.1628, L2 Loss: 0.0906, time: 103.3378\n",
      "Epoch [442/600], Training Loss: 1.7497, vgg Loss: 166.4883, L2 Loss: 0.0848, time: 105.3894\n",
      "Epoch [443/600], Training Loss: 1.8189, vgg Loss: 172.4669, L2 Loss: 0.0942, time: 104.7448\n",
      "Epoch [444/600], Training Loss: 1.7971, vgg Loss: 171.0412, L2 Loss: 0.0867, time: 126.4044\n",
      "Epoch [445/600], Training Loss: 1.7855, vgg Loss: 169.4780, L2 Loss: 0.0907, time: 103.2447\n",
      "Epoch [446/600], Training Loss: 1.8345, vgg Loss: 174.0656, L2 Loss: 0.0938, time: 102.3411\n",
      "Epoch [447/600], Training Loss: 1.8238, vgg Loss: 173.4279, L2 Loss: 0.0896, time: 102.4788\n",
      "Epoch [448/600], Training Loss: 1.8061, vgg Loss: 171.9399, L2 Loss: 0.0868, time: 102.2658\n",
      "Epoch [449/600], Training Loss: 1.7979, vgg Loss: 170.7530, L2 Loss: 0.0904, time: 102.2830\n",
      "Epoch [450/600], Training Loss: 1.7568, vgg Loss: 167.1172, L2 Loss: 0.0856, time: 102.3478\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [451/600], Training Loss: 1.7782, vgg Loss: 168.8057, L2 Loss: 0.0901, time: 102.4794\n",
      "Epoch [452/600], Training Loss: 1.7661, vgg Loss: 168.2996, L2 Loss: 0.0831, time: 106.4496\n",
      "Epoch [453/600], Training Loss: 1.8176, vgg Loss: 172.6759, L2 Loss: 0.0909, time: 105.6013\n",
      "Epoch [454/600], Training Loss: 1.8006, vgg Loss: 170.9950, L2 Loss: 0.0906, time: 107.0261\n",
      "Epoch [455/600], Training Loss: 1.7672, vgg Loss: 167.6921, L2 Loss: 0.0903, time: 106.2017\n",
      "Epoch [456/600], Training Loss: 1.7604, vgg Loss: 167.0941, L2 Loss: 0.0895, time: 106.8772\n",
      "Epoch [457/600], Training Loss: 1.8075, vgg Loss: 171.4257, L2 Loss: 0.0933, time: 106.1299\n",
      "Epoch [458/600], Training Loss: 1.7919, vgg Loss: 170.2957, L2 Loss: 0.0889, time: 111.5677\n",
      "Epoch [459/600], Training Loss: 1.7921, vgg Loss: 170.0140, L2 Loss: 0.0920, time: 104.9966\n",
      "Epoch [460/600], Training Loss: 1.7259, vgg Loss: 164.4544, L2 Loss: 0.0814, time: 104.5332\n",
      "Epoch [461/600], Training Loss: 1.8001, vgg Loss: 171.3415, L2 Loss: 0.0867, time: 106.1198\n",
      "Epoch [462/600], Training Loss: 1.8128, vgg Loss: 172.2484, L2 Loss: 0.0903, time: 105.8735\n",
      "Epoch [463/600], Training Loss: 1.8016, vgg Loss: 171.4560, L2 Loss: 0.0870, time: 102.7268\n",
      "Epoch [464/600], Training Loss: 1.7621, vgg Loss: 167.5769, L2 Loss: 0.0863, time: 104.6786\n",
      "Epoch [465/600], Training Loss: 1.7921, vgg Loss: 171.0212, L2 Loss: 0.0819, time: 106.1375\n",
      "Epoch [466/600], Training Loss: 1.8293, vgg Loss: 174.2164, L2 Loss: 0.0872, time: 109.4440\n",
      "Epoch [467/600], Training Loss: 1.7957, vgg Loss: 171.0822, L2 Loss: 0.0849, time: 112.3012\n",
      "Epoch [468/600], Training Loss: 1.8600, vgg Loss: 176.6289, L2 Loss: 0.0937, time: 114.4803\n",
      "Epoch [469/600], Training Loss: 1.7710, vgg Loss: 168.2482, L2 Loss: 0.0885, time: 121.9812\n",
      "Epoch [470/600], Training Loss: 1.8008, vgg Loss: 170.9237, L2 Loss: 0.0915, time: 109.5632\n",
      "Epoch [471/600], Training Loss: 1.7909, vgg Loss: 170.6096, L2 Loss: 0.0848, time: 102.3583\n",
      "Epoch [472/600], Training Loss: 1.8437, vgg Loss: 175.1407, L2 Loss: 0.0923, time: 102.2853\n",
      "Epoch [473/600], Training Loss: 1.8061, vgg Loss: 171.8107, L2 Loss: 0.0880, time: 102.2809\n",
      "Epoch [474/600], Training Loss: 1.8428, vgg Loss: 174.7100, L2 Loss: 0.0957, time: 102.4368\n",
      "Epoch [475/600], Training Loss: 1.8343, vgg Loss: 174.1764, L2 Loss: 0.0925, time: 102.1018\n",
      "Epoch [476/600], Training Loss: 1.7603, vgg Loss: 167.1864, L2 Loss: 0.0885, time: 102.1231\n",
      "Epoch [477/600], Training Loss: 1.7745, vgg Loss: 169.1047, L2 Loss: 0.0835, time: 102.2910\n",
      "Epoch [478/600], Training Loss: 1.7499, vgg Loss: 166.8011, L2 Loss: 0.0819, time: 102.2815\n",
      "Epoch [479/600], Training Loss: 1.7721, vgg Loss: 168.5242, L2 Loss: 0.0868, time: 102.1175\n",
      "Epoch [480/600], Training Loss: 1.7732, vgg Loss: 168.4631, L2 Loss: 0.0885, time: 102.2452\n",
      "Epoch [481/600], Training Loss: 1.8084, vgg Loss: 171.8206, L2 Loss: 0.0902, time: 102.3407\n",
      "Epoch [482/600], Training Loss: 1.7651, vgg Loss: 167.9089, L2 Loss: 0.0860, time: 102.2337\n",
      "Epoch [483/600], Training Loss: 1.7848, vgg Loss: 169.5468, L2 Loss: 0.0893, time: 102.1419\n",
      "Epoch [484/600], Training Loss: 1.7811, vgg Loss: 169.7891, L2 Loss: 0.0832, time: 102.1150\n",
      "Epoch [485/600], Training Loss: 1.7310, vgg Loss: 164.7420, L2 Loss: 0.0836, time: 102.5270\n",
      "Epoch [486/600], Training Loss: 1.7964, vgg Loss: 170.6666, L2 Loss: 0.0897, time: 102.1649\n",
      "Epoch [487/600], Training Loss: 1.7997, vgg Loss: 170.8232, L2 Loss: 0.0915, time: 102.2938\n",
      "Epoch [488/600], Training Loss: 1.7569, vgg Loss: 167.5561, L2 Loss: 0.0813, time: 102.1860\n",
      "Epoch [489/600], Training Loss: 1.8033, vgg Loss: 171.6838, L2 Loss: 0.0864, time: 102.2294\n",
      "Epoch [490/600], Training Loss: 1.7983, vgg Loss: 170.4190, L2 Loss: 0.0941, time: 102.2867\n",
      "Epoch [491/600], Training Loss: 1.7826, vgg Loss: 169.3620, L2 Loss: 0.0890, time: 102.3392\n",
      "Epoch [492/600], Training Loss: 1.7705, vgg Loss: 168.5473, L2 Loss: 0.0851, time: 102.2761\n",
      "Epoch [493/600], Training Loss: 1.7905, vgg Loss: 170.4186, L2 Loss: 0.0863, time: 102.2613\n",
      "Epoch [494/600], Training Loss: 1.7914, vgg Loss: 170.4442, L2 Loss: 0.0869, time: 102.1859\n",
      "Epoch [495/600], Training Loss: 1.7713, vgg Loss: 168.8562, L2 Loss: 0.0827, time: 102.1458\n",
      "Epoch [496/600], Training Loss: 1.7600, vgg Loss: 167.4299, L2 Loss: 0.0857, time: 102.2456\n",
      "Epoch [497/600], Training Loss: 1.7729, vgg Loss: 168.3650, L2 Loss: 0.0893, time: 102.1981\n",
      "Epoch [498/600], Training Loss: 1.7675, vgg Loss: 167.2658, L2 Loss: 0.0948, time: 102.2952\n",
      "Epoch [499/600], Training Loss: 1.7707, vgg Loss: 168.8040, L2 Loss: 0.0826, time: 102.1439\n",
      "Epoch [500/600], Training Loss: 1.8059, vgg Loss: 171.3679, L2 Loss: 0.0922, time: 102.2524\n",
      "Epoch [501/600], Training Loss: 1.8159, vgg Loss: 172.1996, L2 Loss: 0.0939, time: 102.2398\n",
      "Epoch [502/600], Training Loss: 1.7862, vgg Loss: 169.2831, L2 Loss: 0.0934, time: 102.2295\n",
      "Epoch [503/600], Training Loss: 1.7836, vgg Loss: 169.7823, L2 Loss: 0.0857, time: 102.2467\n",
      "Epoch [504/600], Training Loss: 1.7548, vgg Loss: 167.2040, L2 Loss: 0.0828, time: 102.1814\n",
      "Epoch [505/600], Training Loss: 1.7684, vgg Loss: 167.9096, L2 Loss: 0.0893, time: 102.2362\n",
      "Epoch [506/600], Training Loss: 1.7918, vgg Loss: 170.1009, L2 Loss: 0.0908, time: 102.2911\n",
      "Epoch [507/600], Training Loss: 1.8042, vgg Loss: 172.0602, L2 Loss: 0.0836, time: 102.1131\n",
      "Epoch [508/600], Training Loss: 1.8022, vgg Loss: 171.4589, L2 Loss: 0.0876, time: 102.3083\n",
      "Epoch [509/600], Training Loss: 1.7926, vgg Loss: 170.6001, L2 Loss: 0.0866, time: 102.2113\n",
      "Epoch [510/600], Training Loss: 1.8039, vgg Loss: 171.9103, L2 Loss: 0.0848, time: 102.2565\n",
      "Epoch [511/600], Training Loss: 1.8126, vgg Loss: 171.8805, L2 Loss: 0.0938, time: 102.2910\n",
      "Epoch [512/600], Training Loss: 1.7751, vgg Loss: 168.7994, L2 Loss: 0.0871, time: 102.3365\n",
      "Epoch [513/600], Training Loss: 1.7034, vgg Loss: 161.8205, L2 Loss: 0.0852, time: 102.2783\n",
      "Epoch [514/600], Training Loss: 1.7711, vgg Loss: 168.6662, L2 Loss: 0.0844, time: 102.2425\n",
      "Epoch [515/600], Training Loss: 1.7558, vgg Loss: 167.5463, L2 Loss: 0.0803, time: 102.1335\n",
      "Epoch [516/600], Training Loss: 1.7550, vgg Loss: 167.2769, L2 Loss: 0.0823, time: 102.1923\n",
      "Epoch [517/600], Training Loss: 1.7898, vgg Loss: 169.6634, L2 Loss: 0.0931, time: 102.3513\n",
      "Epoch [518/600], Training Loss: 1.7766, vgg Loss: 168.9130, L2 Loss: 0.0875, time: 102.1734\n",
      "Epoch [519/600], Training Loss: 1.7691, vgg Loss: 168.7063, L2 Loss: 0.0821, time: 102.1466\n",
      "Epoch [520/600], Training Loss: 1.7649, vgg Loss: 167.8411, L2 Loss: 0.0865, time: 102.2758\n",
      "Epoch [521/600], Training Loss: 1.7377, vgg Loss: 165.2112, L2 Loss: 0.0856, time: 102.2155\n",
      "Epoch [522/600], Training Loss: 1.7476, vgg Loss: 166.4579, L2 Loss: 0.0830, time: 102.0641\n",
      "Epoch [523/600], Training Loss: 1.7516, vgg Loss: 166.8231, L2 Loss: 0.0833, time: 102.2626\n",
      "Epoch [524/600], Training Loss: 1.7699, vgg Loss: 168.9959, L2 Loss: 0.0800, time: 102.1416\n",
      "Epoch [525/600], Training Loss: 1.7785, vgg Loss: 169.2488, L2 Loss: 0.0860, time: 102.5626\n",
      "Epoch [526/600], Training Loss: 1.7611, vgg Loss: 167.9064, L2 Loss: 0.0820, time: 102.2734\n",
      "Epoch [527/600], Training Loss: 1.7761, vgg Loss: 169.0998, L2 Loss: 0.0851, time: 102.1908\n",
      "Epoch [528/600], Training Loss: 1.7659, vgg Loss: 167.9912, L2 Loss: 0.0860, time: 102.1889\n",
      "Epoch [529/600], Training Loss: 1.8000, vgg Loss: 170.9693, L2 Loss: 0.0903, time: 102.2435\n",
      "Epoch [530/600], Training Loss: 1.7314, vgg Loss: 165.3174, L2 Loss: 0.0783, time: 102.1268\n",
      "Epoch [531/600], Training Loss: 1.7454, vgg Loss: 165.9534, L2 Loss: 0.0859, time: 102.2059\n",
      "Epoch [532/600], Training Loss: 1.7438, vgg Loss: 166.1624, L2 Loss: 0.0822, time: 102.1888\n",
      "Epoch [533/600], Training Loss: 1.7107, vgg Loss: 163.2533, L2 Loss: 0.0782, time: 102.2031\n",
      "Epoch [534/600], Training Loss: 1.7641, vgg Loss: 168.2014, L2 Loss: 0.0821, time: 102.1582\n",
      "Epoch [535/600], Training Loss: 1.7303, vgg Loss: 164.9260, L2 Loss: 0.0810, time: 102.0461\n",
      "Epoch [536/600], Training Loss: 1.7842, vgg Loss: 169.8068, L2 Loss: 0.0861, time: 102.1949\n",
      "Epoch [537/600], Training Loss: 1.7118, vgg Loss: 162.7442, L2 Loss: 0.0844, time: 102.1736\n",
      "Epoch [538/600], Training Loss: 1.7292, vgg Loss: 164.4248, L2 Loss: 0.0849, time: 102.3053\n",
      "Epoch [539/600], Training Loss: 1.7532, vgg Loss: 166.8857, L2 Loss: 0.0844, time: 102.2864\n",
      "Epoch [540/600], Training Loss: 1.6874, vgg Loss: 160.9711, L2 Loss: 0.0777, time: 102.2448\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [541/600], Training Loss: 1.8010, vgg Loss: 171.0112, L2 Loss: 0.0909, time: 102.2845\n",
      "Epoch [542/600], Training Loss: 1.7746, vgg Loss: 169.4372, L2 Loss: 0.0802, time: 102.4294\n",
      "Epoch [543/600], Training Loss: 1.7769, vgg Loss: 169.5301, L2 Loss: 0.0816, time: 105.6822\n",
      "Epoch [544/600], Training Loss: 1.7335, vgg Loss: 164.9791, L2 Loss: 0.0837, time: 107.1197\n",
      "Epoch [545/600], Training Loss: 1.7918, vgg Loss: 170.3321, L2 Loss: 0.0885, time: 111.0040\n",
      "Epoch [546/600], Training Loss: 1.8050, vgg Loss: 171.6841, L2 Loss: 0.0882, time: 107.5666\n",
      "Epoch [547/600], Training Loss: 1.7251, vgg Loss: 163.8823, L2 Loss: 0.0863, time: 119.9624\n",
      "Epoch [548/600], Training Loss: 1.7357, vgg Loss: 165.0745, L2 Loss: 0.0850, time: 135.9485\n",
      "Epoch [549/600], Training Loss: 1.7417, vgg Loss: 166.0554, L2 Loss: 0.0811, time: 116.8246\n",
      "Epoch [550/600], Training Loss: 1.7267, vgg Loss: 164.5173, L2 Loss: 0.0815, time: 103.5070\n",
      "Epoch [551/600], Training Loss: 1.7801, vgg Loss: 168.7897, L2 Loss: 0.0922, time: 103.3215\n",
      "Epoch [552/600], Training Loss: 1.7149, vgg Loss: 163.4485, L2 Loss: 0.0804, time: 103.2314\n",
      "Epoch [553/600], Training Loss: 1.7257, vgg Loss: 164.5454, L2 Loss: 0.0802, time: 103.2006\n",
      "Epoch [554/600], Training Loss: 1.7321, vgg Loss: 165.3137, L2 Loss: 0.0790, time: 103.1493\n",
      "Epoch [555/600], Training Loss: 1.7515, vgg Loss: 166.9686, L2 Loss: 0.0819, time: 103.1258\n",
      "Epoch [556/600], Training Loss: 1.7420, vgg Loss: 166.4597, L2 Loss: 0.0774, time: 102.7658\n",
      "Epoch [557/600], Training Loss: 1.7632, vgg Loss: 167.8902, L2 Loss: 0.0843, time: 102.3099\n",
      "Epoch [558/600], Training Loss: 1.7448, vgg Loss: 166.3735, L2 Loss: 0.0811, time: 102.5268\n",
      "Epoch [559/600], Training Loss: 1.7078, vgg Loss: 162.5362, L2 Loss: 0.0824, time: 102.3413\n",
      "Epoch [560/600], Training Loss: 1.7581, vgg Loss: 167.0603, L2 Loss: 0.0875, time: 109.8082\n",
      "Epoch [561/600], Training Loss: 1.7164, vgg Loss: 163.2992, L2 Loss: 0.0834, time: 112.1119\n",
      "Epoch [562/600], Training Loss: 1.7735, vgg Loss: 168.3327, L2 Loss: 0.0902, time: 112.6726\n",
      "Epoch [563/600], Training Loss: 1.7352, vgg Loss: 165.4128, L2 Loss: 0.0811, time: 112.4898\n",
      "Epoch [564/600], Training Loss: 1.7496, vgg Loss: 166.4834, L2 Loss: 0.0848, time: 112.6013\n",
      "Epoch [565/600], Training Loss: 1.7648, vgg Loss: 167.3745, L2 Loss: 0.0910, time: 112.2930\n",
      "Epoch [566/600], Training Loss: 1.7328, vgg Loss: 165.3766, L2 Loss: 0.0790, time: 112.8249\n",
      "Epoch [567/600], Training Loss: 1.7389, vgg Loss: 165.8098, L2 Loss: 0.0808, time: 112.5703\n",
      "Epoch [568/600], Training Loss: 1.7376, vgg Loss: 165.5235, L2 Loss: 0.0824, time: 112.1055\n",
      "Epoch [569/600], Training Loss: 1.7455, vgg Loss: 166.5544, L2 Loss: 0.0800, time: 112.3704\n",
      "Epoch [570/600], Training Loss: 1.7415, vgg Loss: 166.1985, L2 Loss: 0.0795, time: 114.1358\n",
      "Epoch [571/600], Training Loss: 1.7391, vgg Loss: 165.7511, L2 Loss: 0.0816, time: 116.3379\n",
      "Epoch [572/600], Training Loss: 1.7157, vgg Loss: 164.0933, L2 Loss: 0.0748, time: 106.6313\n",
      "Epoch [573/600], Training Loss: 1.7239, vgg Loss: 164.7214, L2 Loss: 0.0766, time: 103.3824\n",
      "Epoch [574/600], Training Loss: 1.7418, vgg Loss: 165.7230, L2 Loss: 0.0846, time: 103.2419\n",
      "Epoch [575/600], Training Loss: 1.7388, vgg Loss: 165.4218, L2 Loss: 0.0846, time: 103.2689\n",
      "Epoch [576/600], Training Loss: 1.6789, vgg Loss: 160.2626, L2 Loss: 0.0762, time: 103.4393\n",
      "Epoch [577/600], Training Loss: 1.7334, vgg Loss: 165.2909, L2 Loss: 0.0805, time: 103.1780\n",
      "Epoch [578/600], Training Loss: 1.6713, vgg Loss: 159.4668, L2 Loss: 0.0766, time: 102.1037\n",
      "Epoch [579/600], Training Loss: 1.7773, vgg Loss: 169.4116, L2 Loss: 0.0832, time: 102.1875\n",
      "Epoch [580/600], Training Loss: 1.7041, vgg Loss: 162.6546, L2 Loss: 0.0776, time: 102.1329\n",
      "Epoch [581/600], Training Loss: 1.7710, vgg Loss: 168.6106, L2 Loss: 0.0849, time: 102.2321\n",
      "Epoch [582/600], Training Loss: 1.7465, vgg Loss: 166.0367, L2 Loss: 0.0862, time: 102.2268\n",
      "Epoch [583/600], Training Loss: 1.7034, vgg Loss: 162.4839, L2 Loss: 0.0786, time: 102.1905\n",
      "Epoch [584/600], Training Loss: 1.7514, vgg Loss: 166.9460, L2 Loss: 0.0819, time: 103.0103\n",
      "Epoch [585/600], Training Loss: 1.6855, vgg Loss: 161.1863, L2 Loss: 0.0737, time: 119.2520\n",
      "Epoch [586/600], Training Loss: 1.7729, vgg Loss: 169.0255, L2 Loss: 0.0826, time: 112.7610\n",
      "Epoch [587/600], Training Loss: 1.7986, vgg Loss: 170.2019, L2 Loss: 0.0966, time: 111.6773\n",
      "Epoch [588/600], Training Loss: 1.7544, vgg Loss: 167.1197, L2 Loss: 0.0832, time: 111.8161\n",
      "Epoch [589/600], Training Loss: 1.7433, vgg Loss: 166.7226, L2 Loss: 0.0760, time: 111.5572\n",
      "Epoch [590/600], Training Loss: 1.7289, vgg Loss: 164.6346, L2 Loss: 0.0826, time: 112.3109\n",
      "Epoch [591/600], Training Loss: 1.7286, vgg Loss: 164.5983, L2 Loss: 0.0826, time: 111.9403\n",
      "Epoch [592/600], Training Loss: 1.7834, vgg Loss: 170.2589, L2 Loss: 0.0808, time: 112.4019\n",
      "Epoch [593/600], Training Loss: 1.7320, vgg Loss: 165.2283, L2 Loss: 0.0797, time: 111.9701\n",
      "Epoch [594/600], Training Loss: 1.6879, vgg Loss: 160.9796, L2 Loss: 0.0781, time: 111.4858\n",
      "Epoch [595/600], Training Loss: 1.7654, vgg Loss: 168.4109, L2 Loss: 0.0813, time: 111.7870\n",
      "Epoch [596/600], Training Loss: 1.6777, vgg Loss: 160.0638, L2 Loss: 0.0771, time: 111.8970\n",
      "Epoch [597/600], Training Loss: 1.7151, vgg Loss: 163.4008, L2 Loss: 0.0811, time: 118.7372\n",
      "Epoch [598/600], Training Loss: 1.7492, vgg Loss: 166.8251, L2 Loss: 0.0810, time: 141.3371\n",
      "Epoch [599/600], Training Loss: 1.7711, vgg Loss: 168.5579, L2 Loss: 0.0855, time: 136.0797\n",
      "Epoch [600/600], Training Loss: 1.7442, vgg Loss: 166.3086, L2 Loss: 0.0811, time: 141.9518\n",
      "--- 58223.8690 seconds ---\n"
     ]
    }
   ],
   "source": [
    "train_dataset = DatasetMultiple('rgb train/', 'rgb train/', '380 train a/', '640 train a/', True)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                           batch_size=16,\n",
    "                                           shuffle=True,\n",
    "                                           num_workers=4)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "imageFilter = MFFNet().to(device).float()\n",
    "\n",
    "# Initializing VGG16 model for perceptual loss\n",
    "VGG = Vgg16(requires_grad=False)\n",
    "VGG = VGG.to(device)\n",
    "\n",
    "\n",
    "num_epochs = 600\n",
    "learning_rate = 1e-4\n",
    "\n",
    "criterion_img = nn.MSELoss()\n",
    "criterion_vgg = nn.MSELoss()\n",
    "\n",
    "optimizer = torch.optim.Adam(imageFilter.parameters(), lr=learning_rate)\n",
    "total_step = len(train_loader)\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "previous_time = start_time\n",
    "for epoch in range(num_epochs):\n",
    "    loss_tol = 0\n",
    "    loss_tol_vgg  = 0\n",
    "    loss_tol_l2   = 0\n",
    "    \n",
    "    if epoch == 300:\n",
    "        learning_rate = 1e-5\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = learning_rate\n",
    "        \n",
    "    if epoch == 600:\n",
    "        learning_rate = 1e-6\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = learning_rate\n",
    "    \n",
    "    for i, im in enumerate(train_loader):\n",
    "        inputs = im[0].float().to(device)\n",
    "        target = im[1].float().to(device)\n",
    "        \n",
    "        outputs = imageFilter(inputs)\n",
    "        \n",
    "        loss_l2 = criterion_img( outputs, target )\n",
    "        \n",
    "        outputs_n = utils.normalize_ImageNet_stats(outputs)\n",
    "        target_n  = utils.normalize_ImageNet_stats(target)\n",
    "        \n",
    "        feature_o = VGG(outputs_n, 3)\n",
    "        feature_t = VGG(target_n, 3)\n",
    "        VGG_loss = []\n",
    "        for l in range(3+1):\n",
    "            VGG_loss.append( criterion_vgg(feature_o[l], feature_t[l]) )\n",
    "        \n",
    "        loss_vgg = sum(VGG_loss)\n",
    "        loss = loss_l2 + 0.01*loss_vgg\n",
    "    \n",
    "        loss_tol += loss.item()\n",
    "        \n",
    "        loss_tol_vgg  += loss_vgg\n",
    "        loss_tol_l2   += loss_l2\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    print ( 'Epoch [{}/{}], Training Loss: {:.4f}, vgg Loss: {:.4f}, L2 Loss: {:.4f}, time: {:.4f}' .format(epoch+1, num_epochs, loss_tol, loss_tol_vgg, loss_tol_l2, time.time() - previous_time) )\n",
    "    previous_time = time.time()\n",
    "    \n",
    "print(\"--- %0.4f seconds ---\" % (time.time() - start_time)) \n",
    "torch.save(imageFilter.state_dict(), 'MFF-net_all3_old.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "572c6474",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

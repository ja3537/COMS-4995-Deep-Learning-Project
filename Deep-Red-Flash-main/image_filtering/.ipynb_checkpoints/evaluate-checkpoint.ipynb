{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "import os\n",
    "import torchvision\n",
    "import torch\n",
    "from skimage import io, transform\n",
    "import numpy as np\n",
    "from torchvision import transforms, utils\n",
    "import torch.nn as nn\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torchvision.utils import save_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MFFNet(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MFFNet, self).__init__()\n",
    "        \n",
    "        self.conv1 = ConvLayer(4, 32, kernel_size=9, stride=1)\n",
    "        self.in1 = torch.nn.InstanceNorm2d(32, affine=True)\n",
    "        self.conv2 = ConvLayer(32, 64, kernel_size=3, stride=2)\n",
    "        self.in2 = torch.nn.InstanceNorm2d(64, affine=True)\n",
    "        self.conv3 = ConvLayer(64, 128, kernel_size=3, stride=2)\n",
    "        self.in3 = torch.nn.InstanceNorm2d(128, affine=True)\n",
    "        # Residual layers\n",
    "        self.res1 = ResidualBlock(128)\n",
    "        self.res2 = ResidualBlock(128)\n",
    "        self.res3 = ResidualBlock(128)\n",
    "        self.res4 = ResidualBlock(128)\n",
    "        self.res5 = ResidualBlock(128)\n",
    "        self.res6 = ResidualBlock(128)\n",
    "        self.res7 = ResidualBlock(128)\n",
    "        self.res8 = ResidualBlock(128)\n",
    "        self.res9 = ResidualBlock(128)\n",
    "        self.res10 = ResidualBlock(128)\n",
    "        self.res11 = ResidualBlock(128)\n",
    "        self.res12 = ResidualBlock(128)\n",
    "        self.res13 = ResidualBlock(128)\n",
    "        self.res14 = ResidualBlock(128)\n",
    "        self.res15 = ResidualBlock(128)\n",
    "        self.res16 = ResidualBlock(128)\n",
    "        \n",
    "        self.deconv1 = UpsampleConvLayer(128*2, 64, kernel_size=3, stride=1, upsample=2)\n",
    "        self.in4 = torch.nn.InstanceNorm2d(64, affine=True)\n",
    "        self.deconv2 = UpsampleConvLayer(64*2, 32, kernel_size=3, stride=1, upsample=2)\n",
    "        self.in5 = torch.nn.InstanceNorm2d(32, affine=True)\n",
    "        self.deconv3 = ConvLayer(32*2, 3, kernel_size=9, stride=1)\n",
    "\n",
    "        self.relu = torch.nn.ReLU()\n",
    "    \n",
    "    def forward(self, X):\n",
    "        o1 = self.relu(self.conv1(X))\n",
    "        o2 = self.relu(self.conv2(o1))\n",
    "        o3 = self.relu(self.conv3(o2))\n",
    "\n",
    "        y = self.res1(o3)\n",
    "        y = self.res2(y)\n",
    "        y = self.res3(y)\n",
    "        y = self.res4(y)\n",
    "        y = self.res5(y)\n",
    "        y = self.res6(y)\n",
    "        y = self.res7(y)\n",
    "        y = self.res8(y)\n",
    "        y = self.res9(y)\n",
    "        y = self.res10(y)\n",
    "        y = self.res11(y)\n",
    "        y = self.res12(y)\n",
    "        y = self.res13(y)\n",
    "        y = self.res14(y)\n",
    "        y = self.res15(y)\n",
    "        y = self.res16(y)\n",
    "        \n",
    "        in1 = torch.cat( (y, o3), 1 )\n",
    "        y = self.relu(self.deconv1(in1))\n",
    "        in2 = torch.cat( (y, o2), 1 )\n",
    "        y = self.relu(self.deconv2(in2))\n",
    "        in3 = torch.cat( (y, o1), 1 )\n",
    "        y = self.deconv3(in3)\n",
    "        \n",
    "        return y\n",
    "\n",
    "class ConvLayer(torch.nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride):\n",
    "        super(ConvLayer, self).__init__()\n",
    "        reflection_padding = kernel_size // 2\n",
    "        self.reflection_pad = torch.nn.ReflectionPad2d(reflection_padding)\n",
    "        self.conv2d = torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.reflection_pad(x)\n",
    "        out = self.conv2d(out)\n",
    "        return out\n",
    "\n",
    "class ResidualBlock(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, channels):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.conv1 = ConvLayer(channels, channels, kernel_size=3, stride=1)\n",
    "        self.in1 = torch.nn.InstanceNorm2d(channels, affine=True)\n",
    "        self.conv2 = ConvLayer(channels, channels, kernel_size=3, stride=1)\n",
    "        self.in2 = torch.nn.InstanceNorm2d(channels, affine=True)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        out = self.relu(self.conv1(x))\n",
    "        out = self.conv2(out)\n",
    "        out = out + residual\n",
    "        return out\n",
    "\n",
    "\n",
    "class UpsampleConvLayer(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride, upsample=None):\n",
    "        super(UpsampleConvLayer, self).__init__()\n",
    "        self.upsample = upsample\n",
    "        reflection_padding = kernel_size // 2\n",
    "        self.reflection_pad = torch.nn.ReflectionPad2d(reflection_padding)\n",
    "        self.conv2d = torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x_in = x\n",
    "        if self.upsample:\n",
    "            x_in = torch.nn.functional.interpolate(x_in, mode='nearest', scale_factor=self.upsample)\n",
    "        out = self.reflection_pad(x_in)\n",
    "        out = self.conv2d(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "imageFilter = MFFNet()\n",
    "\n",
    "model_name = 'MFF-net'\n",
    "imageFilter.load_state_dict( torch.load('../trained_model/%s.ckpt'%(model_name)) )\n",
    "imageFilter = imageFilter.to(device).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 1536, 2048])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_root = 'test'\n",
    "out_root = 'output'\n",
    "if not os.path.exists(out_root):\n",
    "    os.mkdir(out_root)\n",
    "\n",
    "for seq in range(1,2):#1,6\n",
    "    # input rgb image is obtained from demosaicing the raw (no other manipulation)\n",
    "    # saved in 16-bit TIFF image\n",
    "    file = ('rgb_%s.tiff' % (seq) )\n",
    "    filename = os.path.join( data_root, file )\n",
    "#     inputs = io.imread(filename) / 65535\n",
    "    inputs = io.imread(filename)\n",
    "    plt.imshow(inputs)\n",
    "\n",
    "    file = ('guide_%s.bmp' % (seq) )\n",
    "    filename = os.path.join( data_root, file )\n",
    "    guided = io.imread(filename) / 255\n",
    "\n",
    "    guided = guided[:,:,0]+guided[:,:,1]+guided[:,:,2]\n",
    "    inputs = (inputs*80)**0.4\n",
    "    inputs = np.concatenate((inputs, guided[:,:,None]), 2)\n",
    "    inputs = np.transpose(inputs,(2,0,1))\n",
    "    inputs = torch.from_numpy(inputs)\n",
    "    inputs = inputs[None,:,:,:].float()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        inputs = inputs.to(device) \n",
    "        outputs = imageFilter(inputs)\n",
    "    outputs[outputs>1] = 1\n",
    "    outputs[outputs<0] = 0    \n",
    "\n",
    "    # the parameter for color balance and brightness should be tuned for different scenes\n",
    "    outputs[0,0,:,:] = outputs[0,0,:,:]*1.1*1.5\n",
    "    outputs[0,1,:,:] = outputs[0,1,:,:]*1*1.5\n",
    "    outputs[0,2,:,:] = outputs[0,2,:,:]*1.5*1.5\n",
    "    \n",
    "    print(outputs.shape)\n",
    "\n",
    "#     save_image(outputs[0,:,:,:], '%s/out_%s.png' % (out_root, seq))\n",
    "#     save_image(inputs[0,0:3,:,:], '%s/inp_%s.png' % (out_root, seq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

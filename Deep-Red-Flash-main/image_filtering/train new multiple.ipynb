{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "95ef4dd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\adams\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\numpy\\_distributor_init.py:30: UserWarning: loaded more than 1 DLL from .libs:\n",
      "c:\\users\\adams\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\numpy\\.libs\\libopenblas.EL2C6PLE4ZYW3ECEVIV3OXXGRN2NRFM2.gfortran-win_amd64.dll\n",
      "c:\\users\\adams\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\numpy\\.libs\\libopenblas.GK7GX5KEQ4F6UYO3P26ULGBQYHGQO7J4.gfortran-win_amd64.dll\n",
      "  warnings.warn(\"loaded more than 1 DLL from .libs:\"\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function, division\n",
    "import os\n",
    "import torchvision\n",
    "import torch\n",
    "from skimage import io, transform\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "import torch.nn as nn\n",
    "from math import log10, pi\n",
    "import time\n",
    "\n",
    "import utils\n",
    "from datasetsMultiple import DatasetMultiple\n",
    "from vgg import Vgg16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "17d23fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MFFNet(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MFFNet, self).__init__()\n",
    "        \n",
    "        self.conv1 = ConvLayer(3, 32, kernel_size=9, stride=1)\n",
    "        self.in1 = torch.nn.InstanceNorm2d(32, affine=True)\n",
    "        self.conv2 = ConvLayer(32, 64, kernel_size=3, stride=2)\n",
    "        self.in2 = torch.nn.InstanceNorm2d(64, affine=True)\n",
    "        self.conv3 = ConvLayer(64, 128, kernel_size=3, stride=2)\n",
    "        self.in3 = torch.nn.InstanceNorm2d(128, affine=True)\n",
    "        \n",
    "        # Residual layers\n",
    "        self.res1 = ResidualBlock(128)\n",
    "        self.res2 = ResidualBlock(128)\n",
    "        self.res3 = ResidualBlock(128)\n",
    "        self.res4 = ResidualBlock(128)\n",
    "        self.res5 = ResidualBlock(128)\n",
    "        self.res6 = ResidualBlock(128)\n",
    "        self.res7 = ResidualBlock(128)\n",
    "        self.res8 = ResidualBlock(128)\n",
    "        self.res9 = ResidualBlock(128)\n",
    "        self.res10 = ResidualBlock(128)\n",
    "        self.res11 = ResidualBlock(128)\n",
    "        self.res12 = ResidualBlock(128)\n",
    "        self.res13 = ResidualBlock(128)\n",
    "        self.res14 = ResidualBlock(128)\n",
    "        self.res15 = ResidualBlock(128)\n",
    "        self.res16 = ResidualBlock(128)\n",
    "        \n",
    "        self.deconv1 = UpsampleConvLayer(128*2, 64, kernel_size=3, stride=1, upsample=2)\n",
    "        self.in4 = torch.nn.InstanceNorm2d(64, affine=True)\n",
    "        self.deconv2 = UpsampleConvLayer(64*2, 32, kernel_size=3, stride=1, upsample=2)\n",
    "        self.in5 = torch.nn.InstanceNorm2d(32, affine=True)\n",
    "        self.deconv3 = ConvLayer(32*2, 3, kernel_size=9, stride=1)\n",
    "\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        \n",
    "        # Attentions\n",
    "        self.att1 = AttentionBlock(128, 128, 64)\n",
    "        self.att2 = AttentionBlock(64, 64, 32)\n",
    "        self.att3 = AttentionBlock(32, 32, 16)\n",
    "\n",
    "    \n",
    "    def forward(self, X):\n",
    "        o1 = self.relu(self.conv1(X))\n",
    "        o2 = self.relu(self.conv2(o1))\n",
    "        o3 = self.relu(self.conv3(o2))\n",
    "\n",
    "        y = self.res1(o3)\n",
    "        y = self.res2(y)\n",
    "        y = self.res3(y)\n",
    "        y = self.res4(y)\n",
    "        y = self.res5(y)\n",
    "        y = self.res6(y)\n",
    "        y = self.res7(y)\n",
    "        y = self.res8(y)\n",
    "        y = self.res9(y)\n",
    "        y = self.res10(y)\n",
    "        y = self.res11(y)\n",
    "        y = self.res12(y)\n",
    "        y = self.res13(y)\n",
    "        y = self.res14(y)\n",
    "        y = self.res15(y)\n",
    "        y = self.res16(y)\n",
    "        \n",
    "        o3 = self.att1(y, o3)\n",
    "        in1 = torch.cat((y, o3), 1)\n",
    "        y = self.relu(self.deconv1(in1))\n",
    "\n",
    "        o2 = self.att2(y, o2)\n",
    "        in2 = torch.cat((y, o2), 1)\n",
    "        y = self.relu(self.deconv2(in2))\n",
    "\n",
    "        o1 = self.att3(y, o1)\n",
    "        in3 = torch.cat((y, o1), 1)\n",
    "        y = self.deconv3(in3)\n",
    "        \n",
    "        return y\n",
    "\n",
    "\n",
    "\n",
    "class ConvLayer(torch.nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride):\n",
    "        super(ConvLayer, self).__init__()\n",
    "        reflection_padding = kernel_size // 2\n",
    "        self.reflection_pad = torch.nn.ReflectionPad2d(reflection_padding)\n",
    "        self.conv2d = torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.reflection_pad(x)\n",
    "        out = self.conv2d(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "\n",
    "class ResidualBlock(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, channels):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.conv1 = ConvLayer(channels, channels, kernel_size=3, stride=1)\n",
    "        self.in1 = torch.nn.InstanceNorm2d(channels, affine=True)\n",
    "        self.conv2 = ConvLayer(channels, channels, kernel_size=3, stride=1)\n",
    "        self.in2 = torch.nn.InstanceNorm2d(channels, affine=True)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        out = self.relu(self.conv1(x))\n",
    "        out = self.conv2(out)\n",
    "        out = out + residual\n",
    "        return out\n",
    "\n",
    "\n",
    "\n",
    "class UpsampleConvLayer(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride, upsample=None):\n",
    "        super(UpsampleConvLayer, self).__init__()\n",
    "        self.upsample = upsample\n",
    "        reflection_padding = kernel_size // 2\n",
    "        self.reflection_pad = torch.nn.ReflectionPad2d(reflection_padding)\n",
    "        self.conv2d = torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x_in = x\n",
    "        if self.upsample:\n",
    "            x_in = torch.nn.functional.interpolate(x_in, mode='nearest', scale_factor=self.upsample)\n",
    "        out = self.reflection_pad(x_in)\n",
    "        out = self.conv2d(out)\n",
    "        return out\n",
    "    \n",
    "\n",
    "\n",
    "class AttentionBlock(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, Fg, Fl, Fint):\n",
    "        super(AttentionBlock, self).__init__()               \n",
    "        self.Wg = nn.Sequential(nn.Conv2d(Fg, Fint, kernel_size=1, stride=1, padding=0), nn.BatchNorm2d(Fint))\n",
    "        self.Wx = nn.Sequential(nn.Conv2d(Fl, Fint, kernel_size=1, stride=1, padding=0), nn.BatchNorm2d(Fint))\n",
    "        self.relu = nn.ReLU()\n",
    "        self.psi = nn.Sequential(nn.Conv2d(Fint, 1, kernel_size=1, stride=1, padding=0), nn.BatchNorm2d(1), nn.Sigmoid())\n",
    "        \n",
    "    def forward(self, g, x):\n",
    "        g1 = self.Wg(g)   \n",
    "        x1 = self.Wx(x)\n",
    "        y = self.relu(g1 + x1)  \n",
    "        y = self.psi(y)\n",
    "        return torch.mul(y, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e6d04290",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/1200], Training Loss: 307.7778, vgg Loss: 23608.3438, L2 Loss: 71.6943, time: 7.4991\n",
      "Epoch [2/1200], Training Loss: 86.6120, vgg Loss: 7045.3936, L2 Loss: 16.1580, time: 6.1299\n",
      "Epoch [3/1200], Training Loss: 43.7618, vgg Loss: 3827.6487, L2 Loss: 5.4853, time: 6.1572\n",
      "Epoch [4/1200], Training Loss: 24.1681, vgg Loss: 2045.1240, L2 Loss: 3.7168, time: 6.3490\n",
      "Epoch [5/1200], Training Loss: 20.4471, vgg Loss: 1726.8345, L2 Loss: 3.1788, time: 6.8984\n",
      "Epoch [6/1200], Training Loss: 13.7848, vgg Loss: 1174.2084, L2 Loss: 2.0428, time: 7.6637\n",
      "Epoch [7/1200], Training Loss: 11.3603, vgg Loss: 975.6278, L2 Loss: 1.6040, time: 6.7759\n",
      "Epoch [8/1200], Training Loss: 8.7645, vgg Loss: 755.9742, L2 Loss: 1.2047, time: 6.5011\n",
      "Epoch [9/1200], Training Loss: 6.9145, vgg Loss: 604.1609, L2 Loss: 0.8729, time: 6.2949\n",
      "Epoch [10/1200], Training Loss: 7.7816, vgg Loss: 680.0017, L2 Loss: 0.9816, time: 6.6113\n",
      "Epoch [11/1200], Training Loss: 6.1232, vgg Loss: 531.7476, L2 Loss: 0.8058, time: 6.6988\n",
      "Epoch [12/1200], Training Loss: 6.5560, vgg Loss: 572.4742, L2 Loss: 0.8313, time: 6.6494\n",
      "Epoch [13/1200], Training Loss: 6.0907, vgg Loss: 528.5280, L2 Loss: 0.8055, time: 6.8846\n",
      "Epoch [14/1200], Training Loss: 5.5293, vgg Loss: 481.1113, L2 Loss: 0.7182, time: 6.9274\n",
      "Epoch [15/1200], Training Loss: 4.9037, vgg Loss: 426.5338, L2 Loss: 0.6384, time: 6.9135\n",
      "Epoch [16/1200], Training Loss: 4.7654, vgg Loss: 413.7975, L2 Loss: 0.6274, time: 7.0832\n",
      "Epoch [17/1200], Training Loss: 5.1843, vgg Loss: 451.6863, L2 Loss: 0.6675, time: 6.7480\n",
      "Epoch [18/1200], Training Loss: 5.4084, vgg Loss: 467.8833, L2 Loss: 0.7296, time: 7.1339\n",
      "Epoch [19/1200], Training Loss: 4.3227, vgg Loss: 378.8094, L2 Loss: 0.5346, time: 7.1409\n",
      "Epoch [20/1200], Training Loss: 4.8498, vgg Loss: 425.8308, L2 Loss: 0.5915, time: 7.2859\n",
      "Epoch [21/1200], Training Loss: 5.3600, vgg Loss: 469.7563, L2 Loss: 0.6624, time: 7.5595\n",
      "Epoch [22/1200], Training Loss: 4.3356, vgg Loss: 380.1182, L2 Loss: 0.5344, time: 7.6803\n",
      "Epoch [23/1200], Training Loss: 4.4071, vgg Loss: 384.2296, L2 Loss: 0.5648, time: 8.0792\n",
      "Epoch [24/1200], Training Loss: 3.2831, vgg Loss: 290.8155, L2 Loss: 0.3749, time: 7.7986\n",
      "Epoch [25/1200], Training Loss: 4.4086, vgg Loss: 387.2803, L2 Loss: 0.5358, time: 7.4020\n",
      "Epoch [26/1200], Training Loss: 4.1426, vgg Loss: 364.0580, L2 Loss: 0.5020, time: 7.4674\n",
      "Epoch [27/1200], Training Loss: 4.0499, vgg Loss: 354.3450, L2 Loss: 0.5064, time: 8.2837\n",
      "Epoch [28/1200], Training Loss: 4.4429, vgg Loss: 385.9964, L2 Loss: 0.5830, time: 7.9397\n",
      "Epoch [29/1200], Training Loss: 4.1815, vgg Loss: 365.9491, L2 Loss: 0.5220, time: 7.8780\n",
      "Epoch [30/1200], Training Loss: 3.8374, vgg Loss: 337.0349, L2 Loss: 0.4671, time: 7.8222\n",
      "Epoch [31/1200], Training Loss: 3.4161, vgg Loss: 302.1539, L2 Loss: 0.3946, time: 7.5481\n",
      "Epoch [32/1200], Training Loss: 3.1501, vgg Loss: 274.8218, L2 Loss: 0.4018, time: 7.3883\n",
      "Epoch [33/1200], Training Loss: 3.6582, vgg Loss: 321.7780, L2 Loss: 0.4404, time: 7.5439\n",
      "Epoch [34/1200], Training Loss: 3.4734, vgg Loss: 303.0349, L2 Loss: 0.4430, time: 7.1316\n",
      "Epoch [35/1200], Training Loss: 3.3432, vgg Loss: 295.5039, L2 Loss: 0.3882, time: 7.0847\n",
      "Epoch [36/1200], Training Loss: 3.0223, vgg Loss: 268.8211, L2 Loss: 0.3340, time: 7.3717\n",
      "Epoch [37/1200], Training Loss: 3.4615, vgg Loss: 304.7661, L2 Loss: 0.4138, time: 7.8206\n",
      "Epoch [38/1200], Training Loss: 3.3092, vgg Loss: 290.2457, L2 Loss: 0.4068, time: 8.0071\n",
      "Epoch [39/1200], Training Loss: 3.1228, vgg Loss: 274.5585, L2 Loss: 0.3772, time: 7.5210\n",
      "Epoch [40/1200], Training Loss: 2.7112, vgg Loss: 238.6702, L2 Loss: 0.3245, time: 7.5471\n",
      "Epoch [41/1200], Training Loss: 3.1301, vgg Loss: 278.5294, L2 Loss: 0.3448, time: 7.6020\n",
      "Epoch [42/1200], Training Loss: 2.8599, vgg Loss: 252.4521, L2 Loss: 0.3353, time: 7.3186\n",
      "Epoch [43/1200], Training Loss: 2.9729, vgg Loss: 263.4562, L2 Loss: 0.3384, time: 7.6063\n",
      "Epoch [44/1200], Training Loss: 2.7377, vgg Loss: 244.9941, L2 Loss: 0.2878, time: 7.7581\n",
      "Epoch [45/1200], Training Loss: 2.7574, vgg Loss: 244.1604, L2 Loss: 0.3158, time: 7.6845\n",
      "Epoch [46/1200], Training Loss: 2.7662, vgg Loss: 244.9688, L2 Loss: 0.3165, time: 7.5771\n",
      "Epoch [47/1200], Training Loss: 2.9310, vgg Loss: 259.5379, L2 Loss: 0.3356, time: 7.7213\n",
      "Epoch [48/1200], Training Loss: 3.2481, vgg Loss: 290.2141, L2 Loss: 0.3459, time: 7.5870\n",
      "Epoch [49/1200], Training Loss: 2.4856, vgg Loss: 217.3206, L2 Loss: 0.3123, time: 7.4928\n",
      "Epoch [50/1200], Training Loss: 2.9395, vgg Loss: 260.1551, L2 Loss: 0.3380, time: 7.6618\n",
      "Epoch [51/1200], Training Loss: 2.6366, vgg Loss: 230.9226, L2 Loss: 0.3274, time: 7.4707\n",
      "Epoch [52/1200], Training Loss: 2.7176, vgg Loss: 239.6935, L2 Loss: 0.3206, time: 7.5545\n",
      "Epoch [53/1200], Training Loss: 3.2201, vgg Loss: 272.8901, L2 Loss: 0.4912, time: 7.6601\n",
      "Epoch [54/1200], Training Loss: 2.8200, vgg Loss: 234.3990, L2 Loss: 0.4760, time: 7.6261\n",
      "Epoch [55/1200], Training Loss: 2.9259, vgg Loss: 250.0133, L2 Loss: 0.4257, time: 7.6033\n",
      "Epoch [56/1200], Training Loss: 2.5850, vgg Loss: 217.7747, L2 Loss: 0.4073, time: 7.6039\n",
      "Epoch [57/1200], Training Loss: 2.5811, vgg Loss: 226.1064, L2 Loss: 0.3200, time: 7.6877\n",
      "Epoch [58/1200], Training Loss: 2.9648, vgg Loss: 258.8230, L2 Loss: 0.3766, time: 7.5215\n",
      "Epoch [59/1200], Training Loss: 2.1075, vgg Loss: 185.7480, L2 Loss: 0.2500, time: 7.6514\n",
      "Epoch [60/1200], Training Loss: 2.4580, vgg Loss: 219.2375, L2 Loss: 0.2657, time: 7.6411\n",
      "Epoch [61/1200], Training Loss: 2.7197, vgg Loss: 239.7138, L2 Loss: 0.3226, time: 8.6228\n",
      "Epoch [62/1200], Training Loss: 2.3576, vgg Loss: 207.9014, L2 Loss: 0.2786, time: 7.9860\n",
      "Epoch [63/1200], Training Loss: 2.6179, vgg Loss: 222.5628, L2 Loss: 0.3923, time: 7.7100\n",
      "Epoch [64/1200], Training Loss: 2.5059, vgg Loss: 217.2088, L2 Loss: 0.3338, time: 7.6966\n",
      "Epoch [65/1200], Training Loss: 2.6426, vgg Loss: 229.2370, L2 Loss: 0.3502, time: 7.7212\n",
      "Epoch [66/1200], Training Loss: 2.1754, vgg Loss: 192.4887, L2 Loss: 0.2505, time: 7.7370\n",
      "Epoch [67/1200], Training Loss: 2.3602, vgg Loss: 210.9246, L2 Loss: 0.2509, time: 7.6370\n",
      "Epoch [68/1200], Training Loss: 2.1755, vgg Loss: 194.2611, L2 Loss: 0.2329, time: 7.4004\n",
      "Epoch [69/1200], Training Loss: 2.1383, vgg Loss: 190.2103, L2 Loss: 0.2362, time: 7.6753\n",
      "Epoch [70/1200], Training Loss: 2.1455, vgg Loss: 190.9612, L2 Loss: 0.2359, time: 7.5877\n",
      "Epoch [71/1200], Training Loss: 2.3994, vgg Loss: 214.8271, L2 Loss: 0.2512, time: 7.3609\n",
      "Epoch [72/1200], Training Loss: 1.9788, vgg Loss: 178.0627, L2 Loss: 0.1982, time: 7.7825\n",
      "Epoch [73/1200], Training Loss: 2.3850, vgg Loss: 211.7121, L2 Loss: 0.2679, time: 7.6662\n",
      "Epoch [74/1200], Training Loss: 2.3965, vgg Loss: 208.3005, L2 Loss: 0.3135, time: 7.9633\n",
      "Epoch [75/1200], Training Loss: 1.9998, vgg Loss: 175.8749, L2 Loss: 0.2410, time: 7.9381\n",
      "Epoch [76/1200], Training Loss: 2.0671, vgg Loss: 182.4280, L2 Loss: 0.2428, time: 7.8341\n",
      "Epoch [77/1200], Training Loss: 2.0530, vgg Loss: 183.0848, L2 Loss: 0.2221, time: 7.7441\n",
      "Epoch [78/1200], Training Loss: 1.9870, vgg Loss: 171.3468, L2 Loss: 0.2735, time: 7.6414\n",
      "Epoch [79/1200], Training Loss: 2.2597, vgg Loss: 196.5004, L2 Loss: 0.2947, time: 7.7215\n",
      "Epoch [80/1200], Training Loss: 2.0743, vgg Loss: 180.3619, L2 Loss: 0.2707, time: 7.6816\n",
      "Epoch [81/1200], Training Loss: 1.9923, vgg Loss: 168.7745, L2 Loss: 0.3046, time: 7.7643\n",
      "Epoch [82/1200], Training Loss: 2.1458, vgg Loss: 184.9778, L2 Loss: 0.2961, time: 7.8329\n",
      "Epoch [83/1200], Training Loss: 2.4234, vgg Loss: 204.5387, L2 Loss: 0.3780, time: 7.6907\n",
      "Epoch [84/1200], Training Loss: 2.7062, vgg Loss: 234.9517, L2 Loss: 0.3567, time: 7.8019\n",
      "Epoch [85/1200], Training Loss: 2.0712, vgg Loss: 182.9803, L2 Loss: 0.2414, time: 7.7461\n",
      "Epoch [86/1200], Training Loss: 2.1340, vgg Loss: 187.0257, L2 Loss: 0.2638, time: 7.8117\n",
      "Epoch [87/1200], Training Loss: 2.4395, vgg Loss: 211.4641, L2 Loss: 0.3248, time: 7.6574\n",
      "Epoch [88/1200], Training Loss: 2.1878, vgg Loss: 191.2120, L2 Loss: 0.2757, time: 7.9038\n",
      "Epoch [89/1200], Training Loss: 2.1805, vgg Loss: 190.9102, L2 Loss: 0.2714, time: 7.7757\n",
      "Epoch [90/1200], Training Loss: 2.6125, vgg Loss: 227.4156, L2 Loss: 0.3384, time: 7.7343\n",
      "Epoch [91/1200], Training Loss: 2.8856, vgg Loss: 237.9886, L2 Loss: 0.5057, time: 7.7305\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [92/1200], Training Loss: 2.4300, vgg Loss: 205.9497, L2 Loss: 0.3705, time: 7.8561\n",
      "Epoch [93/1200], Training Loss: 2.4145, vgg Loss: 205.5609, L2 Loss: 0.3589, time: 7.7754\n",
      "Epoch [94/1200], Training Loss: 2.2502, vgg Loss: 190.7835, L2 Loss: 0.3424, time: 7.8321\n",
      "Epoch [95/1200], Training Loss: 1.9611, vgg Loss: 164.9713, L2 Loss: 0.3114, time: 8.3739\n",
      "Epoch [96/1200], Training Loss: 2.0868, vgg Loss: 171.9582, L2 Loss: 0.3672, time: 8.0797\n",
      "Epoch [97/1200], Training Loss: 2.0947, vgg Loss: 175.8123, L2 Loss: 0.3366, time: 8.2531\n",
      "Epoch [98/1200], Training Loss: 2.1551, vgg Loss: 185.5701, L2 Loss: 0.2994, time: 8.0496\n",
      "Epoch [99/1200], Training Loss: 2.4991, vgg Loss: 201.0182, L2 Loss: 0.4889, time: 7.7776\n",
      "Epoch [100/1200], Training Loss: 2.3438, vgg Loss: 198.0789, L2 Loss: 0.3630, time: 7.8165\n",
      "Epoch [101/1200], Training Loss: 1.9846, vgg Loss: 166.3882, L2 Loss: 0.3207, time: 7.7216\n",
      "Epoch [102/1200], Training Loss: 2.0223, vgg Loss: 171.0741, L2 Loss: 0.3116, time: 7.7301\n",
      "Epoch [103/1200], Training Loss: 1.9401, vgg Loss: 167.6045, L2 Loss: 0.2641, time: 7.8413\n",
      "Epoch [104/1200], Training Loss: 1.9232, vgg Loss: 170.3948, L2 Loss: 0.2192, time: 7.8776\n",
      "Epoch [105/1200], Training Loss: 1.7368, vgg Loss: 152.4695, L2 Loss: 0.2121, time: 7.6778\n",
      "Epoch [106/1200], Training Loss: 1.6512, vgg Loss: 148.4081, L2 Loss: 0.1672, time: 7.7173\n",
      "Epoch [107/1200], Training Loss: 2.2813, vgg Loss: 200.4830, L2 Loss: 0.2764, time: 7.7529\n",
      "Epoch [108/1200], Training Loss: 1.7257, vgg Loss: 150.6014, L2 Loss: 0.2197, time: 7.8663\n",
      "Epoch [109/1200], Training Loss: 1.7163, vgg Loss: 153.9634, L2 Loss: 0.1766, time: 7.7221\n",
      "Epoch [110/1200], Training Loss: 1.7232, vgg Loss: 153.2100, L2 Loss: 0.1911, time: 7.6917\n",
      "Epoch [111/1200], Training Loss: 1.8680, vgg Loss: 165.4584, L2 Loss: 0.2134, time: 7.8349\n",
      "Epoch [112/1200], Training Loss: 1.7774, vgg Loss: 160.3770, L2 Loss: 0.1736, time: 7.7585\n",
      "Epoch [113/1200], Training Loss: 1.7948, vgg Loss: 158.8035, L2 Loss: 0.2068, time: 7.7585\n",
      "Epoch [114/1200], Training Loss: 1.9167, vgg Loss: 168.0165, L2 Loss: 0.2365, time: 7.8151\n",
      "Epoch [115/1200], Training Loss: 1.7750, vgg Loss: 157.8726, L2 Loss: 0.1962, time: 7.7949\n",
      "Epoch [116/1200], Training Loss: 1.7233, vgg Loss: 154.0101, L2 Loss: 0.1832, time: 7.8351\n",
      "Epoch [117/1200], Training Loss: 1.7068, vgg Loss: 151.5546, L2 Loss: 0.1912, time: 7.7199\n",
      "Epoch [118/1200], Training Loss: 1.6286, vgg Loss: 145.3149, L2 Loss: 0.1754, time: 7.7716\n",
      "Epoch [119/1200], Training Loss: 1.6065, vgg Loss: 144.0201, L2 Loss: 0.1663, time: 7.8053\n",
      "Epoch [120/1200], Training Loss: 1.4644, vgg Loss: 131.2488, L2 Loss: 0.1520, time: 7.8175\n",
      "Epoch [121/1200], Training Loss: 1.5657, vgg Loss: 138.5094, L2 Loss: 0.1806, time: 7.8530\n",
      "Epoch [122/1200], Training Loss: 1.5948, vgg Loss: 141.7854, L2 Loss: 0.1769, time: 7.9158\n",
      "Epoch [123/1200], Training Loss: 1.7311, vgg Loss: 155.1249, L2 Loss: 0.1799, time: 7.6929\n",
      "Epoch [124/1200], Training Loss: 1.6041, vgg Loss: 143.9946, L2 Loss: 0.1642, time: 8.2291\n",
      "Epoch [125/1200], Training Loss: 1.5117, vgg Loss: 135.0102, L2 Loss: 0.1615, time: 8.3189\n",
      "Epoch [126/1200], Training Loss: 1.5373, vgg Loss: 135.7095, L2 Loss: 0.1802, time: 8.0051\n",
      "Epoch [127/1200], Training Loss: 1.8282, vgg Loss: 157.0463, L2 Loss: 0.2578, time: 7.8329\n",
      "Epoch [128/1200], Training Loss: 1.5121, vgg Loss: 134.4232, L2 Loss: 0.1678, time: 7.4836\n",
      "Epoch [129/1200], Training Loss: 1.6914, vgg Loss: 152.7886, L2 Loss: 0.1635, time: 7.8947\n",
      "Epoch [130/1200], Training Loss: 1.6973, vgg Loss: 153.3036, L2 Loss: 0.1643, time: 7.8590\n",
      "Epoch [131/1200], Training Loss: 1.5411, vgg Loss: 136.5424, L2 Loss: 0.1757, time: 7.7770\n",
      "Epoch [132/1200], Training Loss: 1.6137, vgg Loss: 141.4140, L2 Loss: 0.1996, time: 7.7599\n",
      "Epoch [133/1200], Training Loss: 1.5066, vgg Loss: 136.5879, L2 Loss: 0.1407, time: 7.8261\n",
      "Epoch [134/1200], Training Loss: 1.5823, vgg Loss: 140.4674, L2 Loss: 0.1776, time: 7.8527\n",
      "Epoch [135/1200], Training Loss: 1.7052, vgg Loss: 149.9086, L2 Loss: 0.2061, time: 7.7130\n",
      "Epoch [136/1200], Training Loss: 1.5498, vgg Loss: 137.2706, L2 Loss: 0.1771, time: 7.7851\n",
      "Epoch [137/1200], Training Loss: 1.7089, vgg Loss: 147.8523, L2 Loss: 0.2304, time: 7.8338\n",
      "Epoch [138/1200], Training Loss: 1.5606, vgg Loss: 136.6012, L2 Loss: 0.1945, time: 7.5217\n",
      "Epoch [139/1200], Training Loss: 1.6534, vgg Loss: 146.5563, L2 Loss: 0.1879, time: 7.8499\n",
      "Epoch [140/1200], Training Loss: 1.5719, vgg Loss: 137.9699, L2 Loss: 0.1922, time: 7.8515\n",
      "Epoch [141/1200], Training Loss: 2.2369, vgg Loss: 189.0609, L2 Loss: 0.3463, time: 7.6892\n",
      "Epoch [142/1200], Training Loss: 1.8736, vgg Loss: 157.4059, L2 Loss: 0.2996, time: 7.7462\n",
      "Epoch [143/1200], Training Loss: 1.7039, vgg Loss: 147.9224, L2 Loss: 0.2247, time: 7.7910\n",
      "Epoch [144/1200], Training Loss: 1.7563, vgg Loss: 155.9135, L2 Loss: 0.1971, time: 7.7273\n",
      "Epoch [145/1200], Training Loss: 1.4997, vgg Loss: 134.9408, L2 Loss: 0.1503, time: 7.8321\n",
      "Epoch [146/1200], Training Loss: 1.5035, vgg Loss: 136.2870, L2 Loss: 0.1406, time: 7.9658\n",
      "Epoch [147/1200], Training Loss: 1.4081, vgg Loss: 128.8174, L2 Loss: 0.1199, time: 8.1328\n",
      "Epoch [148/1200], Training Loss: 1.3546, vgg Loss: 122.0245, L2 Loss: 0.1344, time: 7.7370\n",
      "Epoch [149/1200], Training Loss: 1.9037, vgg Loss: 163.8664, L2 Loss: 0.2650, time: 7.8230\n",
      "Epoch [150/1200], Training Loss: 1.7969, vgg Loss: 156.7130, L2 Loss: 0.2298, time: 7.8543\n",
      "Epoch [151/1200], Training Loss: 1.6473, vgg Loss: 142.3075, L2 Loss: 0.2243, time: 8.0436\n",
      "Epoch [152/1200], Training Loss: 1.5059, vgg Loss: 133.7989, L2 Loss: 0.1679, time: 7.9304\n",
      "Epoch [153/1200], Training Loss: 1.5763, vgg Loss: 138.9500, L2 Loss: 0.1868, time: 7.8600\n",
      "Epoch [154/1200], Training Loss: 1.4449, vgg Loss: 130.2836, L2 Loss: 0.1421, time: 7.9958\n",
      "Epoch [155/1200], Training Loss: 1.3869, vgg Loss: 123.8276, L2 Loss: 0.1486, time: 7.8106\n",
      "Epoch [156/1200], Training Loss: 1.8041, vgg Loss: 155.1102, L2 Loss: 0.2530, time: 7.8083\n",
      "Epoch [157/1200], Training Loss: 1.4463, vgg Loss: 129.2116, L2 Loss: 0.1542, time: 7.5139\n",
      "Epoch [158/1200], Training Loss: 1.3417, vgg Loss: 119.1438, L2 Loss: 0.1502, time: 7.8497\n",
      "Epoch [159/1200], Training Loss: 1.3579, vgg Loss: 117.7364, L2 Loss: 0.1806, time: 7.7471\n",
      "Epoch [160/1200], Training Loss: 1.4133, vgg Loss: 123.0096, L2 Loss: 0.1832, time: 7.5435\n",
      "Epoch [161/1200], Training Loss: 1.4497, vgg Loss: 129.3315, L2 Loss: 0.1564, time: 7.8403\n",
      "Epoch [162/1200], Training Loss: 1.4182, vgg Loss: 125.2825, L2 Loss: 0.1654, time: 7.8462\n",
      "Epoch [163/1200], Training Loss: 1.5184, vgg Loss: 132.3883, L2 Loss: 0.1945, time: 7.7353\n",
      "Epoch [164/1200], Training Loss: 1.4557, vgg Loss: 129.8986, L2 Loss: 0.1567, time: 7.8090\n",
      "Epoch [165/1200], Training Loss: 1.7980, vgg Loss: 153.5135, L2 Loss: 0.2628, time: 7.9250\n",
      "Epoch [166/1200], Training Loss: 1.5307, vgg Loss: 137.5706, L2 Loss: 0.1550, time: 7.8038\n",
      "Epoch [167/1200], Training Loss: 1.2983, vgg Loss: 118.0785, L2 Loss: 0.1175, time: 7.6725\n",
      "Epoch [168/1200], Training Loss: 1.3046, vgg Loss: 118.3574, L2 Loss: 0.1210, time: 7.9235\n",
      "Epoch [169/1200], Training Loss: 1.3731, vgg Loss: 124.3569, L2 Loss: 0.1295, time: 7.8220\n",
      "Epoch [170/1200], Training Loss: 1.2856, vgg Loss: 116.3596, L2 Loss: 0.1220, time: 7.8068\n",
      "Epoch [171/1200], Training Loss: 1.2446, vgg Loss: 112.0723, L2 Loss: 0.1239, time: 7.7314\n",
      "Epoch [172/1200], Training Loss: 1.2040, vgg Loss: 108.6685, L2 Loss: 0.1173, time: 7.8271\n",
      "Epoch [173/1200], Training Loss: 1.3735, vgg Loss: 123.3866, L2 Loss: 0.1396, time: 8.3197\n",
      "Epoch [174/1200], Training Loss: 1.5323, vgg Loss: 132.9186, L2 Loss: 0.2031, time: 8.0710\n",
      "Epoch [175/1200], Training Loss: 1.7067, vgg Loss: 145.2638, L2 Loss: 0.2541, time: 7.9468\n",
      "Epoch [176/1200], Training Loss: 1.3800, vgg Loss: 123.3540, L2 Loss: 0.1465, time: 7.8452\n",
      "Epoch [177/1200], Training Loss: 1.3383, vgg Loss: 119.5411, L2 Loss: 0.1429, time: 7.8309\n",
      "Epoch [178/1200], Training Loss: 1.4732, vgg Loss: 132.0615, L2 Loss: 0.1526, time: 7.7867\n",
      "Epoch [179/1200], Training Loss: 1.2348, vgg Loss: 110.5816, L2 Loss: 0.1290, time: 7.8481\n",
      "Epoch [180/1200], Training Loss: 1.7179, vgg Loss: 148.0160, L2 Loss: 0.2378, time: 8.1619\n",
      "Epoch [181/1200], Training Loss: 1.5631, vgg Loss: 135.6431, L2 Loss: 0.2067, time: 8.0116\n",
      "Epoch [182/1200], Training Loss: 1.4220, vgg Loss: 125.5001, L2 Loss: 0.1670, time: 7.9033\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [183/1200], Training Loss: 1.5759, vgg Loss: 131.7879, L2 Loss: 0.2581, time: 8.0510\n",
      "Epoch [184/1200], Training Loss: 2.1162, vgg Loss: 171.9152, L2 Loss: 0.3970, time: 7.7402\n",
      "Epoch [185/1200], Training Loss: 3.1633, vgg Loss: 236.9584, L2 Loss: 0.7937, time: 8.0591\n",
      "Epoch [186/1200], Training Loss: 2.4741, vgg Loss: 196.0651, L2 Loss: 0.5134, time: 7.8739\n",
      "Epoch [187/1200], Training Loss: 1.8193, vgg Loss: 153.7144, L2 Loss: 0.2821, time: 7.7645\n",
      "Epoch [188/1200], Training Loss: 1.5868, vgg Loss: 138.6737, L2 Loss: 0.2001, time: 7.8483\n",
      "Epoch [189/1200], Training Loss: 1.5700, vgg Loss: 132.6532, L2 Loss: 0.2435, time: 7.7781\n",
      "Epoch [190/1200], Training Loss: 1.5951, vgg Loss: 134.2108, L2 Loss: 0.2530, time: 7.7836\n",
      "Epoch [191/1200], Training Loss: 1.4756, vgg Loss: 128.8780, L2 Loss: 0.1868, time: 7.8752\n",
      "Epoch [192/1200], Training Loss: 1.6616, vgg Loss: 141.3735, L2 Loss: 0.2479, time: 7.8172\n",
      "Epoch [193/1200], Training Loss: 1.5239, vgg Loss: 124.9854, L2 Loss: 0.2740, time: 7.7928\n",
      "Epoch [194/1200], Training Loss: 1.4333, vgg Loss: 122.3962, L2 Loss: 0.2094, time: 7.9058\n",
      "Epoch [195/1200], Training Loss: 1.5407, vgg Loss: 136.9741, L2 Loss: 0.1710, time: 8.2000\n",
      "Epoch [196/1200], Training Loss: 1.6127, vgg Loss: 140.1423, L2 Loss: 0.2113, time: 7.7858\n",
      "Epoch [197/1200], Training Loss: 1.3631, vgg Loss: 120.7805, L2 Loss: 0.1553, time: 7.8143\n",
      "Epoch [198/1200], Training Loss: 1.4047, vgg Loss: 122.6151, L2 Loss: 0.1786, time: 7.6166\n",
      "Epoch [199/1200], Training Loss: 1.3111, vgg Loss: 114.8335, L2 Loss: 0.1628, time: 7.8924\n",
      "Epoch [200/1200], Training Loss: 2.0268, vgg Loss: 167.8545, L2 Loss: 0.3482, time: 7.6987\n",
      "Epoch [201/1200], Training Loss: 1.3745, vgg Loss: 122.8192, L2 Loss: 0.1463, time: 7.7482\n",
      "Epoch [202/1200], Training Loss: 1.4225, vgg Loss: 128.0473, L2 Loss: 0.1421, time: 7.6931\n",
      "Epoch [203/1200], Training Loss: 1.3727, vgg Loss: 123.8288, L2 Loss: 0.1344, time: 7.6884\n",
      "Epoch [204/1200], Training Loss: 1.4848, vgg Loss: 131.2180, L2 Loss: 0.1726, time: 7.6447\n",
      "Epoch [205/1200], Training Loss: 1.3826, vgg Loss: 122.4085, L2 Loss: 0.1585, time: 7.4351\n",
      "Epoch [206/1200], Training Loss: 1.3666, vgg Loss: 117.5877, L2 Loss: 0.1908, time: 7.4442\n",
      "Epoch [207/1200], Training Loss: 1.2387, vgg Loss: 108.0330, L2 Loss: 0.1584, time: 7.8021\n",
      "Epoch [208/1200], Training Loss: 1.3099, vgg Loss: 112.4599, L2 Loss: 0.1853, time: 7.9090\n",
      "Epoch [209/1200], Training Loss: 1.1976, vgg Loss: 106.1048, L2 Loss: 0.1365, time: 7.8181\n",
      "Epoch [210/1200], Training Loss: 1.4987, vgg Loss: 135.0272, L2 Loss: 0.1485, time: 7.7830\n",
      "Epoch [211/1200], Training Loss: 1.3419, vgg Loss: 118.0460, L2 Loss: 0.1615, time: 7.8512\n",
      "Epoch [212/1200], Training Loss: 1.2228, vgg Loss: 109.5401, L2 Loss: 0.1274, time: 7.5217\n",
      "Epoch [213/1200], Training Loss: 1.2844, vgg Loss: 115.0818, L2 Loss: 0.1336, time: 7.6343\n",
      "Epoch [214/1200], Training Loss: 1.5542, vgg Loss: 132.5872, L2 Loss: 0.2283, time: 7.4295\n",
      "Epoch [215/1200], Training Loss: 1.4586, vgg Loss: 126.5807, L2 Loss: 0.1927, time: 7.5987\n",
      "Epoch [216/1200], Training Loss: 1.3616, vgg Loss: 120.1855, L2 Loss: 0.1598, time: 7.5868\n",
      "Epoch [217/1200], Training Loss: 1.4162, vgg Loss: 125.8170, L2 Loss: 0.1580, time: 7.4855\n",
      "Epoch [218/1200], Training Loss: 1.6171, vgg Loss: 140.3107, L2 Loss: 0.2140, time: 7.5235\n",
      "Epoch [219/1200], Training Loss: 1.2491, vgg Loss: 109.7879, L2 Loss: 0.1512, time: 7.6312\n",
      "Epoch [220/1200], Training Loss: 1.3745, vgg Loss: 120.8541, L2 Loss: 0.1659, time: 7.5832\n",
      "Epoch [221/1200], Training Loss: 1.2870, vgg Loss: 113.8457, L2 Loss: 0.1486, time: 7.5447\n",
      "Epoch [222/1200], Training Loss: 1.4307, vgg Loss: 123.8008, L2 Loss: 0.1927, time: 7.4942\n",
      "Epoch [223/1200], Training Loss: 1.5716, vgg Loss: 138.4042, L2 Loss: 0.1876, time: 7.5060\n",
      "Epoch [224/1200], Training Loss: 1.2936, vgg Loss: 112.0390, L2 Loss: 0.1732, time: 7.6460\n",
      "Epoch [225/1200], Training Loss: 1.6437, vgg Loss: 138.7080, L2 Loss: 0.2566, time: 7.3166\n",
      "Epoch [226/1200], Training Loss: 1.5864, vgg Loss: 131.8122, L2 Loss: 0.2683, time: 7.4565\n",
      "Epoch [227/1200], Training Loss: 1.7335, vgg Loss: 144.4297, L2 Loss: 0.2892, time: 7.5253\n",
      "Epoch [228/1200], Training Loss: 1.7985, vgg Loss: 147.4088, L2 Loss: 0.3244, time: 7.6931\n",
      "Epoch [229/1200], Training Loss: 1.5591, vgg Loss: 134.2918, L2 Loss: 0.2161, time: 7.9464\n",
      "Epoch [230/1200], Training Loss: 1.7415, vgg Loss: 144.3062, L2 Loss: 0.2984, time: 7.7303\n",
      "Epoch [231/1200], Training Loss: 1.3147, vgg Loss: 116.2744, L2 Loss: 0.1519, time: 7.9236\n",
      "Epoch [232/1200], Training Loss: 1.4204, vgg Loss: 123.6528, L2 Loss: 0.1839, time: 7.7622\n",
      "Epoch [233/1200], Training Loss: 1.3191, vgg Loss: 117.6552, L2 Loss: 0.1426, time: 7.4736\n",
      "Epoch [234/1200], Training Loss: 1.4159, vgg Loss: 123.5922, L2 Loss: 0.1800, time: 7.6945\n",
      "Epoch [235/1200], Training Loss: 1.1324, vgg Loss: 100.3885, L2 Loss: 0.1285, time: 7.4912\n",
      "Epoch [236/1200], Training Loss: 1.2305, vgg Loss: 110.6107, L2 Loss: 0.1244, time: 7.6674\n",
      "Epoch [237/1200], Training Loss: 1.2591, vgg Loss: 110.0050, L2 Loss: 0.1590, time: 7.2442\n",
      "Epoch [238/1200], Training Loss: 1.2603, vgg Loss: 113.4305, L2 Loss: 0.1260, time: 7.5989\n",
      "Epoch [239/1200], Training Loss: 1.1964, vgg Loss: 107.7865, L2 Loss: 0.1186, time: 7.6827\n",
      "Epoch [240/1200], Training Loss: 1.2938, vgg Loss: 117.3104, L2 Loss: 0.1207, time: 7.6021\n",
      "Epoch [241/1200], Training Loss: 1.1962, vgg Loss: 107.5979, L2 Loss: 0.1203, time: 7.5001\n",
      "Epoch [242/1200], Training Loss: 1.0519, vgg Loss: 93.2677, L2 Loss: 0.1192, time: 7.5347\n",
      "Epoch [243/1200], Training Loss: 1.1024, vgg Loss: 99.8463, L2 Loss: 0.1039, time: 7.6828\n",
      "Epoch [244/1200], Training Loss: 1.1802, vgg Loss: 107.9979, L2 Loss: 0.1002, time: 7.3770\n",
      "Epoch [245/1200], Training Loss: 1.0314, vgg Loss: 90.5807, L2 Loss: 0.1256, time: 7.4942\n",
      "Epoch [246/1200], Training Loss: 1.1713, vgg Loss: 100.9068, L2 Loss: 0.1622, time: 7.5476\n",
      "Epoch [247/1200], Training Loss: 1.1300, vgg Loss: 101.0295, L2 Loss: 0.1197, time: 7.5841\n",
      "Epoch [248/1200], Training Loss: 1.1182, vgg Loss: 101.4864, L2 Loss: 0.1033, time: 7.4241\n",
      "Epoch [249/1200], Training Loss: 1.1007, vgg Loss: 99.4535, L2 Loss: 0.1062, time: 7.4420\n",
      "Epoch [250/1200], Training Loss: 1.3879, vgg Loss: 117.5841, L2 Loss: 0.2121, time: 7.5154\n",
      "Epoch [251/1200], Training Loss: 1.2847, vgg Loss: 110.7219, L2 Loss: 0.1775, time: 7.7161\n",
      "Epoch [252/1200], Training Loss: 1.3119, vgg Loss: 113.3853, L2 Loss: 0.1780, time: 7.3045\n",
      "Epoch [253/1200], Training Loss: 1.7823, vgg Loss: 148.4679, L2 Loss: 0.2976, time: 7.4910\n",
      "Epoch [254/1200], Training Loss: 1.4623, vgg Loss: 123.9947, L2 Loss: 0.2224, time: 7.6612\n",
      "Epoch [255/1200], Training Loss: 1.2602, vgg Loss: 109.5016, L2 Loss: 0.1652, time: 7.7073\n",
      "Epoch [256/1200], Training Loss: 1.5767, vgg Loss: 129.3628, L2 Loss: 0.2830, time: 7.6346\n",
      "Epoch [257/1200], Training Loss: 1.8159, vgg Loss: 143.1839, L2 Loss: 0.3840, time: 7.5006\n",
      "Epoch [258/1200], Training Loss: 1.3394, vgg Loss: 117.4058, L2 Loss: 0.1654, time: 7.6295\n",
      "Epoch [259/1200], Training Loss: 1.3462, vgg Loss: 114.2832, L2 Loss: 0.2033, time: 8.3703\n",
      "Epoch [260/1200], Training Loss: 2.0936, vgg Loss: 166.9806, L2 Loss: 0.4238, time: 7.6306\n",
      "Epoch [261/1200], Training Loss: 2.8441, vgg Loss: 218.6242, L2 Loss: 0.6579, time: 7.8492\n",
      "Epoch [262/1200], Training Loss: 2.2225, vgg Loss: 188.2091, L2 Loss: 0.3404, time: 8.0418\n",
      "Epoch [263/1200], Training Loss: 1.8689, vgg Loss: 151.7888, L2 Loss: 0.3510, time: 9.1937\n",
      "Epoch [264/1200], Training Loss: 1.5971, vgg Loss: 135.0257, L2 Loss: 0.2468, time: 8.4933\n",
      "Epoch [265/1200], Training Loss: 1.2794, vgg Loss: 113.4738, L2 Loss: 0.1447, time: 8.2315\n",
      "Epoch [266/1200], Training Loss: 1.3261, vgg Loss: 117.4836, L2 Loss: 0.1512, time: 8.1094\n",
      "Epoch [267/1200], Training Loss: 1.3465, vgg Loss: 116.4462, L2 Loss: 0.1820, time: 7.8831\n",
      "Epoch [268/1200], Training Loss: 1.2893, vgg Loss: 109.5577, L2 Loss: 0.1937, time: 8.0641\n",
      "Epoch [269/1200], Training Loss: 1.2656, vgg Loss: 110.4783, L2 Loss: 0.1608, time: 8.1360\n",
      "Epoch [270/1200], Training Loss: 1.2337, vgg Loss: 106.3727, L2 Loss: 0.1700, time: 8.2089\n",
      "Epoch [271/1200], Training Loss: 1.2141, vgg Loss: 105.8804, L2 Loss: 0.1553, time: 8.0155\n",
      "Epoch [272/1200], Training Loss: 1.2594, vgg Loss: 113.3830, L2 Loss: 0.1256, time: 8.0135\n",
      "Epoch [273/1200], Training Loss: 1.0083, vgg Loss: 91.7893, L2 Loss: 0.0904, time: 8.0228\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [274/1200], Training Loss: 1.0448, vgg Loss: 94.4585, L2 Loss: 0.1002, time: 8.0232\n",
      "Epoch [275/1200], Training Loss: 1.0461, vgg Loss: 94.6049, L2 Loss: 0.1000, time: 8.0470\n",
      "Epoch [276/1200], Training Loss: 1.1792, vgg Loss: 105.5041, L2 Loss: 0.1241, time: 8.1004\n",
      "Epoch [277/1200], Training Loss: 1.0449, vgg Loss: 93.3711, L2 Loss: 0.1112, time: 7.8586\n",
      "Epoch [278/1200], Training Loss: 0.8887, vgg Loss: 80.2407, L2 Loss: 0.0863, time: 8.1460\n",
      "Epoch [279/1200], Training Loss: 0.9788, vgg Loss: 87.8370, L2 Loss: 0.1004, time: 7.9041\n",
      "Epoch [280/1200], Training Loss: 1.0342, vgg Loss: 93.5136, L2 Loss: 0.0990, time: 8.0524\n",
      "Epoch [281/1200], Training Loss: 1.0637, vgg Loss: 96.2908, L2 Loss: 0.1008, time: 8.0873\n",
      "Epoch [282/1200], Training Loss: 0.9364, vgg Loss: 85.4315, L2 Loss: 0.0821, time: 7.8765\n",
      "Epoch [283/1200], Training Loss: 1.1473, vgg Loss: 103.2082, L2 Loss: 0.1152, time: 8.0931\n",
      "Epoch [284/1200], Training Loss: 1.0709, vgg Loss: 98.2114, L2 Loss: 0.0888, time: 8.1169\n",
      "Epoch [285/1200], Training Loss: 0.9993, vgg Loss: 91.0340, L2 Loss: 0.0890, time: 8.0050\n",
      "Epoch [286/1200], Training Loss: 1.0410, vgg Loss: 95.3886, L2 Loss: 0.0871, time: 8.0384\n",
      "Epoch [287/1200], Training Loss: 1.0929, vgg Loss: 96.6396, L2 Loss: 0.1265, time: 8.0284\n",
      "Epoch [288/1200], Training Loss: 1.0608, vgg Loss: 95.1865, L2 Loss: 0.1089, time: 8.0953\n",
      "Epoch [289/1200], Training Loss: 0.9738, vgg Loss: 88.8204, L2 Loss: 0.0856, time: 8.0945\n",
      "Epoch [290/1200], Training Loss: 1.1502, vgg Loss: 102.1982, L2 Loss: 0.1282, time: 7.8971\n",
      "Epoch [291/1200], Training Loss: 0.9679, vgg Loss: 88.1983, L2 Loss: 0.0859, time: 8.2612\n",
      "Epoch [292/1200], Training Loss: 1.0436, vgg Loss: 94.9464, L2 Loss: 0.0941, time: 8.0623\n",
      "Epoch [293/1200], Training Loss: 1.1300, vgg Loss: 99.6006, L2 Loss: 0.1340, time: 8.0483\n",
      "Epoch [294/1200], Training Loss: 1.0648, vgg Loss: 95.5928, L2 Loss: 0.1089, time: 7.9882\n",
      "Epoch [295/1200], Training Loss: 0.9596, vgg Loss: 86.2925, L2 Loss: 0.0966, time: 8.6085\n",
      "Epoch [296/1200], Training Loss: 1.0027, vgg Loss: 89.4495, L2 Loss: 0.1082, time: 8.1284\n",
      "Epoch [297/1200], Training Loss: 1.3125, vgg Loss: 110.7387, L2 Loss: 0.2052, time: 7.9920\n",
      "Epoch [298/1200], Training Loss: 1.1375, vgg Loss: 100.1964, L2 Loss: 0.1355, time: 8.5806\n",
      "Epoch [299/1200], Training Loss: 1.4512, vgg Loss: 120.7478, L2 Loss: 0.2437, time: 9.2612\n",
      "Epoch [300/1200], Training Loss: 1.2294, vgg Loss: 106.4588, L2 Loss: 0.1648, time: 8.3730\n",
      "Epoch [301/1200], Training Loss: 1.0609, vgg Loss: 92.0315, L2 Loss: 0.1406, time: 8.5661\n",
      "Epoch [302/1200], Training Loss: 0.9493, vgg Loss: 86.1294, L2 Loss: 0.0880, time: 8.4070\n",
      "Epoch [303/1200], Training Loss: 1.0999, vgg Loss: 96.8571, L2 Loss: 0.1314, time: 8.1268\n",
      "Epoch [304/1200], Training Loss: 1.2297, vgg Loss: 103.2059, L2 Loss: 0.1976, time: 8.4593\n",
      "Epoch [305/1200], Training Loss: 0.9904, vgg Loss: 87.4059, L2 Loss: 0.1163, time: 8.7003\n",
      "Epoch [306/1200], Training Loss: 1.0916, vgg Loss: 97.3775, L2 Loss: 0.1178, time: 8.0293\n",
      "Epoch [307/1200], Training Loss: 1.2068, vgg Loss: 107.0251, L2 Loss: 0.1365, time: 7.8065\n",
      "Epoch [308/1200], Training Loss: 1.1504, vgg Loss: 99.9746, L2 Loss: 0.1507, time: 7.9275\n",
      "Epoch [309/1200], Training Loss: 1.1626, vgg Loss: 101.3709, L2 Loss: 0.1489, time: 8.5182\n",
      "Epoch [310/1200], Training Loss: 1.0965, vgg Loss: 96.4021, L2 Loss: 0.1324, time: 8.3920\n",
      "Epoch [311/1200], Training Loss: 0.9410, vgg Loss: 84.2597, L2 Loss: 0.0984, time: 8.5632\n",
      "Epoch [312/1200], Training Loss: 1.1533, vgg Loss: 101.5555, L2 Loss: 0.1378, time: 8.3756\n",
      "Epoch [313/1200], Training Loss: 1.1762, vgg Loss: 101.1733, L2 Loss: 0.1644, time: 8.6518\n",
      "Epoch [314/1200], Training Loss: 1.2386, vgg Loss: 105.0356, L2 Loss: 0.1882, time: 8.1009\n",
      "Epoch [315/1200], Training Loss: 1.2472, vgg Loss: 105.2517, L2 Loss: 0.1946, time: 8.3697\n",
      "Epoch [316/1200], Training Loss: 1.2596, vgg Loss: 108.6262, L2 Loss: 0.1733, time: 8.0756\n",
      "Epoch [317/1200], Training Loss: 1.2724, vgg Loss: 107.4523, L2 Loss: 0.1979, time: 8.5651\n",
      "Epoch [318/1200], Training Loss: 1.3610, vgg Loss: 113.5236, L2 Loss: 0.2257, time: 8.0992\n",
      "Epoch [319/1200], Training Loss: 1.4345, vgg Loss: 123.6147, L2 Loss: 0.1983, time: 8.0274\n",
      "Epoch [320/1200], Training Loss: 1.2897, vgg Loss: 111.4412, L2 Loss: 0.1753, time: 8.3877\n",
      "Epoch [321/1200], Training Loss: 1.1335, vgg Loss: 100.5355, L2 Loss: 0.1282, time: 8.2853\n",
      "Epoch [322/1200], Training Loss: 1.2068, vgg Loss: 107.5981, L2 Loss: 0.1308, time: 8.2026\n",
      "Epoch [323/1200], Training Loss: 1.2636, vgg Loss: 109.6698, L2 Loss: 0.1670, time: 8.2392\n",
      "Epoch [324/1200], Training Loss: 1.0518, vgg Loss: 94.4087, L2 Loss: 0.1077, time: 8.1496\n",
      "Epoch [325/1200], Training Loss: 0.9697, vgg Loss: 88.1001, L2 Loss: 0.0887, time: 7.6800\n",
      "Epoch [326/1200], Training Loss: 1.1422, vgg Loss: 101.6444, L2 Loss: 0.1258, time: 7.8944\n",
      "Epoch [327/1200], Training Loss: 1.0387, vgg Loss: 92.1300, L2 Loss: 0.1174, time: 8.8506\n",
      "Epoch [328/1200], Training Loss: 1.0579, vgg Loss: 93.1078, L2 Loss: 0.1269, time: 8.4069\n",
      "Epoch [329/1200], Training Loss: 1.1331, vgg Loss: 99.0209, L2 Loss: 0.1429, time: 8.8426\n",
      "Epoch [330/1200], Training Loss: 1.0610, vgg Loss: 94.5441, L2 Loss: 0.1156, time: 9.0012\n",
      "Epoch [331/1200], Training Loss: 0.9943, vgg Loss: 86.7462, L2 Loss: 0.1268, time: 8.4726\n",
      "Epoch [332/1200], Training Loss: 1.0479, vgg Loss: 93.3937, L2 Loss: 0.1140, time: 8.7593\n",
      "Epoch [333/1200], Training Loss: 1.0665, vgg Loss: 95.9552, L2 Loss: 0.1069, time: 8.7144\n",
      "Epoch [334/1200], Training Loss: 1.0108, vgg Loss: 88.4252, L2 Loss: 0.1265, time: 9.4528\n",
      "Epoch [335/1200], Training Loss: 1.0127, vgg Loss: 91.4974, L2 Loss: 0.0978, time: 9.5510\n",
      "Epoch [336/1200], Training Loss: 1.3106, vgg Loss: 115.3961, L2 Loss: 0.1567, time: 9.7756\n",
      "Epoch [337/1200], Training Loss: 1.0504, vgg Loss: 94.3295, L2 Loss: 0.1071, time: 9.7953\n",
      "Epoch [338/1200], Training Loss: 1.0964, vgg Loss: 96.5296, L2 Loss: 0.1311, time: 10.1188\n",
      "Epoch [339/1200], Training Loss: 1.1935, vgg Loss: 105.6499, L2 Loss: 0.1370, time: 10.1627\n",
      "Epoch [340/1200], Training Loss: 1.4443, vgg Loss: 118.8323, L2 Loss: 0.2560, time: 9.7621\n",
      "Epoch [341/1200], Training Loss: 1.3794, vgg Loss: 115.5233, L2 Loss: 0.2242, time: 9.4893\n",
      "Epoch [342/1200], Training Loss: 1.1068, vgg Loss: 97.7293, L2 Loss: 0.1295, time: 8.6703\n",
      "Epoch [343/1200], Training Loss: 1.1976, vgg Loss: 102.6082, L2 Loss: 0.1715, time: 10.1255\n",
      "Epoch [344/1200], Training Loss: 1.6221, vgg Loss: 136.1704, L2 Loss: 0.2604, time: 9.4263\n",
      "Epoch [345/1200], Training Loss: 1.1824, vgg Loss: 100.9920, L2 Loss: 0.1725, time: 9.2399\n",
      "Epoch [346/1200], Training Loss: 1.0780, vgg Loss: 93.5811, L2 Loss: 0.1421, time: 7.6359\n",
      "Epoch [347/1200], Training Loss: 0.9316, vgg Loss: 83.5165, L2 Loss: 0.0964, time: 7.7327\n",
      "Epoch [348/1200], Training Loss: 0.9836, vgg Loss: 89.5880, L2 Loss: 0.0877, time: 7.3915\n",
      "Epoch [349/1200], Training Loss: 0.9078, vgg Loss: 83.0675, L2 Loss: 0.0772, time: 8.3273\n",
      "Epoch [350/1200], Training Loss: 0.8376, vgg Loss: 74.8334, L2 Loss: 0.0892, time: 8.9539\n",
      "Epoch [351/1200], Training Loss: 0.9175, vgg Loss: 80.6245, L2 Loss: 0.1113, time: 8.8515\n",
      "Epoch [352/1200], Training Loss: 1.0679, vgg Loss: 90.6559, L2 Loss: 0.1614, time: 8.5287\n",
      "Epoch [353/1200], Training Loss: 0.9033, vgg Loss: 79.2623, L2 Loss: 0.1107, time: 8.4735\n",
      "Epoch [354/1200], Training Loss: 0.9820, vgg Loss: 86.3928, L2 Loss: 0.1181, time: 7.8178\n",
      "Epoch [355/1200], Training Loss: 0.9745, vgg Loss: 87.6058, L2 Loss: 0.0984, time: 8.0558\n",
      "Epoch [356/1200], Training Loss: 0.9633, vgg Loss: 87.6500, L2 Loss: 0.0868, time: 8.0766\n",
      "Epoch [357/1200], Training Loss: 0.9499, vgg Loss: 84.1293, L2 Loss: 0.1086, time: 7.9841\n",
      "Epoch [358/1200], Training Loss: 0.9590, vgg Loss: 84.1802, L2 Loss: 0.1172, time: 8.1424\n",
      "Epoch [359/1200], Training Loss: 1.1971, vgg Loss: 103.4458, L2 Loss: 0.1626, time: 8.0371\n",
      "Epoch [360/1200], Training Loss: 1.2546, vgg Loss: 106.3834, L2 Loss: 0.1908, time: 8.1522\n",
      "Epoch [361/1200], Training Loss: 1.2113, vgg Loss: 103.7296, L2 Loss: 0.1740, time: 8.1087\n",
      "Epoch [362/1200], Training Loss: 1.1213, vgg Loss: 97.9210, L2 Loss: 0.1421, time: 7.9700\n",
      "Epoch [363/1200], Training Loss: 0.8491, vgg Loss: 77.2674, L2 Loss: 0.0764, time: 7.9952\n",
      "Epoch [364/1200], Training Loss: 0.8911, vgg Loss: 81.1275, L2 Loss: 0.0798, time: 8.4781\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [365/1200], Training Loss: 0.8295, vgg Loss: 75.1429, L2 Loss: 0.0781, time: 8.4286\n",
      "Epoch [366/1200], Training Loss: 0.8872, vgg Loss: 80.3351, L2 Loss: 0.0839, time: 8.3138\n",
      "Epoch [367/1200], Training Loss: 1.2090, vgg Loss: 105.0205, L2 Loss: 0.1588, time: 8.8643\n",
      "Epoch [368/1200], Training Loss: 0.9991, vgg Loss: 90.4870, L2 Loss: 0.0942, time: 8.6421\n",
      "Epoch [369/1200], Training Loss: 1.0593, vgg Loss: 93.8334, L2 Loss: 0.1210, time: 10.4343\n",
      "Epoch [370/1200], Training Loss: 1.1343, vgg Loss: 98.0773, L2 Loss: 0.1535, time: 13.1623\n",
      "Epoch [371/1200], Training Loss: 1.3912, vgg Loss: 115.8058, L2 Loss: 0.2331, time: 13.7559\n",
      "Epoch [372/1200], Training Loss: 1.3262, vgg Loss: 110.5332, L2 Loss: 0.2209, time: 13.4620\n",
      "Epoch [373/1200], Training Loss: 1.2247, vgg Loss: 102.7617, L2 Loss: 0.1971, time: 13.2043\n",
      "Epoch [374/1200], Training Loss: 1.1538, vgg Loss: 100.5476, L2 Loss: 0.1483, time: 13.2895\n",
      "Epoch [375/1200], Training Loss: 1.1749, vgg Loss: 102.0527, L2 Loss: 0.1543, time: 13.4363\n",
      "Epoch [376/1200], Training Loss: 1.0294, vgg Loss: 90.0271, L2 Loss: 0.1291, time: 14.2077\n",
      "Epoch [377/1200], Training Loss: 1.0412, vgg Loss: 90.5698, L2 Loss: 0.1355, time: 13.4246\n",
      "Epoch [378/1200], Training Loss: 0.9358, vgg Loss: 84.6331, L2 Loss: 0.0894, time: 13.2233\n",
      "Epoch [379/1200], Training Loss: 0.8397, vgg Loss: 76.5261, L2 Loss: 0.0745, time: 12.6846\n",
      "Epoch [380/1200], Training Loss: 0.9134, vgg Loss: 82.7838, L2 Loss: 0.0855, time: 12.8272\n",
      "Epoch [381/1200], Training Loss: 0.8895, vgg Loss: 81.2047, L2 Loss: 0.0775, time: 13.5596\n",
      "Epoch [382/1200], Training Loss: 0.9334, vgg Loss: 84.6440, L2 Loss: 0.0870, time: 13.5456\n",
      "Epoch [383/1200], Training Loss: 0.9466, vgg Loss: 84.5094, L2 Loss: 0.1015, time: 13.1717\n",
      "Epoch [384/1200], Training Loss: 0.9338, vgg Loss: 84.3437, L2 Loss: 0.0903, time: 12.7192\n",
      "Epoch [385/1200], Training Loss: 0.9427, vgg Loss: 85.5855, L2 Loss: 0.0868, time: 12.6941\n",
      "Epoch [386/1200], Training Loss: 1.0048, vgg Loss: 90.8309, L2 Loss: 0.0965, time: 12.6483\n",
      "Epoch [387/1200], Training Loss: 1.0131, vgg Loss: 88.7443, L2 Loss: 0.1257, time: 12.6811\n",
      "Epoch [388/1200], Training Loss: 1.1865, vgg Loss: 94.6658, L2 Loss: 0.2398, time: 12.7998\n",
      "Epoch [389/1200], Training Loss: 1.2333, vgg Loss: 106.1114, L2 Loss: 0.1722, time: 13.4326\n",
      "Epoch [390/1200], Training Loss: 0.9953, vgg Loss: 87.6526, L2 Loss: 0.1188, time: 13.5695\n",
      "Epoch [391/1200], Training Loss: 0.8951, vgg Loss: 79.9655, L2 Loss: 0.0954, time: 12.6729\n",
      "Epoch [392/1200], Training Loss: 0.9997, vgg Loss: 89.6815, L2 Loss: 0.1028, time: 13.2923\n",
      "Epoch [393/1200], Training Loss: 1.1489, vgg Loss: 100.7056, L2 Loss: 0.1418, time: 12.8737\n",
      "Epoch [394/1200], Training Loss: 0.9412, vgg Loss: 84.3938, L2 Loss: 0.0972, time: 12.8849\n",
      "Epoch [395/1200], Training Loss: 0.9340, vgg Loss: 82.0189, L2 Loss: 0.1138, time: 12.9517\n",
      "Epoch [396/1200], Training Loss: 0.8621, vgg Loss: 76.2231, L2 Loss: 0.0998, time: 12.9429\n",
      "Epoch [397/1200], Training Loss: 1.0121, vgg Loss: 88.3158, L2 Loss: 0.1290, time: 12.9908\n",
      "Epoch [398/1200], Training Loss: 1.1149, vgg Loss: 89.1237, L2 Loss: 0.2236, time: 12.9888\n",
      "Epoch [399/1200], Training Loss: 1.1624, vgg Loss: 94.3351, L2 Loss: 0.2190, time: 13.0188\n",
      "Epoch [400/1200], Training Loss: 1.1865, vgg Loss: 92.5815, L2 Loss: 0.2607, time: 12.9706\n",
      "Epoch [401/1200], Training Loss: 0.9918, vgg Loss: 78.6848, L2 Loss: 0.2049, time: 12.9464\n",
      "Epoch [402/1200], Training Loss: 0.9646, vgg Loss: 84.6615, L2 Loss: 0.1180, time: 13.3199\n",
      "Epoch [403/1200], Training Loss: 0.9137, vgg Loss: 81.6917, L2 Loss: 0.0968, time: 12.6670\n",
      "Epoch [404/1200], Training Loss: 0.8511, vgg Loss: 74.7985, L2 Loss: 0.1031, time: 12.6694\n",
      "Epoch [405/1200], Training Loss: 0.8767, vgg Loss: 77.7367, L2 Loss: 0.0993, time: 12.7450\n",
      "Epoch [406/1200], Training Loss: 0.9232, vgg Loss: 81.9617, L2 Loss: 0.1036, time: 14.0180\n",
      "Epoch [407/1200], Training Loss: 0.8488, vgg Loss: 76.4582, L2 Loss: 0.0842, time: 13.8695\n",
      "Epoch [408/1200], Training Loss: 1.0313, vgg Loss: 87.7311, L2 Loss: 0.1540, time: 13.5913\n",
      "Epoch [409/1200], Training Loss: 0.9920, vgg Loss: 82.9241, L2 Loss: 0.1628, time: 13.5305\n",
      "Epoch [410/1200], Training Loss: 0.9186, vgg Loss: 78.8482, L2 Loss: 0.1301, time: 13.7785\n",
      "Epoch [411/1200], Training Loss: 1.0916, vgg Loss: 94.8994, L2 Loss: 0.1426, time: 13.4613\n",
      "Epoch [412/1200], Training Loss: 1.1625, vgg Loss: 99.2246, L2 Loss: 0.1702, time: 13.2941\n",
      "Epoch [413/1200], Training Loss: 1.7927, vgg Loss: 139.4404, L2 Loss: 0.3983, time: 14.1056\n",
      "Epoch [414/1200], Training Loss: 1.5314, vgg Loss: 125.0824, L2 Loss: 0.2806, time: 13.7464\n",
      "Epoch [415/1200], Training Loss: 1.9381, vgg Loss: 156.8357, L2 Loss: 0.3697, time: 12.7514\n",
      "Epoch [416/1200], Training Loss: 1.7481, vgg Loss: 136.3636, L2 Loss: 0.3844, time: 12.5464\n",
      "Epoch [417/1200], Training Loss: 1.1784, vgg Loss: 102.4052, L2 Loss: 0.1544, time: 12.5097\n",
      "Epoch [418/1200], Training Loss: 1.0488, vgg Loss: 92.9906, L2 Loss: 0.1189, time: 12.7963\n",
      "Epoch [419/1200], Training Loss: 1.1160, vgg Loss: 99.0845, L2 Loss: 0.1251, time: 15.0752\n",
      "Epoch [420/1200], Training Loss: 0.9066, vgg Loss: 79.6169, L2 Loss: 0.1104, time: 14.7357\n",
      "Epoch [421/1200], Training Loss: 0.8718, vgg Loss: 79.7027, L2 Loss: 0.0748, time: 14.2304\n",
      "Epoch [422/1200], Training Loss: 0.8427, vgg Loss: 77.2785, L2 Loss: 0.0699, time: 14.0222\n",
      "Epoch [423/1200], Training Loss: 0.8717, vgg Loss: 79.2586, L2 Loss: 0.0791, time: 14.2639\n",
      "Epoch [424/1200], Training Loss: 0.7764, vgg Loss: 70.5056, L2 Loss: 0.0713, time: 13.8786\n",
      "Epoch [425/1200], Training Loss: 0.8643, vgg Loss: 78.6721, L2 Loss: 0.0776, time: 14.0912\n",
      "Epoch [426/1200], Training Loss: 0.8676, vgg Loss: 78.3669, L2 Loss: 0.0840, time: 14.0396\n",
      "Epoch [427/1200], Training Loss: 0.8549, vgg Loss: 78.1467, L2 Loss: 0.0734, time: 14.4634\n",
      "Epoch [428/1200], Training Loss: 0.9587, vgg Loss: 86.8576, L2 Loss: 0.0901, time: 13.2385\n",
      "Epoch [429/1200], Training Loss: 0.7847, vgg Loss: 71.0246, L2 Loss: 0.0744, time: 13.6230\n",
      "Epoch [430/1200], Training Loss: 0.8631, vgg Loss: 78.7793, L2 Loss: 0.0753, time: 13.3330\n",
      "Epoch [431/1200], Training Loss: 0.8834, vgg Loss: 76.5734, L2 Loss: 0.1177, time: 13.8562\n",
      "Epoch [432/1200], Training Loss: 0.8477, vgg Loss: 74.9860, L2 Loss: 0.0978, time: 13.7591\n",
      "Epoch [433/1200], Training Loss: 1.0701, vgg Loss: 96.4597, L2 Loss: 0.1055, time: 12.6461\n",
      "Epoch [434/1200], Training Loss: 0.8637, vgg Loss: 77.8638, L2 Loss: 0.0850, time: 13.7034\n",
      "Epoch [435/1200], Training Loss: 0.8093, vgg Loss: 73.8393, L2 Loss: 0.0709, time: 14.4579\n",
      "Epoch [436/1200], Training Loss: 0.8093, vgg Loss: 72.9681, L2 Loss: 0.0796, time: 13.3743\n",
      "Epoch [437/1200], Training Loss: 0.8896, vgg Loss: 81.1027, L2 Loss: 0.0785, time: 12.8033\n",
      "Epoch [438/1200], Training Loss: 0.8493, vgg Loss: 75.9405, L2 Loss: 0.0899, time: 12.6704\n",
      "Epoch [439/1200], Training Loss: 0.7764, vgg Loss: 69.9410, L2 Loss: 0.0770, time: 12.5785\n",
      "Epoch [440/1200], Training Loss: 0.7979, vgg Loss: 71.9112, L2 Loss: 0.0788, time: 12.7324\n",
      "Epoch [441/1200], Training Loss: 1.0347, vgg Loss: 90.8690, L2 Loss: 0.1260, time: 12.8938\n",
      "Epoch [442/1200], Training Loss: 1.2196, vgg Loss: 99.7645, L2 Loss: 0.2219, time: 13.1222\n",
      "Epoch [443/1200], Training Loss: 1.2049, vgg Loss: 100.6649, L2 Loss: 0.1982, time: 12.5597\n",
      "Epoch [444/1200], Training Loss: 1.0440, vgg Loss: 89.7451, L2 Loss: 0.1465, time: 12.5680\n",
      "Epoch [445/1200], Training Loss: 1.0131, vgg Loss: 84.6845, L2 Loss: 0.1662, time: 12.9648\n",
      "Epoch [446/1200], Training Loss: 1.0127, vgg Loss: 88.7304, L2 Loss: 0.1254, time: 12.5675\n",
      "Epoch [447/1200], Training Loss: 1.0065, vgg Loss: 88.1082, L2 Loss: 0.1254, time: 12.4906\n",
      "Epoch [448/1200], Training Loss: 0.9330, vgg Loss: 84.6797, L2 Loss: 0.0862, time: 12.6451\n",
      "Epoch [449/1200], Training Loss: 0.8422, vgg Loss: 76.2626, L2 Loss: 0.0796, time: 13.2785\n",
      "Epoch [450/1200], Training Loss: 0.9253, vgg Loss: 79.1739, L2 Loss: 0.1336, time: 12.6478\n",
      "Epoch [451/1200], Training Loss: 0.8532, vgg Loss: 75.0779, L2 Loss: 0.1024, time: 12.8588\n",
      "Epoch [452/1200], Training Loss: 0.8421, vgg Loss: 75.8205, L2 Loss: 0.0839, time: 6.2081\n",
      "Epoch [453/1200], Training Loss: 0.8490, vgg Loss: 76.7772, L2 Loss: 0.0812, time: 6.2993\n",
      "Epoch [454/1200], Training Loss: 0.8116, vgg Loss: 73.6041, L2 Loss: 0.0755, time: 6.3326\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [455/1200], Training Loss: 0.8107, vgg Loss: 74.0939, L2 Loss: 0.0698, time: 6.3075\n",
      "Epoch [456/1200], Training Loss: 0.7604, vgg Loss: 69.7090, L2 Loss: 0.0633, time: 6.4986\n",
      "Epoch [457/1200], Training Loss: 0.9621, vgg Loss: 83.8031, L2 Loss: 0.1241, time: 6.3820\n",
      "Epoch [458/1200], Training Loss: 0.9502, vgg Loss: 82.9958, L2 Loss: 0.1202, time: 6.3401\n",
      "Epoch [459/1200], Training Loss: 0.7872, vgg Loss: 70.3602, L2 Loss: 0.0836, time: 6.4757\n",
      "Epoch [460/1200], Training Loss: 0.7867, vgg Loss: 71.3575, L2 Loss: 0.0731, time: 6.5954\n",
      "Epoch [461/1200], Training Loss: 0.8437, vgg Loss: 75.8628, L2 Loss: 0.0851, time: 6.5345\n",
      "Epoch [462/1200], Training Loss: 0.8813, vgg Loss: 77.7218, L2 Loss: 0.1041, time: 6.5236\n",
      "Epoch [463/1200], Training Loss: 0.8593, vgg Loss: 76.6826, L2 Loss: 0.0925, time: 6.5367\n",
      "Epoch [464/1200], Training Loss: 0.9126, vgg Loss: 82.5109, L2 Loss: 0.0875, time: 6.4730\n",
      "Epoch [465/1200], Training Loss: 0.8690, vgg Loss: 76.7035, L2 Loss: 0.1020, time: 6.5914\n",
      "Epoch [466/1200], Training Loss: 0.8153, vgg Loss: 72.9604, L2 Loss: 0.0857, time: 6.5714\n",
      "Epoch [467/1200], Training Loss: 0.8713, vgg Loss: 78.3825, L2 Loss: 0.0875, time: 6.5675\n",
      "Epoch [468/1200], Training Loss: 0.7856, vgg Loss: 71.6587, L2 Loss: 0.0690, time: 6.7580\n",
      "Epoch [469/1200], Training Loss: 0.9005, vgg Loss: 82.2081, L2 Loss: 0.0784, time: 6.8248\n",
      "Epoch [470/1200], Training Loss: 0.8250, vgg Loss: 70.6744, L2 Loss: 0.1183, time: 6.8103\n",
      "Epoch [471/1200], Training Loss: 0.8880, vgg Loss: 77.4346, L2 Loss: 0.1137, time: 6.7191\n",
      "Epoch [472/1200], Training Loss: 0.9062, vgg Loss: 79.2528, L2 Loss: 0.1137, time: 6.7892\n",
      "Epoch [473/1200], Training Loss: 0.7498, vgg Loss: 65.4404, L2 Loss: 0.0954, time: 6.6954\n",
      "Epoch [474/1200], Training Loss: 0.8603, vgg Loss: 77.7182, L2 Loss: 0.0831, time: 6.8179\n",
      "Epoch [475/1200], Training Loss: 0.9116, vgg Loss: 79.2272, L2 Loss: 0.1194, time: 6.9161\n",
      "Epoch [476/1200], Training Loss: 0.8868, vgg Loss: 78.6876, L2 Loss: 0.0999, time: 6.9502\n",
      "Epoch [477/1200], Training Loss: 0.7626, vgg Loss: 67.8841, L2 Loss: 0.0837, time: 6.9324\n",
      "Epoch [478/1200], Training Loss: 0.8035, vgg Loss: 72.3132, L2 Loss: 0.0804, time: 6.8075\n",
      "Epoch [479/1200], Training Loss: 0.8743, vgg Loss: 76.0079, L2 Loss: 0.1142, time: 6.7502\n",
      "Epoch [480/1200], Training Loss: 0.8547, vgg Loss: 76.5307, L2 Loss: 0.0893, time: 6.7336\n",
      "Epoch [481/1200], Training Loss: 0.7428, vgg Loss: 65.7024, L2 Loss: 0.0858, time: 6.7795\n",
      "Epoch [482/1200], Training Loss: 0.9096, vgg Loss: 82.6728, L2 Loss: 0.0828, time: 6.7306\n",
      "Epoch [483/1200], Training Loss: 0.8016, vgg Loss: 70.2819, L2 Loss: 0.0988, time: 6.7671\n",
      "Epoch [484/1200], Training Loss: 0.7971, vgg Loss: 70.9137, L2 Loss: 0.0880, time: 6.8438\n",
      "Epoch [485/1200], Training Loss: 1.0422, vgg Loss: 91.4207, L2 Loss: 0.1280, time: 6.8547\n",
      "Epoch [486/1200], Training Loss: 0.8831, vgg Loss: 77.1941, L2 Loss: 0.1112, time: 6.8469\n",
      "Epoch [487/1200], Training Loss: 0.8357, vgg Loss: 74.5466, L2 Loss: 0.0903, time: 6.6065\n",
      "Epoch [488/1200], Training Loss: 0.8489, vgg Loss: 75.7394, L2 Loss: 0.0915, time: 6.9254\n",
      "Epoch [489/1200], Training Loss: 0.9460, vgg Loss: 81.9914, L2 Loss: 0.1261, time: 6.7500\n",
      "Epoch [490/1200], Training Loss: 0.8511, vgg Loss: 76.0613, L2 Loss: 0.0905, time: 6.8062\n",
      "Epoch [491/1200], Training Loss: 0.9520, vgg Loss: 82.3877, L2 Loss: 0.1281, time: 6.9122\n",
      "Epoch [492/1200], Training Loss: 1.0568, vgg Loss: 87.7556, L2 Loss: 0.1792, time: 6.9367\n",
      "Epoch [493/1200], Training Loss: 0.9895, vgg Loss: 87.8981, L2 Loss: 0.1105, time: 6.9501\n",
      "Epoch [494/1200], Training Loss: 0.9113, vgg Loss: 81.9924, L2 Loss: 0.0913, time: 6.9260\n",
      "Epoch [495/1200], Training Loss: 0.8585, vgg Loss: 77.2019, L2 Loss: 0.0865, time: 6.6565\n",
      "Epoch [496/1200], Training Loss: 0.8416, vgg Loss: 76.7424, L2 Loss: 0.0742, time: 6.8617\n",
      "Epoch [497/1200], Training Loss: 0.8305, vgg Loss: 75.0471, L2 Loss: 0.0800, time: 6.9356\n",
      "Epoch [498/1200], Training Loss: 0.7644, vgg Loss: 66.4263, L2 Loss: 0.1001, time: 6.9331\n",
      "Epoch [499/1200], Training Loss: 1.1870, vgg Loss: 99.3612, L2 Loss: 0.1934, time: 6.8086\n",
      "Epoch [500/1200], Training Loss: 0.9751, vgg Loss: 84.9238, L2 Loss: 0.1259, time: 7.0267\n",
      "Epoch [501/1200], Training Loss: 0.7971, vgg Loss: 71.4303, L2 Loss: 0.0828, time: 6.8503\n",
      "Epoch [502/1200], Training Loss: 0.7526, vgg Loss: 68.0833, L2 Loss: 0.0718, time: 7.0355\n",
      "Epoch [503/1200], Training Loss: 0.8541, vgg Loss: 73.1722, L2 Loss: 0.1224, time: 6.9424\n",
      "Epoch [504/1200], Training Loss: 0.8650, vgg Loss: 73.0682, L2 Loss: 0.1343, time: 6.7957\n",
      "Epoch [505/1200], Training Loss: 0.8850, vgg Loss: 75.8693, L2 Loss: 0.1264, time: 6.9145\n",
      "Epoch [506/1200], Training Loss: 0.8558, vgg Loss: 74.7673, L2 Loss: 0.1081, time: 6.7413\n",
      "Epoch [507/1200], Training Loss: 0.7924, vgg Loss: 70.0504, L2 Loss: 0.0919, time: 6.9659\n",
      "Epoch [508/1200], Training Loss: 1.0356, vgg Loss: 86.5921, L2 Loss: 0.1697, time: 6.9825\n",
      "Epoch [509/1200], Training Loss: 1.0637, vgg Loss: 92.0424, L2 Loss: 0.1433, time: 6.9157\n",
      "Epoch [510/1200], Training Loss: 0.9460, vgg Loss: 84.4231, L2 Loss: 0.1017, time: 6.9195\n",
      "Epoch [511/1200], Training Loss: 0.7677, vgg Loss: 68.5759, L2 Loss: 0.0819, time: 6.7649\n",
      "Epoch [512/1200], Training Loss: 0.7025, vgg Loss: 63.3029, L2 Loss: 0.0695, time: 6.8830\n",
      "Epoch [513/1200], Training Loss: 0.7227, vgg Loss: 65.5093, L2 Loss: 0.0676, time: 6.9202\n",
      "Epoch [514/1200], Training Loss: 0.7951, vgg Loss: 71.5581, L2 Loss: 0.0796, time: 6.8089\n",
      "Epoch [515/1200], Training Loss: 0.8373, vgg Loss: 73.7585, L2 Loss: 0.0997, time: 6.6841\n",
      "Epoch [516/1200], Training Loss: 0.8407, vgg Loss: 74.4183, L2 Loss: 0.0965, time: 6.8138\n",
      "Epoch [517/1200], Training Loss: 0.7954, vgg Loss: 71.4025, L2 Loss: 0.0814, time: 6.8608\n",
      "Epoch [518/1200], Training Loss: 0.7739, vgg Loss: 69.6510, L2 Loss: 0.0774, time: 6.8770\n",
      "Epoch [519/1200], Training Loss: 0.9250, vgg Loss: 81.8131, L2 Loss: 0.1068, time: 6.8948\n",
      "Epoch [520/1200], Training Loss: 0.9815, vgg Loss: 86.9395, L2 Loss: 0.1121, time: 6.8826\n",
      "Epoch [521/1200], Training Loss: 0.8623, vgg Loss: 75.2080, L2 Loss: 0.1102, time: 6.8442\n",
      "Epoch [522/1200], Training Loss: 1.0483, vgg Loss: 91.0775, L2 Loss: 0.1375, time: 6.6398\n",
      "Epoch [523/1200], Training Loss: 0.8023, vgg Loss: 71.6070, L2 Loss: 0.0862, time: 7.0024\n",
      "Epoch [524/1200], Training Loss: 0.7890, vgg Loss: 69.9891, L2 Loss: 0.0891, time: 6.8641\n",
      "Epoch [525/1200], Training Loss: 0.8215, vgg Loss: 74.5075, L2 Loss: 0.0764, time: 6.8428\n",
      "Epoch [526/1200], Training Loss: 0.8037, vgg Loss: 71.5445, L2 Loss: 0.0882, time: 6.8118\n",
      "Epoch [527/1200], Training Loss: 0.7908, vgg Loss: 71.1774, L2 Loss: 0.0791, time: 6.9876\n",
      "Epoch [528/1200], Training Loss: 0.8333, vgg Loss: 73.9734, L2 Loss: 0.0935, time: 6.8706\n",
      "Epoch [529/1200], Training Loss: 0.8108, vgg Loss: 72.0789, L2 Loss: 0.0900, time: 6.8292\n",
      "Epoch [530/1200], Training Loss: 0.8474, vgg Loss: 75.0434, L2 Loss: 0.0969, time: 6.9635\n",
      "Epoch [531/1200], Training Loss: 0.6880, vgg Loss: 62.1327, L2 Loss: 0.0666, time: 7.0026\n",
      "Epoch [532/1200], Training Loss: 0.7326, vgg Loss: 66.0016, L2 Loss: 0.0726, time: 6.9977\n",
      "Epoch [533/1200], Training Loss: 0.7961, vgg Loss: 71.9938, L2 Loss: 0.0761, time: 6.9195\n",
      "Epoch [534/1200], Training Loss: 0.8869, vgg Loss: 78.7159, L2 Loss: 0.0998, time: 6.8806\n",
      "Epoch [535/1200], Training Loss: 0.7942, vgg Loss: 71.2131, L2 Loss: 0.0820, time: 6.6856\n",
      "Epoch [536/1200], Training Loss: 0.8054, vgg Loss: 71.0885, L2 Loss: 0.0945, time: 6.8692\n",
      "Epoch [537/1200], Training Loss: 0.8161, vgg Loss: 71.7477, L2 Loss: 0.0986, time: 6.8527\n",
      "Epoch [538/1200], Training Loss: 0.8519, vgg Loss: 74.7795, L2 Loss: 0.1041, time: 6.9580\n",
      "Epoch [539/1200], Training Loss: 0.7694, vgg Loss: 69.1185, L2 Loss: 0.0783, time: 6.8796\n",
      "Epoch [540/1200], Training Loss: 0.6771, vgg Loss: 61.3120, L2 Loss: 0.0640, time: 6.8467\n",
      "Epoch [541/1200], Training Loss: 0.7957, vgg Loss: 71.4874, L2 Loss: 0.0808, time: 6.8826\n",
      "Epoch [542/1200], Training Loss: 0.8852, vgg Loss: 77.2111, L2 Loss: 0.1131, time: 6.9220\n",
      "Epoch [543/1200], Training Loss: 0.7929, vgg Loss: 71.1277, L2 Loss: 0.0816, time: 6.6441\n",
      "Epoch [544/1200], Training Loss: 0.6589, vgg Loss: 59.9778, L2 Loss: 0.0591, time: 6.7350\n",
      "Epoch [545/1200], Training Loss: 0.8092, vgg Loss: 73.5909, L2 Loss: 0.0733, time: 6.9275\n",
      "Epoch [546/1200], Training Loss: 0.8016, vgg Loss: 72.6331, L2 Loss: 0.0753, time: 6.7630\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [547/1200], Training Loss: 0.7974, vgg Loss: 69.6979, L2 Loss: 0.1005, time: 6.8665\n",
      "Epoch [548/1200], Training Loss: 0.9787, vgg Loss: 83.0554, L2 Loss: 0.1482, time: 6.6762\n",
      "Epoch [549/1200], Training Loss: 0.8240, vgg Loss: 69.9082, L2 Loss: 0.1250, time: 7.0192\n",
      "Epoch [550/1200], Training Loss: 0.8299, vgg Loss: 71.5004, L2 Loss: 0.1148, time: 7.2666\n",
      "Epoch [551/1200], Training Loss: 0.8655, vgg Loss: 74.7745, L2 Loss: 0.1177, time: 6.9940\n",
      "Epoch [552/1200], Training Loss: 0.7679, vgg Loss: 68.4708, L2 Loss: 0.0832, time: 6.9035\n",
      "Epoch [553/1200], Training Loss: 0.7339, vgg Loss: 65.2910, L2 Loss: 0.0810, time: 6.9174\n",
      "Epoch [554/1200], Training Loss: 0.7564, vgg Loss: 68.4849, L2 Loss: 0.0715, time: 6.8656\n",
      "Epoch [555/1200], Training Loss: 0.8490, vgg Loss: 76.4330, L2 Loss: 0.0847, time: 6.8515\n",
      "Epoch [556/1200], Training Loss: 0.7684, vgg Loss: 67.7971, L2 Loss: 0.0904, time: 6.9054\n",
      "Epoch [557/1200], Training Loss: 0.9354, vgg Loss: 80.4451, L2 Loss: 0.1309, time: 6.8446\n",
      "Epoch [558/1200], Training Loss: 0.7843, vgg Loss: 68.0263, L2 Loss: 0.1041, time: 6.9395\n",
      "Epoch [559/1200], Training Loss: 0.7436, vgg Loss: 64.6127, L2 Loss: 0.0974, time: 6.9269\n",
      "Epoch [560/1200], Training Loss: 0.7469, vgg Loss: 66.2417, L2 Loss: 0.0845, time: 6.9474\n",
      "Epoch [561/1200], Training Loss: 0.7845, vgg Loss: 70.5791, L2 Loss: 0.0787, time: 6.6420\n",
      "Epoch [562/1200], Training Loss: 0.7476, vgg Loss: 67.7406, L2 Loss: 0.0701, time: 6.8757\n",
      "Epoch [563/1200], Training Loss: 0.8452, vgg Loss: 76.3111, L2 Loss: 0.0821, time: 6.9624\n",
      "Epoch [564/1200], Training Loss: 0.8300, vgg Loss: 75.7105, L2 Loss: 0.0729, time: 6.7529\n",
      "Epoch [565/1200], Training Loss: 0.7043, vgg Loss: 64.5529, L2 Loss: 0.0587, time: 7.0007\n",
      "Epoch [566/1200], Training Loss: 0.6536, vgg Loss: 59.4509, L2 Loss: 0.0591, time: 6.8330\n",
      "Epoch [567/1200], Training Loss: 0.9111, vgg Loss: 83.0366, L2 Loss: 0.0807, time: 6.9804\n",
      "Epoch [568/1200], Training Loss: 0.8359, vgg Loss: 74.2710, L2 Loss: 0.0932, time: 6.8547\n",
      "Epoch [569/1200], Training Loss: 0.6911, vgg Loss: 62.7355, L2 Loss: 0.0637, time: 6.8784\n",
      "Epoch [570/1200], Training Loss: 0.8513, vgg Loss: 74.7135, L2 Loss: 0.1041, time: 6.9395\n",
      "Epoch [571/1200], Training Loss: 0.8084, vgg Loss: 72.2213, L2 Loss: 0.0862, time: 6.9012\n",
      "Epoch [572/1200], Training Loss: 0.7429, vgg Loss: 67.3177, L2 Loss: 0.0697, time: 7.0172\n",
      "Epoch [573/1200], Training Loss: 0.8212, vgg Loss: 74.6341, L2 Loss: 0.0749, time: 6.9284\n",
      "Epoch [574/1200], Training Loss: 0.7130, vgg Loss: 65.0581, L2 Loss: 0.0624, time: 6.8736\n",
      "Epoch [575/1200], Training Loss: 0.7530, vgg Loss: 67.9834, L2 Loss: 0.0731, time: 7.0612\n",
      "Epoch [576/1200], Training Loss: 0.6680, vgg Loss: 60.6238, L2 Loss: 0.0617, time: 6.8994\n",
      "Epoch [577/1200], Training Loss: 0.8043, vgg Loss: 72.7792, L2 Loss: 0.0766, time: 6.8904\n",
      "Epoch [578/1200], Training Loss: 0.8757, vgg Loss: 79.2202, L2 Loss: 0.0835, time: 6.8794\n",
      "Epoch [579/1200], Training Loss: 0.7297, vgg Loss: 66.4413, L2 Loss: 0.0653, time: 6.8726\n",
      "Epoch [580/1200], Training Loss: 0.7382, vgg Loss: 66.6763, L2 Loss: 0.0715, time: 6.9942\n",
      "Epoch [581/1200], Training Loss: 0.7660, vgg Loss: 68.0004, L2 Loss: 0.0860, time: 7.0137\n",
      "Epoch [582/1200], Training Loss: 0.7690, vgg Loss: 68.9096, L2 Loss: 0.0799, time: 6.7321\n",
      "Epoch [583/1200], Training Loss: 1.3401, vgg Loss: 112.6658, L2 Loss: 0.2134, time: 6.9044\n",
      "Epoch [584/1200], Training Loss: 1.0651, vgg Loss: 89.1742, L2 Loss: 0.1733, time: 6.7639\n",
      "Epoch [585/1200], Training Loss: 0.9928, vgg Loss: 84.5627, L2 Loss: 0.1472, time: 6.8913\n",
      "Epoch [586/1200], Training Loss: 0.8360, vgg Loss: 73.7312, L2 Loss: 0.0987, time: 6.7717\n",
      "Epoch [587/1200], Training Loss: 0.9279, vgg Loss: 76.0708, L2 Loss: 0.1672, time: 6.7360\n",
      "Epoch [588/1200], Training Loss: 1.0668, vgg Loss: 81.8716, L2 Loss: 0.2481, time: 6.8560\n",
      "Epoch [589/1200], Training Loss: 1.0710, vgg Loss: 88.1586, L2 Loss: 0.1894, time: 6.9048\n",
      "Epoch [590/1200], Training Loss: 0.9379, vgg Loss: 81.5302, L2 Loss: 0.1226, time: 6.9050\n",
      "Epoch [591/1200], Training Loss: 0.7705, vgg Loss: 67.0387, L2 Loss: 0.1001, time: 6.9108\n",
      "Epoch [592/1200], Training Loss: 0.6754, vgg Loss: 61.3128, L2 Loss: 0.0623, time: 6.9009\n",
      "Epoch [593/1200], Training Loss: 0.6396, vgg Loss: 56.4316, L2 Loss: 0.0753, time: 6.8736\n",
      "Epoch [594/1200], Training Loss: 0.8466, vgg Loss: 69.0207, L2 Loss: 0.1564, time: 6.7635\n",
      "Epoch [595/1200], Training Loss: 0.7392, vgg Loss: 66.0894, L2 Loss: 0.0783, time: 6.8738\n",
      "Epoch [596/1200], Training Loss: 0.6830, vgg Loss: 60.5327, L2 Loss: 0.0777, time: 6.8643\n",
      "Epoch [597/1200], Training Loss: 0.8089, vgg Loss: 72.2457, L2 Loss: 0.0865, time: 6.9284\n",
      "Epoch [598/1200], Training Loss: 0.7041, vgg Loss: 62.7933, L2 Loss: 0.0761, time: 6.7420\n",
      "Epoch [599/1200], Training Loss: 0.6805, vgg Loss: 60.8380, L2 Loss: 0.0721, time: 6.8571\n",
      "Epoch [600/1200], Training Loss: 0.6998, vgg Loss: 63.1389, L2 Loss: 0.0684, time: 6.8417\n",
      "Epoch [601/1200], Training Loss: 0.7263, vgg Loss: 65.0386, L2 Loss: 0.0759, time: 6.7781\n",
      "Epoch [602/1200], Training Loss: 0.6062, vgg Loss: 55.1263, L2 Loss: 0.0549, time: 6.9175\n",
      "Epoch [603/1200], Training Loss: 0.6694, vgg Loss: 61.7495, L2 Loss: 0.0519, time: 6.9059\n",
      "Epoch [604/1200], Training Loss: 0.6081, vgg Loss: 55.8739, L2 Loss: 0.0493, time: 6.9039\n",
      "Epoch [605/1200], Training Loss: 0.6214, vgg Loss: 57.5047, L2 Loss: 0.0463, time: 6.7587\n",
      "Epoch [606/1200], Training Loss: 0.6288, vgg Loss: 58.3085, L2 Loss: 0.0457, time: 6.9481\n",
      "Epoch [607/1200], Training Loss: 0.6288, vgg Loss: 58.3195, L2 Loss: 0.0456, time: 6.9816\n",
      "Epoch [608/1200], Training Loss: 0.6221, vgg Loss: 57.8252, L2 Loss: 0.0438, time: 7.1124\n",
      "Epoch [609/1200], Training Loss: 0.6665, vgg Loss: 61.5511, L2 Loss: 0.0510, time: 6.9621\n",
      "Epoch [610/1200], Training Loss: 0.6375, vgg Loss: 59.3721, L2 Loss: 0.0438, time: 6.8560\n",
      "Epoch [611/1200], Training Loss: 0.6469, vgg Loss: 60.1080, L2 Loss: 0.0458, time: 6.8849\n",
      "Epoch [612/1200], Training Loss: 0.7165, vgg Loss: 66.2868, L2 Loss: 0.0537, time: 6.9792\n",
      "Epoch [613/1200], Training Loss: 0.6260, vgg Loss: 58.1380, L2 Loss: 0.0447, time: 6.9501\n",
      "Epoch [614/1200], Training Loss: 0.5922, vgg Loss: 54.4966, L2 Loss: 0.0472, time: 6.9684\n",
      "Epoch [615/1200], Training Loss: 0.7153, vgg Loss: 65.9563, L2 Loss: 0.0557, time: 7.0737\n",
      "Epoch [616/1200], Training Loss: 0.5659, vgg Loss: 52.4723, L2 Loss: 0.0412, time: 6.8327\n",
      "Epoch [617/1200], Training Loss: 0.7406, vgg Loss: 68.6151, L2 Loss: 0.0545, time: 6.7353\n",
      "Epoch [618/1200], Training Loss: 0.6612, vgg Loss: 61.2657, L2 Loss: 0.0485, time: 6.9096\n",
      "Epoch [619/1200], Training Loss: 0.6520, vgg Loss: 60.2709, L2 Loss: 0.0493, time: 7.3372\n",
      "Epoch [620/1200], Training Loss: 0.6143, vgg Loss: 56.8899, L2 Loss: 0.0454, time: 7.3228\n",
      "Epoch [621/1200], Training Loss: 0.7208, vgg Loss: 66.9450, L2 Loss: 0.0514, time: 7.0943\n",
      "Epoch [622/1200], Training Loss: 0.6571, vgg Loss: 61.1128, L2 Loss: 0.0459, time: 7.0349\n",
      "Epoch [623/1200], Training Loss: 0.5471, vgg Loss: 50.8800, L2 Loss: 0.0383, time: 7.2615\n",
      "Epoch [624/1200], Training Loss: 0.5959, vgg Loss: 55.3208, L2 Loss: 0.0427, time: 6.9056\n",
      "Epoch [625/1200], Training Loss: 0.6783, vgg Loss: 62.9350, L2 Loss: 0.0490, time: 7.0515\n",
      "Epoch [626/1200], Training Loss: 0.5789, vgg Loss: 53.8395, L2 Loss: 0.0405, time: 6.8965\n",
      "Epoch [627/1200], Training Loss: 0.5984, vgg Loss: 55.2968, L2 Loss: 0.0454, time: 6.8984\n",
      "Epoch [628/1200], Training Loss: 0.6181, vgg Loss: 57.3861, L2 Loss: 0.0442, time: 6.9490\n",
      "Epoch [629/1200], Training Loss: 0.6598, vgg Loss: 61.2556, L2 Loss: 0.0472, time: 6.6801\n",
      "Epoch [630/1200], Training Loss: 0.5862, vgg Loss: 54.2138, L2 Loss: 0.0441, time: 6.9744\n",
      "Epoch [631/1200], Training Loss: 0.5692, vgg Loss: 52.7878, L2 Loss: 0.0413, time: 6.9937\n",
      "Epoch [632/1200], Training Loss: 0.6834, vgg Loss: 63.0226, L2 Loss: 0.0532, time: 6.9694\n",
      "Epoch [633/1200], Training Loss: 0.5154, vgg Loss: 48.0213, L2 Loss: 0.0351, time: 7.0562\n",
      "Epoch [634/1200], Training Loss: 0.6591, vgg Loss: 61.1177, L2 Loss: 0.0479, time: 6.8516\n",
      "Epoch [635/1200], Training Loss: 0.6945, vgg Loss: 64.4147, L2 Loss: 0.0503, time: 6.9257\n",
      "Epoch [636/1200], Training Loss: 0.5879, vgg Loss: 54.2380, L2 Loss: 0.0455, time: 7.0973\n",
      "Epoch [637/1200], Training Loss: 0.6470, vgg Loss: 59.8147, L2 Loss: 0.0489, time: 6.9103\n",
      "Epoch [638/1200], Training Loss: 0.5514, vgg Loss: 51.4170, L2 Loss: 0.0372, time: 6.9478\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [639/1200], Training Loss: 0.6687, vgg Loss: 62.4028, L2 Loss: 0.0447, time: 6.8169\n",
      "Epoch [640/1200], Training Loss: 0.7224, vgg Loss: 66.6224, L2 Loss: 0.0562, time: 6.8509\n",
      "Epoch [641/1200], Training Loss: 0.6256, vgg Loss: 58.1706, L2 Loss: 0.0438, time: 6.9633\n",
      "Epoch [642/1200], Training Loss: 0.6399, vgg Loss: 59.5226, L2 Loss: 0.0447, time: 6.8823\n",
      "Epoch [643/1200], Training Loss: 0.6383, vgg Loss: 59.3957, L2 Loss: 0.0443, time: 6.6871\n",
      "Epoch [644/1200], Training Loss: 0.6111, vgg Loss: 56.6907, L2 Loss: 0.0442, time: 6.9135\n",
      "Epoch [645/1200], Training Loss: 0.6316, vgg Loss: 58.7818, L2 Loss: 0.0438, time: 6.8643\n",
      "Epoch [646/1200], Training Loss: 0.7613, vgg Loss: 70.4402, L2 Loss: 0.0569, time: 6.7711\n",
      "Epoch [647/1200], Training Loss: 0.6793, vgg Loss: 63.2012, L2 Loss: 0.0473, time: 7.2375\n",
      "Epoch [648/1200], Training Loss: 0.5861, vgg Loss: 54.6445, L2 Loss: 0.0396, time: 7.2439\n",
      "Epoch [649/1200], Training Loss: 0.6833, vgg Loss: 62.5759, L2 Loss: 0.0575, time: 6.9824\n",
      "Epoch [650/1200], Training Loss: 0.6027, vgg Loss: 55.5087, L2 Loss: 0.0476, time: 7.0780\n",
      "Epoch [651/1200], Training Loss: 0.6127, vgg Loss: 57.1354, L2 Loss: 0.0413, time: 6.9973\n",
      "Epoch [652/1200], Training Loss: 0.6265, vgg Loss: 58.0295, L2 Loss: 0.0462, time: 7.0442\n",
      "Epoch [653/1200], Training Loss: 0.6265, vgg Loss: 58.2141, L2 Loss: 0.0443, time: 7.0086\n",
      "Epoch [654/1200], Training Loss: 0.5753, vgg Loss: 52.9062, L2 Loss: 0.0462, time: 7.1034\n",
      "Epoch [655/1200], Training Loss: 0.6179, vgg Loss: 56.5929, L2 Loss: 0.0519, time: 6.8375\n",
      "Epoch [656/1200], Training Loss: 0.6649, vgg Loss: 61.7513, L2 Loss: 0.0474, time: 6.8038\n",
      "Epoch [657/1200], Training Loss: 0.6110, vgg Loss: 56.7929, L2 Loss: 0.0430, time: 6.8520\n",
      "Epoch [658/1200], Training Loss: 0.6921, vgg Loss: 64.0660, L2 Loss: 0.0514, time: 6.8398\n",
      "Epoch [659/1200], Training Loss: 0.7911, vgg Loss: 72.1399, L2 Loss: 0.0697, time: 6.9085\n",
      "Epoch [660/1200], Training Loss: 0.6559, vgg Loss: 60.5183, L2 Loss: 0.0507, time: 6.9362\n",
      "Epoch [661/1200], Training Loss: 0.6707, vgg Loss: 61.9931, L2 Loss: 0.0508, time: 6.8895\n",
      "Epoch [662/1200], Training Loss: 0.6532, vgg Loss: 60.6220, L2 Loss: 0.0470, time: 6.9041\n",
      "Epoch [663/1200], Training Loss: 0.6072, vgg Loss: 56.1506, L2 Loss: 0.0457, time: 6.8221\n",
      "Epoch [664/1200], Training Loss: 0.6476, vgg Loss: 60.2102, L2 Loss: 0.0455, time: 6.8228\n",
      "Epoch [665/1200], Training Loss: 0.6263, vgg Loss: 58.1966, L2 Loss: 0.0443, time: 6.9125\n",
      "Epoch [666/1200], Training Loss: 0.6053, vgg Loss: 55.4924, L2 Loss: 0.0503, time: 6.8587\n",
      "Epoch [667/1200], Training Loss: 0.7127, vgg Loss: 65.8069, L2 Loss: 0.0546, time: 6.9424\n",
      "Epoch [668/1200], Training Loss: 0.7058, vgg Loss: 64.7146, L2 Loss: 0.0587, time: 6.9062\n",
      "Epoch [669/1200], Training Loss: 0.6610, vgg Loss: 60.7050, L2 Loss: 0.0540, time: 6.9942\n",
      "Epoch [670/1200], Training Loss: 0.6266, vgg Loss: 58.0757, L2 Loss: 0.0459, time: 7.0966\n",
      "Epoch [671/1200], Training Loss: 0.6301, vgg Loss: 57.9524, L2 Loss: 0.0506, time: 7.0008\n",
      "Epoch [672/1200], Training Loss: 0.6641, vgg Loss: 61.5229, L2 Loss: 0.0489, time: 6.7101\n",
      "Epoch [673/1200], Training Loss: 0.6196, vgg Loss: 57.9038, L2 Loss: 0.0406, time: 6.9250\n",
      "Epoch [674/1200], Training Loss: 0.6236, vgg Loss: 57.7830, L2 Loss: 0.0458, time: 6.8218\n",
      "Epoch [675/1200], Training Loss: 0.6008, vgg Loss: 56.0878, L2 Loss: 0.0399, time: 7.0821\n",
      "Epoch [676/1200], Training Loss: 0.6353, vgg Loss: 59.1111, L2 Loss: 0.0441, time: 6.6841\n",
      "Epoch [677/1200], Training Loss: 0.6582, vgg Loss: 60.8957, L2 Loss: 0.0493, time: 6.7239\n",
      "Epoch [678/1200], Training Loss: 0.6587, vgg Loss: 61.4602, L2 Loss: 0.0441, time: 6.8737\n",
      "Epoch [679/1200], Training Loss: 0.6420, vgg Loss: 59.4523, L2 Loss: 0.0475, time: 6.8131\n",
      "Epoch [680/1200], Training Loss: 0.5848, vgg Loss: 54.6885, L2 Loss: 0.0380, time: 6.7390\n",
      "Epoch [681/1200], Training Loss: 0.5956, vgg Loss: 55.4576, L2 Loss: 0.0411, time: 6.8794\n",
      "Epoch [682/1200], Training Loss: 0.5800, vgg Loss: 53.7618, L2 Loss: 0.0424, time: 6.9435\n",
      "Epoch [683/1200], Training Loss: 0.5360, vgg Loss: 50.0085, L2 Loss: 0.0359, time: 6.9170\n",
      "Epoch [684/1200], Training Loss: 0.6744, vgg Loss: 62.0057, L2 Loss: 0.0544, time: 6.9184\n",
      "Epoch [685/1200], Training Loss: 0.6705, vgg Loss: 62.3735, L2 Loss: 0.0467, time: 6.6515\n",
      "Epoch [686/1200], Training Loss: 0.7312, vgg Loss: 67.4366, L2 Loss: 0.0568, time: 6.9080\n",
      "Epoch [687/1200], Training Loss: 0.5956, vgg Loss: 55.4053, L2 Loss: 0.0415, time: 6.9066\n",
      "Epoch [688/1200], Training Loss: 0.7292, vgg Loss: 67.4105, L2 Loss: 0.0551, time: 6.5405\n",
      "Epoch [689/1200], Training Loss: 0.6745, vgg Loss: 62.7557, L2 Loss: 0.0469, time: 6.8015\n",
      "Epoch [690/1200], Training Loss: 0.6176, vgg Loss: 57.4101, L2 Loss: 0.0435, time: 6.9185\n",
      "Epoch [691/1200], Training Loss: 0.6543, vgg Loss: 60.7725, L2 Loss: 0.0466, time: 6.9016\n",
      "Epoch [692/1200], Training Loss: 0.6297, vgg Loss: 58.7885, L2 Loss: 0.0418, time: 6.8753\n",
      "Epoch [693/1200], Training Loss: 0.7401, vgg Loss: 67.8321, L2 Loss: 0.0618, time: 6.9524\n",
      "Epoch [694/1200], Training Loss: 0.6029, vgg Loss: 56.2001, L2 Loss: 0.0409, time: 6.8239\n",
      "Epoch [695/1200], Training Loss: 0.6639, vgg Loss: 61.2803, L2 Loss: 0.0511, time: 6.8637\n",
      "Epoch [696/1200], Training Loss: 0.6013, vgg Loss: 55.7770, L2 Loss: 0.0436, time: 6.8745\n",
      "Epoch [697/1200], Training Loss: 0.6487, vgg Loss: 60.0373, L2 Loss: 0.0483, time: 6.9085\n",
      "Epoch [698/1200], Training Loss: 0.6005, vgg Loss: 55.6782, L2 Loss: 0.0437, time: 6.8638\n",
      "Epoch [699/1200], Training Loss: 0.5721, vgg Loss: 53.3639, L2 Loss: 0.0385, time: 6.9290\n",
      "Epoch [700/1200], Training Loss: 0.5870, vgg Loss: 54.4028, L2 Loss: 0.0430, time: 7.0372\n",
      "Epoch [701/1200], Training Loss: 0.6628, vgg Loss: 61.3262, L2 Loss: 0.0496, time: 6.9681\n",
      "Epoch [702/1200], Training Loss: 0.6185, vgg Loss: 57.7153, L2 Loss: 0.0414, time: 7.0910\n",
      "Epoch [703/1200], Training Loss: 0.5753, vgg Loss: 53.4425, L2 Loss: 0.0408, time: 6.8846\n",
      "Epoch [704/1200], Training Loss: 0.6555, vgg Loss: 60.9079, L2 Loss: 0.0464, time: 6.9890\n",
      "Epoch [705/1200], Training Loss: 0.6178, vgg Loss: 57.0700, L2 Loss: 0.0471, time: 6.9394\n",
      "Epoch [706/1200], Training Loss: 0.6299, vgg Loss: 58.9087, L2 Loss: 0.0408, time: 7.1402\n",
      "Epoch [707/1200], Training Loss: 0.6405, vgg Loss: 59.0558, L2 Loss: 0.0499, time: 6.8502\n",
      "Epoch [708/1200], Training Loss: 0.5972, vgg Loss: 55.4328, L2 Loss: 0.0428, time: 6.8414\n",
      "Epoch [709/1200], Training Loss: 0.5925, vgg Loss: 55.1674, L2 Loss: 0.0408, time: 6.8907\n",
      "Epoch [710/1200], Training Loss: 0.6113, vgg Loss: 57.0219, L2 Loss: 0.0411, time: 6.9441\n",
      "Epoch [711/1200], Training Loss: 0.5692, vgg Loss: 52.9159, L2 Loss: 0.0401, time: 6.9630\n",
      "Epoch [712/1200], Training Loss: 0.6118, vgg Loss: 56.5728, L2 Loss: 0.0461, time: 6.8693\n",
      "Epoch [713/1200], Training Loss: 0.6383, vgg Loss: 58.9866, L2 Loss: 0.0485, time: 6.8723\n",
      "Epoch [714/1200], Training Loss: 0.6077, vgg Loss: 55.9951, L2 Loss: 0.0478, time: 6.8437\n",
      "Epoch [715/1200], Training Loss: 0.6507, vgg Loss: 60.4771, L2 Loss: 0.0459, time: 6.8676\n",
      "Epoch [716/1200], Training Loss: 0.6019, vgg Loss: 56.2960, L2 Loss: 0.0389, time: 6.9244\n",
      "Epoch [717/1200], Training Loss: 0.5675, vgg Loss: 52.8820, L2 Loss: 0.0387, time: 7.0717\n",
      "Epoch [718/1200], Training Loss: 0.6209, vgg Loss: 57.2804, L2 Loss: 0.0481, time: 6.8383\n",
      "Epoch [719/1200], Training Loss: 0.6173, vgg Loss: 57.0341, L2 Loss: 0.0470, time: 6.9389\n",
      "Epoch [720/1200], Training Loss: 0.5721, vgg Loss: 53.2386, L2 Loss: 0.0397, time: 6.9028\n",
      "Epoch [721/1200], Training Loss: 0.6028, vgg Loss: 56.0921, L2 Loss: 0.0419, time: 6.8308\n",
      "Epoch [722/1200], Training Loss: 0.5952, vgg Loss: 55.2846, L2 Loss: 0.0424, time: 7.6135\n",
      "Epoch [723/1200], Training Loss: 0.6207, vgg Loss: 57.6612, L2 Loss: 0.0441, time: 7.0822\n",
      "Epoch [724/1200], Training Loss: 0.5956, vgg Loss: 55.2231, L2 Loss: 0.0434, time: 7.3322\n",
      "Epoch [725/1200], Training Loss: 0.6409, vgg Loss: 59.6690, L2 Loss: 0.0443, time: 7.0432\n",
      "Epoch [726/1200], Training Loss: 0.6244, vgg Loss: 58.1205, L2 Loss: 0.0432, time: 7.0545\n",
      "Epoch [727/1200], Training Loss: 0.5613, vgg Loss: 52.5458, L2 Loss: 0.0359, time: 7.0123\n",
      "Epoch [728/1200], Training Loss: 0.6097, vgg Loss: 56.9479, L2 Loss: 0.0403, time: 6.7561\n",
      "Epoch [729/1200], Training Loss: 0.7126, vgg Loss: 65.7833, L2 Loss: 0.0548, time: 7.0015\n",
      "Epoch [730/1200], Training Loss: 0.6330, vgg Loss: 58.7223, L2 Loss: 0.0458, time: 6.9664\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [731/1200], Training Loss: 0.6039, vgg Loss: 55.3850, L2 Loss: 0.0501, time: 7.0717\n",
      "Epoch [732/1200], Training Loss: 0.5653, vgg Loss: 52.2316, L2 Loss: 0.0430, time: 9.1675\n",
      "Epoch [733/1200], Training Loss: 0.5709, vgg Loss: 52.8108, L2 Loss: 0.0428, time: 8.4993\n",
      "Epoch [734/1200], Training Loss: 0.5897, vgg Loss: 54.0275, L2 Loss: 0.0495, time: 7.5710\n",
      "Epoch [735/1200], Training Loss: 0.6014, vgg Loss: 55.9202, L2 Loss: 0.0422, time: 7.7274\n",
      "Epoch [736/1200], Training Loss: 0.6038, vgg Loss: 56.0474, L2 Loss: 0.0434, time: 7.7762\n",
      "Epoch [737/1200], Training Loss: 0.6455, vgg Loss: 59.8674, L2 Loss: 0.0468, time: 7.1050\n",
      "Epoch [738/1200], Training Loss: 0.6131, vgg Loss: 57.1972, L2 Loss: 0.0411, time: 6.9724\n",
      "Epoch [739/1200], Training Loss: 0.6440, vgg Loss: 59.8187, L2 Loss: 0.0458, time: 6.8238\n",
      "Epoch [740/1200], Training Loss: 0.5887, vgg Loss: 54.7741, L2 Loss: 0.0409, time: 7.6216\n",
      "Epoch [741/1200], Training Loss: 0.6633, vgg Loss: 61.1560, L2 Loss: 0.0518, time: 7.4740\n",
      "Epoch [742/1200], Training Loss: 0.6005, vgg Loss: 55.1530, L2 Loss: 0.0490, time: 6.8829\n",
      "Epoch [743/1200], Training Loss: 0.6489, vgg Loss: 60.3107, L2 Loss: 0.0458, time: 7.1451\n",
      "Epoch [744/1200], Training Loss: 0.5962, vgg Loss: 55.4129, L2 Loss: 0.0421, time: 7.4581\n",
      "Epoch [745/1200], Training Loss: 0.6405, vgg Loss: 59.6124, L2 Loss: 0.0444, time: 7.4299\n",
      "Epoch [746/1200], Training Loss: 0.6049, vgg Loss: 56.1745, L2 Loss: 0.0432, time: 7.3758\n",
      "Epoch [747/1200], Training Loss: 0.6214, vgg Loss: 57.6679, L2 Loss: 0.0447, time: 7.1483\n",
      "Epoch [748/1200], Training Loss: 0.5846, vgg Loss: 54.1726, L2 Loss: 0.0429, time: 7.0607\n",
      "Epoch [749/1200], Training Loss: 0.5764, vgg Loss: 53.8744, L2 Loss: 0.0376, time: 7.0210\n",
      "Epoch [750/1200], Training Loss: 0.5830, vgg Loss: 54.2991, L2 Loss: 0.0401, time: 7.1793\n",
      "Epoch [751/1200], Training Loss: 0.6035, vgg Loss: 56.0987, L2 Loss: 0.0425, time: 7.0323\n",
      "Epoch [752/1200], Training Loss: 0.6096, vgg Loss: 56.6552, L2 Loss: 0.0431, time: 7.1715\n",
      "Epoch [753/1200], Training Loss: 0.6169, vgg Loss: 57.3322, L2 Loss: 0.0436, time: 7.2455\n",
      "Epoch [754/1200], Training Loss: 0.6223, vgg Loss: 57.7781, L2 Loss: 0.0445, time: 7.1015\n",
      "Epoch [755/1200], Training Loss: 0.6068, vgg Loss: 56.3401, L2 Loss: 0.0434, time: 7.1748\n",
      "Epoch [756/1200], Training Loss: 0.5765, vgg Loss: 53.3911, L2 Loss: 0.0426, time: 7.1464\n",
      "Epoch [757/1200], Training Loss: 0.6875, vgg Loss: 63.6235, L2 Loss: 0.0512, time: 7.0780\n",
      "Epoch [758/1200], Training Loss: 0.5833, vgg Loss: 53.9044, L2 Loss: 0.0443, time: 7.0035\n",
      "Epoch [759/1200], Training Loss: 0.5967, vgg Loss: 55.3609, L2 Loss: 0.0430, time: 7.0684\n",
      "Epoch [760/1200], Training Loss: 0.5893, vgg Loss: 54.5125, L2 Loss: 0.0442, time: 6.9910\n",
      "Epoch [761/1200], Training Loss: 0.5987, vgg Loss: 55.6129, L2 Loss: 0.0425, time: 7.1673\n",
      "Epoch [762/1200], Training Loss: 0.5909, vgg Loss: 54.9206, L2 Loss: 0.0417, time: 6.8627\n",
      "Epoch [763/1200], Training Loss: 0.5869, vgg Loss: 54.8327, L2 Loss: 0.0386, time: 6.9681\n",
      "Epoch [764/1200], Training Loss: 0.5937, vgg Loss: 55.5888, L2 Loss: 0.0378, time: 7.0146\n",
      "Epoch [765/1200], Training Loss: 0.6465, vgg Loss: 60.1958, L2 Loss: 0.0445, time: 7.0446\n",
      "Epoch [766/1200], Training Loss: 0.6044, vgg Loss: 55.8953, L2 Loss: 0.0455, time: 7.0112\n",
      "Epoch [767/1200], Training Loss: 0.6008, vgg Loss: 55.3169, L2 Loss: 0.0476, time: 7.1097\n",
      "Epoch [768/1200], Training Loss: 0.6314, vgg Loss: 57.7835, L2 Loss: 0.0535, time: 6.9970\n",
      "Epoch [769/1200], Training Loss: 0.5944, vgg Loss: 54.9933, L2 Loss: 0.0445, time: 7.1067\n",
      "Epoch [770/1200], Training Loss: 0.5482, vgg Loss: 50.9755, L2 Loss: 0.0384, time: 6.9887\n",
      "Epoch [771/1200], Training Loss: 0.6596, vgg Loss: 61.2933, L2 Loss: 0.0467, time: 6.9643\n",
      "Epoch [772/1200], Training Loss: 0.6020, vgg Loss: 55.7110, L2 Loss: 0.0449, time: 7.0035\n",
      "Epoch [773/1200], Training Loss: 0.5971, vgg Loss: 55.8978, L2 Loss: 0.0382, time: 7.0187\n",
      "Epoch [774/1200], Training Loss: 0.6157, vgg Loss: 56.8622, L2 Loss: 0.0471, time: 6.8520\n",
      "Epoch [775/1200], Training Loss: 0.6299, vgg Loss: 58.6164, L2 Loss: 0.0437, time: 6.9470\n",
      "Epoch [776/1200], Training Loss: 0.6190, vgg Loss: 57.4300, L2 Loss: 0.0447, time: 6.7787\n",
      "Epoch [777/1200], Training Loss: 0.6113, vgg Loss: 56.6070, L2 Loss: 0.0452, time: 7.0047\n",
      "Epoch [778/1200], Training Loss: 0.5653, vgg Loss: 52.5880, L2 Loss: 0.0394, time: 7.1032\n",
      "Epoch [779/1200], Training Loss: 0.6046, vgg Loss: 55.6528, L2 Loss: 0.0481, time: 7.0788\n",
      "Epoch [780/1200], Training Loss: 0.5505, vgg Loss: 50.5854, L2 Loss: 0.0446, time: 7.0832\n",
      "Epoch [781/1200], Training Loss: 0.5990, vgg Loss: 55.5185, L2 Loss: 0.0438, time: 7.0561\n",
      "Epoch [782/1200], Training Loss: 0.5295, vgg Loss: 49.4713, L2 Loss: 0.0347, time: 7.1414\n",
      "Epoch [783/1200], Training Loss: 0.6280, vgg Loss: 58.4310, L2 Loss: 0.0437, time: 7.1487\n",
      "Epoch [784/1200], Training Loss: 0.6121, vgg Loss: 56.3229, L2 Loss: 0.0489, time: 7.0117\n",
      "Epoch [785/1200], Training Loss: 0.5863, vgg Loss: 53.8787, L2 Loss: 0.0475, time: 6.8004\n",
      "Epoch [786/1200], Training Loss: 0.6250, vgg Loss: 57.9778, L2 Loss: 0.0452, time: 6.9849\n",
      "Epoch [787/1200], Training Loss: 0.6238, vgg Loss: 58.0217, L2 Loss: 0.0435, time: 6.9303\n",
      "Epoch [788/1200], Training Loss: 0.6136, vgg Loss: 56.7151, L2 Loss: 0.0464, time: 7.0808\n",
      "Epoch [789/1200], Training Loss: 0.5917, vgg Loss: 54.6667, L2 Loss: 0.0450, time: 7.1000\n",
      "Epoch [790/1200], Training Loss: 0.6393, vgg Loss: 59.2432, L2 Loss: 0.0468, time: 6.9945\n",
      "Epoch [791/1200], Training Loss: 0.5700, vgg Loss: 53.2336, L2 Loss: 0.0377, time: 6.7676\n",
      "Epoch [792/1200], Training Loss: 0.5721, vgg Loss: 52.4889, L2 Loss: 0.0472, time: 7.0099\n",
      "Epoch [793/1200], Training Loss: 0.6048, vgg Loss: 56.1449, L2 Loss: 0.0433, time: 7.0087\n",
      "Epoch [794/1200], Training Loss: 0.6319, vgg Loss: 58.7597, L2 Loss: 0.0443, time: 6.9814\n",
      "Epoch [795/1200], Training Loss: 0.6250, vgg Loss: 58.0850, L2 Loss: 0.0441, time: 7.0620\n",
      "Epoch [796/1200], Training Loss: 0.5756, vgg Loss: 53.5523, L2 Loss: 0.0400, time: 7.0070\n",
      "Epoch [797/1200], Training Loss: 0.5715, vgg Loss: 53.3502, L2 Loss: 0.0380, time: 6.8512\n",
      "Epoch [798/1200], Training Loss: 0.5903, vgg Loss: 54.8233, L2 Loss: 0.0421, time: 7.0004\n",
      "Epoch [799/1200], Training Loss: 0.5766, vgg Loss: 53.5448, L2 Loss: 0.0412, time: 7.0999\n",
      "Epoch [800/1200], Training Loss: 0.6119, vgg Loss: 56.6273, L2 Loss: 0.0457, time: 7.0373\n",
      "Epoch [801/1200], Training Loss: 0.5630, vgg Loss: 52.3593, L2 Loss: 0.0394, time: 7.0009\n",
      "Epoch [802/1200], Training Loss: 0.5705, vgg Loss: 52.7764, L2 Loss: 0.0428, time: 7.0251\n",
      "Epoch [803/1200], Training Loss: 0.5886, vgg Loss: 54.5390, L2 Loss: 0.0432, time: 7.0185\n",
      "Epoch [804/1200], Training Loss: 0.5531, vgg Loss: 51.4470, L2 Loss: 0.0387, time: 6.6743\n",
      "Epoch [805/1200], Training Loss: 0.6222, vgg Loss: 57.9196, L2 Loss: 0.0430, time: 7.0455\n",
      "Epoch [806/1200], Training Loss: 0.5639, vgg Loss: 52.4123, L2 Loss: 0.0397, time: 7.0339\n",
      "Epoch [807/1200], Training Loss: 0.6352, vgg Loss: 58.4604, L2 Loss: 0.0506, time: 7.0323\n",
      "Epoch [808/1200], Training Loss: 0.6325, vgg Loss: 58.6821, L2 Loss: 0.0457, time: 6.9436\n",
      "Epoch [809/1200], Training Loss: 0.5906, vgg Loss: 55.1267, L2 Loss: 0.0393, time: 7.1039\n",
      "Epoch [810/1200], Training Loss: 0.5900, vgg Loss: 54.9314, L2 Loss: 0.0407, time: 6.9601\n",
      "Epoch [811/1200], Training Loss: 0.6655, vgg Loss: 61.4760, L2 Loss: 0.0507, time: 7.0400\n",
      "Epoch [812/1200], Training Loss: 0.5418, vgg Loss: 49.8131, L2 Loss: 0.0437, time: 7.1134\n",
      "Epoch [813/1200], Training Loss: 0.6405, vgg Loss: 59.3709, L2 Loss: 0.0468, time: 7.0982\n",
      "Epoch [814/1200], Training Loss: 0.5839, vgg Loss: 53.3612, L2 Loss: 0.0503, time: 6.9658\n",
      "Epoch [815/1200], Training Loss: 0.6640, vgg Loss: 61.6855, L2 Loss: 0.0472, time: 7.0538\n",
      "Epoch [816/1200], Training Loss: 0.5151, vgg Loss: 47.3405, L2 Loss: 0.0417, time: 6.9337\n",
      "Epoch [817/1200], Training Loss: 0.5951, vgg Loss: 55.3484, L2 Loss: 0.0416, time: 7.2078\n",
      "Epoch [818/1200], Training Loss: 0.6093, vgg Loss: 56.4783, L2 Loss: 0.0445, time: 7.0408\n",
      "Epoch [819/1200], Training Loss: 0.5430, vgg Loss: 49.9865, L2 Loss: 0.0432, time: 7.0791\n",
      "Epoch [820/1200], Training Loss: 0.5730, vgg Loss: 52.7799, L2 Loss: 0.0452, time: 7.0329\n",
      "Epoch [821/1200], Training Loss: 0.5768, vgg Loss: 53.5665, L2 Loss: 0.0411, time: 7.0828\n",
      "Epoch [822/1200], Training Loss: 0.6603, vgg Loss: 60.9173, L2 Loss: 0.0511, time: 7.1554\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [823/1200], Training Loss: 0.6421, vgg Loss: 59.3762, L2 Loss: 0.0484, time: 7.0539\n",
      "Epoch [824/1200], Training Loss: 0.5476, vgg Loss: 51.0543, L2 Loss: 0.0370, time: 7.0613\n",
      "Epoch [825/1200], Training Loss: 0.5582, vgg Loss: 51.8572, L2 Loss: 0.0397, time: 7.0258\n",
      "Epoch [826/1200], Training Loss: 0.6472, vgg Loss: 59.6857, L2 Loss: 0.0504, time: 7.0068\n",
      "Epoch [827/1200], Training Loss: 0.6215, vgg Loss: 57.5612, L2 Loss: 0.0459, time: 7.0218\n",
      "Epoch [828/1200], Training Loss: 0.6681, vgg Loss: 61.8178, L2 Loss: 0.0499, time: 7.0180\n",
      "Epoch [829/1200], Training Loss: 0.6901, vgg Loss: 63.4065, L2 Loss: 0.0560, time: 6.9735\n",
      "Epoch [830/1200], Training Loss: 0.6155, vgg Loss: 57.1446, L2 Loss: 0.0441, time: 6.8011\n",
      "Epoch [831/1200], Training Loss: 0.6162, vgg Loss: 56.8738, L2 Loss: 0.0474, time: 6.9789\n",
      "Epoch [832/1200], Training Loss: 0.5533, vgg Loss: 51.5886, L2 Loss: 0.0374, time: 7.0876\n",
      "Epoch [833/1200], Training Loss: 0.5765, vgg Loss: 53.6461, L2 Loss: 0.0400, time: 6.8923\n",
      "Epoch [834/1200], Training Loss: 0.5577, vgg Loss: 51.7671, L2 Loss: 0.0400, time: 7.1070\n",
      "Epoch [835/1200], Training Loss: 0.5788, vgg Loss: 54.0660, L2 Loss: 0.0381, time: 7.1331\n",
      "Epoch [836/1200], Training Loss: 0.5584, vgg Loss: 52.1060, L2 Loss: 0.0373, time: 6.9574\n",
      "Epoch [837/1200], Training Loss: 0.5149, vgg Loss: 47.9071, L2 Loss: 0.0358, time: 7.0373\n",
      "Epoch [838/1200], Training Loss: 0.6166, vgg Loss: 57.4884, L2 Loss: 0.0417, time: 7.1146\n",
      "Epoch [839/1200], Training Loss: 0.5618, vgg Loss: 52.2601, L2 Loss: 0.0392, time: 6.9309\n",
      "Epoch [840/1200], Training Loss: 0.5917, vgg Loss: 54.5114, L2 Loss: 0.0466, time: 6.9703\n",
      "Epoch [841/1200], Training Loss: 0.5790, vgg Loss: 53.8166, L2 Loss: 0.0409, time: 6.9762\n",
      "Epoch [842/1200], Training Loss: 0.5120, vgg Loss: 47.6232, L2 Loss: 0.0358, time: 7.4579\n",
      "Epoch [843/1200], Training Loss: 0.5952, vgg Loss: 55.2706, L2 Loss: 0.0425, time: 6.9515\n",
      "Epoch [844/1200], Training Loss: 0.5812, vgg Loss: 54.0956, L2 Loss: 0.0403, time: 7.0201\n",
      "Epoch [845/1200], Training Loss: 0.6425, vgg Loss: 59.7666, L2 Loss: 0.0448, time: 7.1494\n",
      "Epoch [846/1200], Training Loss: 0.6631, vgg Loss: 61.4845, L2 Loss: 0.0483, time: 7.5020\n",
      "Epoch [847/1200], Training Loss: 0.6183, vgg Loss: 57.4259, L2 Loss: 0.0440, time: 7.1131\n",
      "Epoch [848/1200], Training Loss: 0.5537, vgg Loss: 51.6967, L2 Loss: 0.0368, time: 7.0919\n",
      "Epoch [849/1200], Training Loss: 0.5963, vgg Loss: 55.6768, L2 Loss: 0.0395, time: 7.0846\n",
      "Epoch [850/1200], Training Loss: 0.5423, vgg Loss: 50.4847, L2 Loss: 0.0374, time: 6.9638\n",
      "Epoch [851/1200], Training Loss: 0.5676, vgg Loss: 52.7381, L2 Loss: 0.0402, time: 6.9805\n",
      "Epoch [852/1200], Training Loss: 0.6195, vgg Loss: 57.0774, L2 Loss: 0.0487, time: 7.1562\n",
      "Epoch [853/1200], Training Loss: 0.6385, vgg Loss: 58.8707, L2 Loss: 0.0498, time: 7.1490\n",
      "Epoch [854/1200], Training Loss: 0.6191, vgg Loss: 57.3554, L2 Loss: 0.0456, time: 6.9750\n",
      "Epoch [855/1200], Training Loss: 0.6136, vgg Loss: 57.0455, L2 Loss: 0.0431, time: 6.9320\n",
      "Epoch [856/1200], Training Loss: 0.5019, vgg Loss: 46.6437, L2 Loss: 0.0355, time: 6.9875\n",
      "Epoch [857/1200], Training Loss: 0.5558, vgg Loss: 51.8564, L2 Loss: 0.0373, time: 6.9765\n",
      "Epoch [858/1200], Training Loss: 0.6126, vgg Loss: 56.7719, L2 Loss: 0.0448, time: 6.9769\n",
      "Epoch [859/1200], Training Loss: 0.6032, vgg Loss: 55.6565, L2 Loss: 0.0466, time: 6.9339\n",
      "Epoch [860/1200], Training Loss: 0.5763, vgg Loss: 53.6879, L2 Loss: 0.0394, time: 6.9105\n",
      "Epoch [861/1200], Training Loss: 0.6312, vgg Loss: 58.2933, L2 Loss: 0.0483, time: 6.9754\n",
      "Epoch [862/1200], Training Loss: 0.5909, vgg Loss: 54.5902, L2 Loss: 0.0450, time: 7.0147\n",
      "Epoch [863/1200], Training Loss: 0.6067, vgg Loss: 56.4016, L2 Loss: 0.0426, time: 7.0203\n",
      "Epoch [864/1200], Training Loss: 0.5375, vgg Loss: 50.0226, L2 Loss: 0.0373, time: 6.8360\n",
      "Epoch [865/1200], Training Loss: 0.5313, vgg Loss: 49.6672, L2 Loss: 0.0346, time: 6.9576\n",
      "Epoch [866/1200], Training Loss: 0.5355, vgg Loss: 49.9784, L2 Loss: 0.0357, time: 6.9612\n",
      "Epoch [867/1200], Training Loss: 0.5380, vgg Loss: 49.6634, L2 Loss: 0.0413, time: 6.8827\n",
      "Epoch [868/1200], Training Loss: 0.5653, vgg Loss: 52.1672, L2 Loss: 0.0437, time: 6.8243\n",
      "Epoch [869/1200], Training Loss: 0.5757, vgg Loss: 53.3156, L2 Loss: 0.0426, time: 6.9843\n",
      "Epoch [870/1200], Training Loss: 0.5904, vgg Loss: 54.7967, L2 Loss: 0.0424, time: 6.7657\n",
      "Epoch [871/1200], Training Loss: 0.5682, vgg Loss: 52.8624, L2 Loss: 0.0395, time: 6.9799\n",
      "Epoch [872/1200], Training Loss: 0.6048, vgg Loss: 56.3664, L2 Loss: 0.0412, time: 6.9800\n",
      "Epoch [873/1200], Training Loss: 0.5088, vgg Loss: 47.6582, L2 Loss: 0.0322, time: 6.9884\n",
      "Epoch [874/1200], Training Loss: 0.5901, vgg Loss: 54.7801, L2 Loss: 0.0423, time: 6.8639\n",
      "Epoch [875/1200], Training Loss: 0.6090, vgg Loss: 56.4170, L2 Loss: 0.0448, time: 7.0611\n",
      "Epoch [876/1200], Training Loss: 0.5269, vgg Loss: 48.2974, L2 Loss: 0.0439, time: 6.8364\n",
      "Epoch [877/1200], Training Loss: 0.5631, vgg Loss: 52.1201, L2 Loss: 0.0419, time: 7.0287\n",
      "Epoch [878/1200], Training Loss: 0.5251, vgg Loss: 48.7708, L2 Loss: 0.0374, time: 6.9028\n",
      "Epoch [879/1200], Training Loss: 0.6589, vgg Loss: 60.5317, L2 Loss: 0.0536, time: 6.7840\n",
      "Epoch [880/1200], Training Loss: 0.5222, vgg Loss: 48.0733, L2 Loss: 0.0414, time: 6.8899\n",
      "Epoch [881/1200], Training Loss: 0.5667, vgg Loss: 52.3931, L2 Loss: 0.0428, time: 6.9153\n",
      "Epoch [882/1200], Training Loss: 0.6259, vgg Loss: 58.1936, L2 Loss: 0.0440, time: 6.9031\n",
      "Epoch [883/1200], Training Loss: 0.5341, vgg Loss: 49.6591, L2 Loss: 0.0375, time: 6.9227\n",
      "Epoch [884/1200], Training Loss: 0.7149, vgg Loss: 66.0522, L2 Loss: 0.0544, time: 6.9398\n",
      "Epoch [885/1200], Training Loss: 0.5291, vgg Loss: 48.6031, L2 Loss: 0.0431, time: 6.8685\n",
      "Epoch [886/1200], Training Loss: 0.5968, vgg Loss: 53.8573, L2 Loss: 0.0582, time: 6.9124\n",
      "Epoch [887/1200], Training Loss: 0.5868, vgg Loss: 54.3342, L2 Loss: 0.0434, time: 7.0634\n",
      "Epoch [888/1200], Training Loss: 0.5937, vgg Loss: 55.4267, L2 Loss: 0.0395, time: 6.9227\n",
      "Epoch [889/1200], Training Loss: 0.5761, vgg Loss: 53.8694, L2 Loss: 0.0374, time: 6.9231\n",
      "Epoch [890/1200], Training Loss: 0.5170, vgg Loss: 48.1491, L2 Loss: 0.0355, time: 6.8971\n",
      "Epoch [891/1200], Training Loss: 0.5373, vgg Loss: 50.1248, L2 Loss: 0.0360, time: 6.9840\n",
      "Epoch [892/1200], Training Loss: 0.5438, vgg Loss: 50.6556, L2 Loss: 0.0372, time: 6.9003\n",
      "Epoch [893/1200], Training Loss: 0.6063, vgg Loss: 56.3393, L2 Loss: 0.0429, time: 6.9478\n",
      "Epoch [894/1200], Training Loss: 0.5512, vgg Loss: 51.2618, L2 Loss: 0.0385, time: 6.9665\n",
      "Epoch [895/1200], Training Loss: 0.5824, vgg Loss: 54.2311, L2 Loss: 0.0400, time: 7.0508\n",
      "Epoch [896/1200], Training Loss: 0.6289, vgg Loss: 57.6454, L2 Loss: 0.0525, time: 6.9190\n",
      "Epoch [897/1200], Training Loss: 0.5568, vgg Loss: 51.8920, L2 Loss: 0.0379, time: 6.9121\n",
      "Epoch [898/1200], Training Loss: 0.5753, vgg Loss: 52.8029, L2 Loss: 0.0472, time: 6.8300\n",
      "Epoch [899/1200], Training Loss: 0.5557, vgg Loss: 51.4067, L2 Loss: 0.0416, time: 6.8236\n",
      "Epoch [900/1200], Training Loss: 0.6214, vgg Loss: 57.6672, L2 Loss: 0.0447, time: 6.9156\n",
      "Epoch [901/1200], Training Loss: 0.5592, vgg Loss: 51.6529, L2 Loss: 0.0427, time: 7.0665\n",
      "Epoch [902/1200], Training Loss: 0.7535, vgg Loss: 69.0712, L2 Loss: 0.0628, time: 6.8931\n",
      "Epoch [903/1200], Training Loss: 0.5793, vgg Loss: 53.6979, L2 Loss: 0.0423, time: 6.8709\n",
      "Epoch [904/1200], Training Loss: 0.5521, vgg Loss: 51.3404, L2 Loss: 0.0386, time: 6.8340\n",
      "Epoch [905/1200], Training Loss: 0.5984, vgg Loss: 55.4879, L2 Loss: 0.0435, time: 6.9461\n",
      "Epoch [906/1200], Training Loss: 0.5300, vgg Loss: 49.4347, L2 Loss: 0.0356, time: 6.9815\n",
      "Epoch [907/1200], Training Loss: 0.5361, vgg Loss: 49.8491, L2 Loss: 0.0377, time: 6.8691\n",
      "Epoch [908/1200], Training Loss: 0.5310, vgg Loss: 49.3781, L2 Loss: 0.0373, time: 6.9380\n",
      "Epoch [909/1200], Training Loss: 0.5570, vgg Loss: 51.4268, L2 Loss: 0.0427, time: 6.9381\n",
      "Epoch [910/1200], Training Loss: 0.6917, vgg Loss: 63.3497, L2 Loss: 0.0582, time: 6.8826\n",
      "Epoch [911/1200], Training Loss: 0.6268, vgg Loss: 57.9950, L2 Loss: 0.0468, time: 6.8743\n",
      "Epoch [912/1200], Training Loss: 0.5926, vgg Loss: 55.2636, L2 Loss: 0.0400, time: 6.9028\n",
      "Epoch [913/1200], Training Loss: 0.5500, vgg Loss: 50.7525, L2 Loss: 0.0425, time: 6.8694\n",
      "Epoch [914/1200], Training Loss: 0.5280, vgg Loss: 48.1686, L2 Loss: 0.0463, time: 6.9177\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [915/1200], Training Loss: 0.6359, vgg Loss: 58.7833, L2 Loss: 0.0480, time: 6.9108\n",
      "Epoch [916/1200], Training Loss: 0.5718, vgg Loss: 52.0592, L2 Loss: 0.0512, time: 6.9421\n",
      "Epoch [917/1200], Training Loss: 0.5570, vgg Loss: 51.4845, L2 Loss: 0.0421, time: 6.8005\n",
      "Epoch [918/1200], Training Loss: 0.5513, vgg Loss: 51.0200, L2 Loss: 0.0411, time: 7.1913\n",
      "Epoch [919/1200], Training Loss: 0.6591, vgg Loss: 60.0144, L2 Loss: 0.0590, time: 6.9234\n",
      "Epoch [920/1200], Training Loss: 0.5340, vgg Loss: 49.6309, L2 Loss: 0.0377, time: 7.0755\n",
      "Epoch [921/1200], Training Loss: 0.5349, vgg Loss: 49.5203, L2 Loss: 0.0397, time: 6.8788\n",
      "Epoch [922/1200], Training Loss: 0.5788, vgg Loss: 53.6068, L2 Loss: 0.0427, time: 6.9989\n",
      "Epoch [923/1200], Training Loss: 0.5250, vgg Loss: 48.9229, L2 Loss: 0.0358, time: 7.1747\n",
      "Epoch [924/1200], Training Loss: 0.4977, vgg Loss: 46.3457, L2 Loss: 0.0342, time: 7.1589\n",
      "Epoch [925/1200], Training Loss: 0.5381, vgg Loss: 50.1064, L2 Loss: 0.0370, time: 7.6956\n",
      "Epoch [926/1200], Training Loss: 0.5557, vgg Loss: 52.1555, L2 Loss: 0.0342, time: 7.4776\n",
      "Epoch [927/1200], Training Loss: 0.5883, vgg Loss: 54.6747, L2 Loss: 0.0415, time: 7.5449\n",
      "Epoch [928/1200], Training Loss: 0.6143, vgg Loss: 56.6741, L2 Loss: 0.0476, time: 8.4198\n",
      "Epoch [929/1200], Training Loss: 0.6724, vgg Loss: 61.4804, L2 Loss: 0.0576, time: 8.1658\n",
      "Epoch [930/1200], Training Loss: 0.5829, vgg Loss: 53.6133, L2 Loss: 0.0468, time: 7.5254\n",
      "Epoch [931/1200], Training Loss: 0.5639, vgg Loss: 52.3928, L2 Loss: 0.0399, time: 7.3654\n",
      "Epoch [932/1200], Training Loss: 0.5705, vgg Loss: 53.0261, L2 Loss: 0.0402, time: 7.3338\n",
      "Epoch [933/1200], Training Loss: 0.6635, vgg Loss: 60.2320, L2 Loss: 0.0612, time: 7.3867\n",
      "Epoch [934/1200], Training Loss: 0.5961, vgg Loss: 55.2111, L2 Loss: 0.0440, time: 8.0617\n",
      "Epoch [935/1200], Training Loss: 0.5760, vgg Loss: 53.3407, L2 Loss: 0.0426, time: 7.5504\n",
      "Epoch [936/1200], Training Loss: 0.5862, vgg Loss: 54.6726, L2 Loss: 0.0395, time: 7.3472\n",
      "Epoch [937/1200], Training Loss: 0.5728, vgg Loss: 53.2322, L2 Loss: 0.0405, time: 7.3650\n",
      "Epoch [938/1200], Training Loss: 0.6107, vgg Loss: 56.6205, L2 Loss: 0.0445, time: 7.4041\n",
      "Epoch [939/1200], Training Loss: 0.4819, vgg Loss: 44.5911, L2 Loss: 0.0360, time: 7.6070\n",
      "Epoch [940/1200], Training Loss: 0.6195, vgg Loss: 56.8190, L2 Loss: 0.0513, time: 7.5302\n",
      "Epoch [941/1200], Training Loss: 0.5786, vgg Loss: 53.6665, L2 Loss: 0.0419, time: 7.5669\n",
      "Epoch [942/1200], Training Loss: 0.5420, vgg Loss: 50.2597, L2 Loss: 0.0394, time: 7.5929\n",
      "Epoch [943/1200], Training Loss: 0.5935, vgg Loss: 55.4787, L2 Loss: 0.0387, time: 7.2244\n",
      "Epoch [944/1200], Training Loss: 0.5139, vgg Loss: 47.9892, L2 Loss: 0.0340, time: 7.6967\n",
      "Epoch [945/1200], Training Loss: 0.5671, vgg Loss: 52.0729, L2 Loss: 0.0464, time: 7.4428\n",
      "Epoch [946/1200], Training Loss: 0.5217, vgg Loss: 47.6580, L2 Loss: 0.0451, time: 7.4682\n",
      "Epoch [947/1200], Training Loss: 0.5132, vgg Loss: 47.8672, L2 Loss: 0.0346, time: 7.6519\n",
      "Epoch [948/1200], Training Loss: 0.5263, vgg Loss: 48.7432, L2 Loss: 0.0389, time: 7.6296\n",
      "Epoch [949/1200], Training Loss: 0.5441, vgg Loss: 49.7862, L2 Loss: 0.0463, time: 8.3218\n",
      "Epoch [950/1200], Training Loss: 0.6105, vgg Loss: 56.2046, L2 Loss: 0.0484, time: 7.5486\n",
      "Epoch [951/1200], Training Loss: 0.6047, vgg Loss: 56.0663, L2 Loss: 0.0440, time: 7.4448\n",
      "Epoch [952/1200], Training Loss: 0.6090, vgg Loss: 56.4233, L2 Loss: 0.0448, time: 8.0274\n",
      "Epoch [953/1200], Training Loss: 0.5742, vgg Loss: 52.5477, L2 Loss: 0.0488, time: 7.9380\n",
      "Epoch [954/1200], Training Loss: 0.5165, vgg Loss: 47.4169, L2 Loss: 0.0423, time: 7.7921\n",
      "Epoch [955/1200], Training Loss: 0.5462, vgg Loss: 50.2850, L2 Loss: 0.0434, time: 7.5793\n",
      "Epoch [956/1200], Training Loss: 0.5289, vgg Loss: 48.8650, L2 Loss: 0.0402, time: 7.6303\n",
      "Epoch [957/1200], Training Loss: 0.6137, vgg Loss: 56.6560, L2 Loss: 0.0471, time: 7.5689\n",
      "Epoch [958/1200], Training Loss: 0.5134, vgg Loss: 47.5929, L2 Loss: 0.0375, time: 8.2030\n",
      "Epoch [959/1200], Training Loss: 0.5569, vgg Loss: 51.7751, L2 Loss: 0.0391, time: 7.7530\n",
      "Epoch [960/1200], Training Loss: 0.5476, vgg Loss: 51.1393, L2 Loss: 0.0362, time: 7.7066\n",
      "Epoch [961/1200], Training Loss: 0.6571, vgg Loss: 60.7159, L2 Loss: 0.0500, time: 7.6841\n",
      "Epoch [962/1200], Training Loss: 0.5894, vgg Loss: 54.5350, L2 Loss: 0.0441, time: 7.5803\n",
      "Epoch [963/1200], Training Loss: 0.5443, vgg Loss: 50.4003, L2 Loss: 0.0403, time: 8.0877\n",
      "Epoch [964/1200], Training Loss: 0.6660, vgg Loss: 60.9694, L2 Loss: 0.0563, time: 7.7671\n",
      "Epoch [965/1200], Training Loss: 0.5610, vgg Loss: 51.5653, L2 Loss: 0.0454, time: 7.6489\n",
      "Epoch [966/1200], Training Loss: 0.5481, vgg Loss: 50.0218, L2 Loss: 0.0478, time: 7.8449\n",
      "Epoch [967/1200], Training Loss: 0.5087, vgg Loss: 47.4069, L2 Loss: 0.0347, time: 7.8241\n",
      "Epoch [968/1200], Training Loss: 0.5413, vgg Loss: 49.6992, L2 Loss: 0.0443, time: 7.6679\n",
      "Epoch [969/1200], Training Loss: 0.5949, vgg Loss: 54.3890, L2 Loss: 0.0510, time: 7.9961\n",
      "Epoch [970/1200], Training Loss: 0.6477, vgg Loss: 59.8175, L2 Loss: 0.0495, time: 7.5278\n",
      "Epoch [971/1200], Training Loss: 0.5213, vgg Loss: 48.6952, L2 Loss: 0.0343, time: 7.5114\n",
      "Epoch [972/1200], Training Loss: 0.5149, vgg Loss: 47.7129, L2 Loss: 0.0378, time: 7.5785\n",
      "Epoch [973/1200], Training Loss: 0.6278, vgg Loss: 58.2402, L2 Loss: 0.0454, time: 7.4362\n",
      "Epoch [974/1200], Training Loss: 0.6009, vgg Loss: 54.5064, L2 Loss: 0.0558, time: 7.6013\n",
      "Epoch [975/1200], Training Loss: 0.6236, vgg Loss: 56.8161, L2 Loss: 0.0555, time: 7.4909\n",
      "Epoch [976/1200], Training Loss: 0.6735, vgg Loss: 61.6373, L2 Loss: 0.0571, time: 7.4251\n",
      "Epoch [977/1200], Training Loss: 0.5480, vgg Loss: 49.9680, L2 Loss: 0.0483, time: 7.4460\n",
      "Epoch [978/1200], Training Loss: 0.6152, vgg Loss: 55.9015, L2 Loss: 0.0562, time: 7.5708\n",
      "Epoch [979/1200], Training Loss: 0.5278, vgg Loss: 48.3773, L2 Loss: 0.0441, time: 7.3596\n",
      "Epoch [980/1200], Training Loss: 0.5783, vgg Loss: 53.8691, L2 Loss: 0.0396, time: 7.5312\n",
      "Epoch [981/1200], Training Loss: 0.5452, vgg Loss: 50.6003, L2 Loss: 0.0392, time: 7.3846\n",
      "Epoch [982/1200], Training Loss: 0.5276, vgg Loss: 49.2581, L2 Loss: 0.0350, time: 7.3614\n",
      "Epoch [983/1200], Training Loss: 0.5848, vgg Loss: 54.1406, L2 Loss: 0.0433, time: 7.5624\n",
      "Epoch [984/1200], Training Loss: 0.5691, vgg Loss: 52.7950, L2 Loss: 0.0411, time: 7.5178\n",
      "Epoch [985/1200], Training Loss: 0.5091, vgg Loss: 47.2181, L2 Loss: 0.0369, time: 7.7768\n",
      "Epoch [986/1200], Training Loss: 0.5525, vgg Loss: 50.8820, L2 Loss: 0.0436, time: 7.6038\n",
      "Epoch [987/1200], Training Loss: 0.6484, vgg Loss: 59.4864, L2 Loss: 0.0535, time: 7.4482\n",
      "Epoch [988/1200], Training Loss: 0.4932, vgg Loss: 45.0023, L2 Loss: 0.0431, time: 7.5051\n",
      "Epoch [989/1200], Training Loss: 0.6284, vgg Loss: 57.6482, L2 Loss: 0.0520, time: 7.2201\n",
      "Epoch [990/1200], Training Loss: 0.5368, vgg Loss: 49.2062, L2 Loss: 0.0447, time: 7.5010\n",
      "Epoch [991/1200], Training Loss: 0.5571, vgg Loss: 51.4016, L2 Loss: 0.0431, time: 7.5299\n",
      "Epoch [992/1200], Training Loss: 0.6026, vgg Loss: 55.3946, L2 Loss: 0.0486, time: 7.2850\n",
      "Epoch [993/1200], Training Loss: 0.5792, vgg Loss: 53.1846, L2 Loss: 0.0474, time: 7.4055\n",
      "Epoch [994/1200], Training Loss: 0.5105, vgg Loss: 46.9477, L2 Loss: 0.0410, time: 7.3761\n",
      "Epoch [995/1200], Training Loss: 0.5341, vgg Loss: 48.7810, L2 Loss: 0.0463, time: 7.5724\n",
      "Epoch [996/1200], Training Loss: 0.5409, vgg Loss: 49.8566, L2 Loss: 0.0423, time: 7.4368\n",
      "Epoch [997/1200], Training Loss: 0.5635, vgg Loss: 52.2914, L2 Loss: 0.0405, time: 7.4643\n",
      "Epoch [998/1200], Training Loss: 0.5486, vgg Loss: 50.7039, L2 Loss: 0.0416, time: 7.6336\n",
      "Epoch [999/1200], Training Loss: 0.6717, vgg Loss: 62.0183, L2 Loss: 0.0515, time: 7.5976\n",
      "Epoch [1000/1200], Training Loss: 0.5463, vgg Loss: 50.4650, L2 Loss: 0.0416, time: 7.3774\n",
      "Epoch [1001/1200], Training Loss: 0.5830, vgg Loss: 54.0414, L2 Loss: 0.0426, time: 7.3330\n",
      "Epoch [1002/1200], Training Loss: 0.5428, vgg Loss: 50.5559, L2 Loss: 0.0373, time: 7.4103\n",
      "Epoch [1003/1200], Training Loss: 0.5793, vgg Loss: 53.1583, L2 Loss: 0.0477, time: 7.5699\n",
      "Epoch [1004/1200], Training Loss: 0.5670, vgg Loss: 52.4122, L2 Loss: 0.0429, time: 7.5333\n",
      "Epoch [1005/1200], Training Loss: 0.5572, vgg Loss: 51.6740, L2 Loss: 0.0404, time: 7.5348\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1006/1200], Training Loss: 0.5078, vgg Loss: 46.6642, L2 Loss: 0.0411, time: 7.4890\n",
      "Epoch [1007/1200], Training Loss: 0.5510, vgg Loss: 51.1553, L2 Loss: 0.0395, time: 7.5036\n",
      "Epoch [1008/1200], Training Loss: 0.5513, vgg Loss: 51.2121, L2 Loss: 0.0391, time: 7.4368\n",
      "Epoch [1009/1200], Training Loss: 0.5332, vgg Loss: 49.4826, L2 Loss: 0.0384, time: 7.5017\n",
      "Epoch [1010/1200], Training Loss: 0.5851, vgg Loss: 54.0737, L2 Loss: 0.0444, time: 7.2856\n",
      "Epoch [1011/1200], Training Loss: 0.6987, vgg Loss: 63.9723, L2 Loss: 0.0590, time: 7.3847\n",
      "Epoch [1012/1200], Training Loss: 0.5530, vgg Loss: 51.0202, L2 Loss: 0.0428, time: 7.4158\n",
      "Epoch [1013/1200], Training Loss: 0.5385, vgg Loss: 49.5729, L2 Loss: 0.0428, time: 7.5995\n",
      "Epoch [1014/1200], Training Loss: 0.5784, vgg Loss: 52.8345, L2 Loss: 0.0500, time: 7.4698\n",
      "Epoch [1015/1200], Training Loss: 0.6134, vgg Loss: 56.7670, L2 Loss: 0.0457, time: 7.4518\n",
      "Epoch [1016/1200], Training Loss: 0.5210, vgg Loss: 48.3365, L2 Loss: 0.0376, time: 7.4159\n",
      "Epoch [1017/1200], Training Loss: 0.5260, vgg Loss: 48.3027, L2 Loss: 0.0430, time: 7.5041\n",
      "Epoch [1018/1200], Training Loss: 0.5374, vgg Loss: 49.2278, L2 Loss: 0.0452, time: 7.4645\n",
      "Epoch [1019/1200], Training Loss: 0.6193, vgg Loss: 56.6733, L2 Loss: 0.0526, time: 7.5905\n",
      "Epoch [1020/1200], Training Loss: 0.4966, vgg Loss: 45.8748, L2 Loss: 0.0379, time: 7.5238\n",
      "Epoch [1021/1200], Training Loss: 0.5404, vgg Loss: 50.1783, L2 Loss: 0.0386, time: 7.4493\n",
      "Epoch [1022/1200], Training Loss: 0.5222, vgg Loss: 48.0450, L2 Loss: 0.0417, time: 7.5353\n",
      "Epoch [1023/1200], Training Loss: 0.5828, vgg Loss: 53.3927, L2 Loss: 0.0488, time: 7.4005\n",
      "Epoch [1024/1200], Training Loss: 0.5357, vgg Loss: 48.9663, L2 Loss: 0.0460, time: 7.3926\n",
      "Epoch [1025/1200], Training Loss: 0.5773, vgg Loss: 52.4323, L2 Loss: 0.0530, time: 7.4410\n",
      "Epoch [1026/1200], Training Loss: 0.5791, vgg Loss: 53.8846, L2 Loss: 0.0402, time: 7.4968\n",
      "Epoch [1027/1200], Training Loss: 0.5180, vgg Loss: 47.9864, L2 Loss: 0.0381, time: 7.2039\n",
      "Epoch [1028/1200], Training Loss: 0.5340, vgg Loss: 49.4028, L2 Loss: 0.0399, time: 7.4476\n",
      "Epoch [1029/1200], Training Loss: 0.5188, vgg Loss: 47.0062, L2 Loss: 0.0488, time: 7.4077\n",
      "Epoch [1030/1200], Training Loss: 0.6380, vgg Loss: 58.6756, L2 Loss: 0.0512, time: 7.4118\n",
      "Epoch [1031/1200], Training Loss: 0.5370, vgg Loss: 49.6229, L2 Loss: 0.0408, time: 7.4305\n",
      "Epoch [1032/1200], Training Loss: 0.5300, vgg Loss: 48.1214, L2 Loss: 0.0488, time: 7.5238\n",
      "Epoch [1033/1200], Training Loss: 0.5680, vgg Loss: 52.4738, L2 Loss: 0.0432, time: 7.5607\n",
      "Epoch [1034/1200], Training Loss: 0.5147, vgg Loss: 48.0238, L2 Loss: 0.0344, time: 7.5351\n",
      "Epoch [1035/1200], Training Loss: 0.5503, vgg Loss: 50.9035, L2 Loss: 0.0413, time: 7.6667\n",
      "Epoch [1036/1200], Training Loss: 0.5344, vgg Loss: 49.3254, L2 Loss: 0.0412, time: 7.6006\n",
      "Epoch [1037/1200], Training Loss: 0.5253, vgg Loss: 48.7493, L2 Loss: 0.0378, time: 7.5297\n",
      "Epoch [1038/1200], Training Loss: 0.5580, vgg Loss: 51.7242, L2 Loss: 0.0407, time: 7.7491\n",
      "Epoch [1039/1200], Training Loss: 0.5483, vgg Loss: 50.2661, L2 Loss: 0.0457, time: 8.0760\n",
      "Epoch [1040/1200], Training Loss: 0.5679, vgg Loss: 52.4118, L2 Loss: 0.0437, time: 7.5869\n",
      "Epoch [1041/1200], Training Loss: 0.5128, vgg Loss: 47.4144, L2 Loss: 0.0387, time: 7.5308\n",
      "Epoch [1042/1200], Training Loss: 0.5444, vgg Loss: 50.3085, L2 Loss: 0.0413, time: 7.4470\n",
      "Epoch [1043/1200], Training Loss: 0.5255, vgg Loss: 48.9799, L2 Loss: 0.0357, time: 7.6248\n",
      "Epoch [1044/1200], Training Loss: 0.5686, vgg Loss: 52.7462, L2 Loss: 0.0412, time: 7.3937\n",
      "Epoch [1045/1200], Training Loss: 0.6424, vgg Loss: 59.4217, L2 Loss: 0.0482, time: 7.4613\n",
      "Epoch [1046/1200], Training Loss: 0.5494, vgg Loss: 51.0612, L2 Loss: 0.0388, time: 7.4954\n",
      "Epoch [1047/1200], Training Loss: 0.5426, vgg Loss: 50.4924, L2 Loss: 0.0376, time: 7.4928\n",
      "Epoch [1048/1200], Training Loss: 0.5050, vgg Loss: 46.7972, L2 Loss: 0.0370, time: 7.5624\n",
      "Epoch [1049/1200], Training Loss: 0.4850, vgg Loss: 44.9354, L2 Loss: 0.0357, time: 7.8888\n",
      "Epoch [1050/1200], Training Loss: 0.5480, vgg Loss: 50.6037, L2 Loss: 0.0420, time: 7.5880\n",
      "Epoch [1051/1200], Training Loss: 0.5428, vgg Loss: 50.4314, L2 Loss: 0.0385, time: 7.6127\n",
      "Epoch [1052/1200], Training Loss: 0.5707, vgg Loss: 52.2704, L2 Loss: 0.0480, time: 7.5364\n",
      "Epoch [1053/1200], Training Loss: 0.4950, vgg Loss: 45.7678, L2 Loss: 0.0373, time: 7.6010\n",
      "Epoch [1054/1200], Training Loss: 0.4989, vgg Loss: 46.7060, L2 Loss: 0.0318, time: 7.6263\n",
      "Epoch [1055/1200], Training Loss: 0.5376, vgg Loss: 49.9970, L2 Loss: 0.0377, time: 7.5484\n",
      "Epoch [1056/1200], Training Loss: 0.5904, vgg Loss: 54.4465, L2 Loss: 0.0459, time: 7.3339\n",
      "Epoch [1057/1200], Training Loss: 0.5513, vgg Loss: 51.3504, L2 Loss: 0.0378, time: 7.6220\n",
      "Epoch [1058/1200], Training Loss: 0.5026, vgg Loss: 46.9871, L2 Loss: 0.0328, time: 7.4337\n",
      "Epoch [1059/1200], Training Loss: 0.5561, vgg Loss: 51.6525, L2 Loss: 0.0395, time: 7.4469\n",
      "Epoch [1060/1200], Training Loss: 0.5819, vgg Loss: 54.0555, L2 Loss: 0.0413, time: 7.4320\n",
      "Epoch [1061/1200], Training Loss: 0.5281, vgg Loss: 49.0833, L2 Loss: 0.0373, time: 7.6739\n",
      "Epoch [1062/1200], Training Loss: 0.5146, vgg Loss: 48.0617, L2 Loss: 0.0340, time: 7.5670\n",
      "Epoch [1063/1200], Training Loss: 0.5184, vgg Loss: 48.3633, L2 Loss: 0.0348, time: 7.5991\n",
      "Epoch [1064/1200], Training Loss: 0.6139, vgg Loss: 56.8511, L2 Loss: 0.0454, time: 7.4411\n",
      "Epoch [1065/1200], Training Loss: 0.5511, vgg Loss: 51.3400, L2 Loss: 0.0377, time: 7.4624\n",
      "Epoch [1066/1200], Training Loss: 0.5397, vgg Loss: 49.9037, L2 Loss: 0.0407, time: 7.4533\n",
      "Epoch [1067/1200], Training Loss: 0.4872, vgg Loss: 45.4555, L2 Loss: 0.0326, time: 7.2339\n",
      "Epoch [1068/1200], Training Loss: 0.5274, vgg Loss: 48.9113, L2 Loss: 0.0383, time: 7.5092\n",
      "Epoch [1069/1200], Training Loss: 0.5608, vgg Loss: 51.6561, L2 Loss: 0.0443, time: 7.5356\n",
      "Epoch [1070/1200], Training Loss: 0.5705, vgg Loss: 52.1808, L2 Loss: 0.0487, time: 7.6756\n",
      "Epoch [1071/1200], Training Loss: 0.5529, vgg Loss: 51.0869, L2 Loss: 0.0420, time: 7.3901\n",
      "Epoch [1072/1200], Training Loss: 0.5357, vgg Loss: 49.4288, L2 Loss: 0.0415, time: 7.5320\n",
      "Epoch [1073/1200], Training Loss: 0.5509, vgg Loss: 50.7178, L2 Loss: 0.0437, time: 7.5667\n",
      "Epoch [1074/1200], Training Loss: 0.5796, vgg Loss: 53.9110, L2 Loss: 0.0405, time: 7.6291\n",
      "Epoch [1075/1200], Training Loss: 0.5895, vgg Loss: 54.6929, L2 Loss: 0.0426, time: 7.5900\n",
      "Epoch [1076/1200], Training Loss: 0.5361, vgg Loss: 49.6586, L2 Loss: 0.0395, time: 7.2750\n",
      "Epoch [1077/1200], Training Loss: 0.5382, vgg Loss: 50.0942, L2 Loss: 0.0373, time: 7.5904\n",
      "Epoch [1078/1200], Training Loss: 0.5051, vgg Loss: 46.6997, L2 Loss: 0.0381, time: 7.5068\n",
      "Epoch [1079/1200], Training Loss: 0.4650, vgg Loss: 43.1526, L2 Loss: 0.0335, time: 7.4835\n",
      "Epoch [1080/1200], Training Loss: 0.5411, vgg Loss: 50.1191, L2 Loss: 0.0399, time: 7.4775\n",
      "Epoch [1081/1200], Training Loss: 0.5532, vgg Loss: 50.6557, L2 Loss: 0.0467, time: 7.2257\n",
      "Epoch [1082/1200], Training Loss: 0.5852, vgg Loss: 53.6378, L2 Loss: 0.0488, time: 7.3975\n",
      "Epoch [1083/1200], Training Loss: 0.5243, vgg Loss: 46.8797, L2 Loss: 0.0555, time: 7.4317\n",
      "Epoch [1084/1200], Training Loss: 0.5699, vgg Loss: 51.6383, L2 Loss: 0.0535, time: 7.5383\n",
      "Epoch [1085/1200], Training Loss: 0.5812, vgg Loss: 52.8750, L2 Loss: 0.0525, time: 7.4883\n",
      "Epoch [1086/1200], Training Loss: 0.5847, vgg Loss: 54.0755, L2 Loss: 0.0439, time: 7.4311\n",
      "Epoch [1087/1200], Training Loss: 0.5243, vgg Loss: 48.3973, L2 Loss: 0.0403, time: 7.5262\n",
      "Epoch [1088/1200], Training Loss: 0.5223, vgg Loss: 48.0093, L2 Loss: 0.0422, time: 7.4520\n",
      "Epoch [1089/1200], Training Loss: 0.4960, vgg Loss: 45.9183, L2 Loss: 0.0368, time: 7.3852\n",
      "Epoch [1090/1200], Training Loss: 0.5411, vgg Loss: 50.0698, L2 Loss: 0.0404, time: 7.5816\n",
      "Epoch [1091/1200], Training Loss: 0.6022, vgg Loss: 55.8211, L2 Loss: 0.0440, time: 7.2378\n",
      "Epoch [1092/1200], Training Loss: 0.6207, vgg Loss: 56.6691, L2 Loss: 0.0540, time: 7.4743\n",
      "Epoch [1093/1200], Training Loss: 0.4854, vgg Loss: 44.9498, L2 Loss: 0.0359, time: 7.4720\n",
      "Epoch [1094/1200], Training Loss: 0.5128, vgg Loss: 47.2297, L2 Loss: 0.0405, time: 7.5314\n",
      "Epoch [1095/1200], Training Loss: 0.5099, vgg Loss: 46.9387, L2 Loss: 0.0405, time: 7.5282\n",
      "Epoch [1096/1200], Training Loss: 0.6090, vgg Loss: 56.2892, L2 Loss: 0.0462, time: 7.5901\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1097/1200], Training Loss: 0.5557, vgg Loss: 51.3229, L2 Loss: 0.0425, time: 7.5006\n",
      "Epoch [1098/1200], Training Loss: 0.5488, vgg Loss: 50.6637, L2 Loss: 0.0422, time: 7.2013\n",
      "Epoch [1099/1200], Training Loss: 0.5508, vgg Loss: 50.7999, L2 Loss: 0.0428, time: 7.4399\n",
      "Epoch [1100/1200], Training Loss: 0.5960, vgg Loss: 54.6335, L2 Loss: 0.0496, time: 7.4905\n",
      "Epoch [1101/1200], Training Loss: 0.5402, vgg Loss: 50.1157, L2 Loss: 0.0391, time: 7.4259\n",
      "Epoch [1102/1200], Training Loss: 0.5717, vgg Loss: 52.5230, L2 Loss: 0.0464, time: 7.2394\n",
      "Epoch [1103/1200], Training Loss: 0.5889, vgg Loss: 54.4132, L2 Loss: 0.0447, time: 7.5125\n",
      "Epoch [1104/1200], Training Loss: 0.5216, vgg Loss: 48.4415, L2 Loss: 0.0372, time: 7.6054\n",
      "Epoch [1105/1200], Training Loss: 0.5301, vgg Loss: 48.8052, L2 Loss: 0.0421, time: 7.4479\n",
      "Epoch [1106/1200], Training Loss: 0.5024, vgg Loss: 46.8423, L2 Loss: 0.0339, time: 7.4838\n",
      "Epoch [1107/1200], Training Loss: 0.5322, vgg Loss: 49.2027, L2 Loss: 0.0401, time: 7.4981\n",
      "Epoch [1108/1200], Training Loss: 0.5445, vgg Loss: 50.6214, L2 Loss: 0.0383, time: 7.4738\n",
      "Epoch [1109/1200], Training Loss: 0.5121, vgg Loss: 47.6122, L2 Loss: 0.0360, time: 7.5468\n",
      "Epoch [1110/1200], Training Loss: 0.5669, vgg Loss: 52.3401, L2 Loss: 0.0435, time: 7.5130\n",
      "Epoch [1111/1200], Training Loss: 0.5796, vgg Loss: 53.9802, L2 Loss: 0.0398, time: 7.3977\n",
      "Epoch [1112/1200], Training Loss: 0.5904, vgg Loss: 54.9095, L2 Loss: 0.0413, time: 7.5450\n",
      "Epoch [1113/1200], Training Loss: 0.5369, vgg Loss: 49.8438, L2 Loss: 0.0385, time: 7.3168\n",
      "Epoch [1114/1200], Training Loss: 0.5557, vgg Loss: 51.8081, L2 Loss: 0.0376, time: 7.5438\n",
      "Epoch [1115/1200], Training Loss: 0.5301, vgg Loss: 49.4969, L2 Loss: 0.0351, time: 7.5324\n",
      "Epoch [1116/1200], Training Loss: 0.5555, vgg Loss: 51.3525, L2 Loss: 0.0419, time: 7.5413\n",
      "Epoch [1117/1200], Training Loss: 0.6274, vgg Loss: 57.2202, L2 Loss: 0.0552, time: 7.5080\n",
      "Epoch [1118/1200], Training Loss: 0.5607, vgg Loss: 50.3442, L2 Loss: 0.0573, time: 7.4909\n",
      "Epoch [1119/1200], Training Loss: 0.6273, vgg Loss: 53.8835, L2 Loss: 0.0885, time: 7.1143\n",
      "Epoch [1120/1200], Training Loss: 0.7364, vgg Loss: 58.7315, L2 Loss: 0.1490, time: 7.4674\n",
      "Epoch [1121/1200], Training Loss: 0.6929, vgg Loss: 59.1561, L2 Loss: 0.1014, time: 7.5155\n",
      "Epoch [1122/1200], Training Loss: 0.5720, vgg Loss: 51.0964, L2 Loss: 0.0610, time: 7.5192\n",
      "Epoch [1123/1200], Training Loss: 0.5654, vgg Loss: 51.7790, L2 Loss: 0.0476, time: 7.4426\n",
      "Epoch [1124/1200], Training Loss: 0.5555, vgg Loss: 51.2462, L2 Loss: 0.0430, time: 7.4014\n",
      "Epoch [1125/1200], Training Loss: 0.4880, vgg Loss: 45.3801, L2 Loss: 0.0342, time: 7.4961\n",
      "Epoch [1126/1200], Training Loss: 0.5523, vgg Loss: 50.6190, L2 Loss: 0.0461, time: 7.5110\n",
      "Epoch [1127/1200], Training Loss: 0.5701, vgg Loss: 52.5243, L2 Loss: 0.0448, time: 7.5045\n",
      "Epoch [1128/1200], Training Loss: 0.5246, vgg Loss: 48.6342, L2 Loss: 0.0382, time: 7.6514\n",
      "Epoch [1129/1200], Training Loss: 0.5610, vgg Loss: 51.3110, L2 Loss: 0.0478, time: 7.3036\n",
      "Epoch [1130/1200], Training Loss: 0.5665, vgg Loss: 51.6222, L2 Loss: 0.0503, time: 7.5515\n",
      "Epoch [1131/1200], Training Loss: 0.5526, vgg Loss: 51.0984, L2 Loss: 0.0416, time: 7.3838\n",
      "Epoch [1132/1200], Training Loss: 0.5194, vgg Loss: 48.1717, L2 Loss: 0.0377, time: 7.4936\n",
      "Epoch [1133/1200], Training Loss: 0.5132, vgg Loss: 47.5487, L2 Loss: 0.0377, time: 7.5250\n",
      "Epoch [1134/1200], Training Loss: 0.4886, vgg Loss: 45.3262, L2 Loss: 0.0353, time: 7.4412\n",
      "Epoch [1135/1200], Training Loss: 0.4915, vgg Loss: 45.8099, L2 Loss: 0.0334, time: 7.5326\n",
      "Epoch [1136/1200], Training Loss: 0.5590, vgg Loss: 51.0430, L2 Loss: 0.0486, time: 7.6907\n",
      "Epoch [1137/1200], Training Loss: 0.6014, vgg Loss: 55.0743, L2 Loss: 0.0506, time: 7.5101\n",
      "Epoch [1138/1200], Training Loss: 0.5017, vgg Loss: 46.2674, L2 Loss: 0.0390, time: 7.5712\n",
      "Epoch [1139/1200], Training Loss: 0.4920, vgg Loss: 45.8276, L2 Loss: 0.0338, time: 7.4124\n",
      "Epoch [1140/1200], Training Loss: 0.4916, vgg Loss: 45.6946, L2 Loss: 0.0346, time: 7.4716\n",
      "Epoch [1141/1200], Training Loss: 0.5011, vgg Loss: 46.8072, L2 Loss: 0.0331, time: 7.4883\n",
      "Epoch [1142/1200], Training Loss: 0.5807, vgg Loss: 53.4722, L2 Loss: 0.0459, time: 7.5171\n",
      "Epoch [1143/1200], Training Loss: 0.5116, vgg Loss: 47.6574, L2 Loss: 0.0350, time: 7.4947\n",
      "Epoch [1144/1200], Training Loss: 0.5114, vgg Loss: 47.4254, L2 Loss: 0.0371, time: 7.4196\n",
      "Epoch [1145/1200], Training Loss: 0.5170, vgg Loss: 47.8876, L2 Loss: 0.0381, time: 7.4410\n",
      "Epoch [1146/1200], Training Loss: 0.4875, vgg Loss: 44.9093, L2 Loss: 0.0384, time: 7.4495\n",
      "Epoch [1147/1200], Training Loss: 0.5882, vgg Loss: 53.0965, L2 Loss: 0.0572, time: 7.4965\n",
      "Epoch [1148/1200], Training Loss: 0.5105, vgg Loss: 46.2734, L2 Loss: 0.0478, time: 7.1615\n",
      "Epoch [1149/1200], Training Loss: 0.5601, vgg Loss: 51.2256, L2 Loss: 0.0478, time: 7.4794\n",
      "Epoch [1150/1200], Training Loss: 0.5778, vgg Loss: 53.2415, L2 Loss: 0.0454, time: 7.5408\n",
      "Epoch [1151/1200], Training Loss: 0.5198, vgg Loss: 48.4483, L2 Loss: 0.0353, time: 7.5277\n",
      "Epoch [1152/1200], Training Loss: 0.6005, vgg Loss: 55.6736, L2 Loss: 0.0438, time: 7.6490\n",
      "Epoch [1153/1200], Training Loss: 0.5457, vgg Loss: 50.1892, L2 Loss: 0.0438, time: 7.5069\n",
      "Epoch [1154/1200], Training Loss: 0.5190, vgg Loss: 47.4619, L2 Loss: 0.0443, time: 7.5891\n",
      "Epoch [1155/1200], Training Loss: 0.5396, vgg Loss: 49.3025, L2 Loss: 0.0465, time: 7.5424\n",
      "Epoch [1156/1200], Training Loss: 0.5640, vgg Loss: 52.0419, L2 Loss: 0.0436, time: 7.5946\n",
      "Epoch [1157/1200], Training Loss: 0.5064, vgg Loss: 46.7384, L2 Loss: 0.0391, time: 7.4378\n",
      "Epoch [1158/1200], Training Loss: 0.5284, vgg Loss: 48.4611, L2 Loss: 0.0438, time: 7.5386\n",
      "Epoch [1159/1200], Training Loss: 0.5721, vgg Loss: 53.0136, L2 Loss: 0.0420, time: 7.5650\n",
      "Epoch [1160/1200], Training Loss: 0.5536, vgg Loss: 50.1924, L2 Loss: 0.0516, time: 7.5227\n",
      "Epoch [1161/1200], Training Loss: 0.5056, vgg Loss: 45.7426, L2 Loss: 0.0482, time: 7.5663\n",
      "Epoch [1162/1200], Training Loss: 0.5969, vgg Loss: 52.8341, L2 Loss: 0.0686, time: 7.5492\n",
      "Epoch [1163/1200], Training Loss: 0.5628, vgg Loss: 51.1399, L2 Loss: 0.0514, time: 7.4786\n",
      "Epoch [1164/1200], Training Loss: 0.5755, vgg Loss: 53.0275, L2 Loss: 0.0452, time: 7.4937\n",
      "Epoch [1165/1200], Training Loss: 0.4563, vgg Loss: 42.5595, L2 Loss: 0.0307, time: 7.5120\n",
      "Epoch [1166/1200], Training Loss: 0.4697, vgg Loss: 43.6502, L2 Loss: 0.0332, time: 7.5078\n",
      "Epoch [1167/1200], Training Loss: 0.5355, vgg Loss: 49.5467, L2 Loss: 0.0401, time: 7.4088\n",
      "Epoch [1168/1200], Training Loss: 0.5007, vgg Loss: 46.6533, L2 Loss: 0.0342, time: 7.6211\n",
      "Epoch [1169/1200], Training Loss: 0.5346, vgg Loss: 49.6899, L2 Loss: 0.0377, time: 7.4036\n",
      "Epoch [1170/1200], Training Loss: 0.5589, vgg Loss: 52.0198, L2 Loss: 0.0387, time: 7.6599\n",
      "Epoch [1171/1200], Training Loss: 0.5243, vgg Loss: 48.8102, L2 Loss: 0.0362, time: 7.7567\n",
      "Epoch [1172/1200], Training Loss: 0.5185, vgg Loss: 48.1874, L2 Loss: 0.0366, time: 7.2819\n",
      "Epoch [1173/1200], Training Loss: 0.4760, vgg Loss: 44.0903, L2 Loss: 0.0351, time: 7.5490\n",
      "Epoch [1174/1200], Training Loss: 0.4817, vgg Loss: 44.8717, L2 Loss: 0.0330, time: 7.4730\n",
      "Epoch [1175/1200], Training Loss: 0.5135, vgg Loss: 48.0063, L2 Loss: 0.0334, time: 7.3423\n",
      "Epoch [1176/1200], Training Loss: 0.5357, vgg Loss: 49.4272, L2 Loss: 0.0414, time: 7.6031\n",
      "Epoch [1177/1200], Training Loss: 0.4653, vgg Loss: 42.8316, L2 Loss: 0.0369, time: 7.4650\n",
      "Epoch [1178/1200], Training Loss: 0.5044, vgg Loss: 46.5889, L2 Loss: 0.0386, time: 7.5037\n",
      "Epoch [1179/1200], Training Loss: 0.6199, vgg Loss: 56.7691, L2 Loss: 0.0522, time: 7.4555\n",
      "Epoch [1180/1200], Training Loss: 0.5685, vgg Loss: 52.0384, L2 Loss: 0.0481, time: 7.4278\n",
      "Epoch [1181/1200], Training Loss: 0.5253, vgg Loss: 48.7083, L2 Loss: 0.0382, time: 7.4793\n",
      "Epoch [1182/1200], Training Loss: 0.5166, vgg Loss: 47.8962, L2 Loss: 0.0376, time: 7.2674\n",
      "Epoch [1183/1200], Training Loss: 0.5831, vgg Loss: 53.8405, L2 Loss: 0.0447, time: 7.3822\n",
      "Epoch [1184/1200], Training Loss: 0.5344, vgg Loss: 48.1840, L2 Loss: 0.0526, time: 7.2754\n",
      "Epoch [1185/1200], Training Loss: 0.5090, vgg Loss: 46.3334, L2 Loss: 0.0457, time: 7.3894\n",
      "Epoch [1186/1200], Training Loss: 0.5072, vgg Loss: 46.1641, L2 Loss: 0.0456, time: 7.5702\n",
      "Epoch [1187/1200], Training Loss: 0.5939, vgg Loss: 54.3174, L2 Loss: 0.0507, time: 7.5252\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1188/1200], Training Loss: 0.4900, vgg Loss: 45.0304, L2 Loss: 0.0397, time: 7.5497\n",
      "Epoch [1189/1200], Training Loss: 0.5064, vgg Loss: 46.9913, L2 Loss: 0.0365, time: 7.4805\n",
      "Epoch [1190/1200], Training Loss: 0.4956, vgg Loss: 46.2934, L2 Loss: 0.0327, time: 7.4572\n",
      "Epoch [1191/1200], Training Loss: 0.5047, vgg Loss: 46.7643, L2 Loss: 0.0370, time: 7.3196\n",
      "Epoch [1192/1200], Training Loss: 0.5146, vgg Loss: 47.8016, L2 Loss: 0.0366, time: 7.5417\n",
      "Epoch [1193/1200], Training Loss: 0.5400, vgg Loss: 50.2936, L2 Loss: 0.0371, time: 7.6745\n",
      "Epoch [1194/1200], Training Loss: 0.6089, vgg Loss: 56.1797, L2 Loss: 0.0471, time: 7.4511\n",
      "Epoch [1195/1200], Training Loss: 0.5786, vgg Loss: 53.5635, L2 Loss: 0.0430, time: 7.5288\n",
      "Epoch [1196/1200], Training Loss: 0.5298, vgg Loss: 48.8570, L2 Loss: 0.0412, time: 7.4525\n",
      "Epoch [1197/1200], Training Loss: 0.5222, vgg Loss: 48.7612, L2 Loss: 0.0346, time: 7.5422\n",
      "Epoch [1198/1200], Training Loss: 0.5598, vgg Loss: 52.2411, L2 Loss: 0.0374, time: 7.5322\n",
      "Epoch [1199/1200], Training Loss: 0.5122, vgg Loss: 47.5681, L2 Loss: 0.0366, time: 7.4581\n",
      "Epoch [1200/1200], Training Loss: 0.4957, vgg Loss: 45.9738, L2 Loss: 0.0359, time: 7.5030\n",
      "--- 9357.8873 seconds ---\n"
     ]
    }
   ],
   "source": [
    "train_dataset = DatasetMultiple('rgb train 100/', 'rgb train 100/', '380 train a 100/', '640 train a 100/', True)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                           batch_size=16,\n",
    "                                           shuffle=True,\n",
    "                                           num_workers=4)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "imageFilter = MFFNet().to(device).float()\n",
    "\n",
    "# Initializing VGG16 model for perceptual loss\n",
    "VGG = Vgg16(requires_grad=False)\n",
    "VGG = VGG.to(device)\n",
    "\n",
    "\n",
    "num_epochs = 1200\n",
    "learning_rate = 1e-4\n",
    "\n",
    "criterion_img = nn.MSELoss()\n",
    "criterion_vgg = nn.MSELoss()\n",
    "\n",
    "optimizer = torch.optim.Adam(imageFilter.parameters(), lr=learning_rate)\n",
    "total_step = len(train_loader)\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "previous_time = start_time\n",
    "for epoch in range(num_epochs):\n",
    "    loss_tol = 0\n",
    "    loss_tol_vgg  = 0\n",
    "    loss_tol_l2   = 0\n",
    "    \n",
    "    if epoch == 300:\n",
    "        learning_rate = 1e-4\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = learning_rate\n",
    "        \n",
    "    if epoch == 600:\n",
    "        learning_rate = 1e-5 #previously 1e-6, with 300 as 1e-5\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = learning_rate\n",
    "    \n",
    "    for i, im in enumerate(train_loader):\n",
    "        inputs = im[0].float().to(device)\n",
    "        target = im[1].float().to(device)\n",
    "        \n",
    "        outputs = imageFilter(inputs)\n",
    "        \n",
    "        loss_l2 = criterion_img( outputs, target )\n",
    "        \n",
    "        outputs_n = utils.normalize_ImageNet_stats(outputs)\n",
    "        target_n  = utils.normalize_ImageNet_stats(target)\n",
    "        \n",
    "        feature_o = VGG(outputs_n, 3)\n",
    "        feature_t = VGG(target_n, 3)\n",
    "        VGG_loss = []\n",
    "        for l in range(3+1):\n",
    "            VGG_loss.append( criterion_vgg(feature_o[l], feature_t[l]) )\n",
    "        \n",
    "        loss_vgg = sum(VGG_loss)\n",
    "        loss = loss_l2 + 0.01*loss_vgg\n",
    "    \n",
    "        loss_tol += loss.item()\n",
    "        \n",
    "        loss_tol_vgg  += loss_vgg\n",
    "        loss_tol_l2   += loss_l2\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    print ( 'Epoch [{}/{}], Training Loss: {:.4f}, vgg Loss: {:.4f}, L2 Loss: {:.4f}, time: {:.4f}' .format(epoch+1, num_epochs, loss_tol, loss_tol_vgg, loss_tol_l2, time.time() - previous_time) )\n",
    "    previous_time = time.time()\n",
    "    \n",
    "print(\"--- %0.4f seconds ---\" % (time.time() - start_time)) \n",
    "torch.save(imageFilter.state_dict(), 'MFF-net_all3_a_new_100_1200epochs.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26d92a8f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f3a02c48",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\adams\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\numpy\\_distributor_init.py:30: UserWarning: loaded more than 1 DLL from .libs:\n",
      "c:\\users\\adams\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\numpy\\.libs\\libopenblas.EL2C6PLE4ZYW3ECEVIV3OXXGRN2NRFM2.gfortran-win_amd64.dll\n",
      "c:\\users\\adams\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\numpy\\.libs\\libopenblas.GK7GX5KEQ4F6UYO3P26ULGBQYHGQO7J4.gfortran-win_amd64.dll\n",
      "  warnings.warn(\"loaded more than 1 DLL from .libs:\"\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function, division\n",
    "import os\n",
    "import torchvision\n",
    "import torch\n",
    "from skimage import io, transform\n",
    "import numpy as np\n",
    "from torchvision import transforms, utils\n",
    "import torch.nn as nn\n",
    "import cv2\n",
    "import utils\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "from torchvision.utils import save_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c148bdff",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MFFNet(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MFFNet, self).__init__()\n",
    "        \n",
    "        self.conv1 = ConvLayer(3, 32, kernel_size=9, stride=1)\n",
    "        self.in1 = torch.nn.InstanceNorm2d(32, affine=True)\n",
    "        self.conv2 = ConvLayer(32, 64, kernel_size=3, stride=2)\n",
    "        self.in2 = torch.nn.InstanceNorm2d(64, affine=True)\n",
    "        self.conv3 = ConvLayer(64, 128, kernel_size=3, stride=2)\n",
    "        self.in3 = torch.nn.InstanceNorm2d(128, affine=True)\n",
    "        # Residual layers\n",
    "        self.res1 = ResidualBlock(128)\n",
    "        self.res2 = ResidualBlock(128)\n",
    "        self.res3 = ResidualBlock(128)\n",
    "        self.res4 = ResidualBlock(128)\n",
    "        self.res5 = ResidualBlock(128)\n",
    "        self.res6 = ResidualBlock(128)\n",
    "        self.res7 = ResidualBlock(128)\n",
    "        self.res8 = ResidualBlock(128)\n",
    "        self.res9 = ResidualBlock(128)\n",
    "        self.res10 = ResidualBlock(128)\n",
    "        self.res11 = ResidualBlock(128)\n",
    "        self.res12 = ResidualBlock(128)\n",
    "        self.res13 = ResidualBlock(128)\n",
    "        self.res14 = ResidualBlock(128)\n",
    "        self.res15 = ResidualBlock(128)\n",
    "        self.res16 = ResidualBlock(128)\n",
    "        \n",
    "        self.deconv1 = UpsampleConvLayer(128*2, 64, kernel_size=3, stride=1, upsample=2)\n",
    "        self.in4 = torch.nn.InstanceNorm2d(64, affine=True)\n",
    "        self.deconv2 = UpsampleConvLayer(64*2, 32, kernel_size=3, stride=1, upsample=2)\n",
    "        self.in5 = torch.nn.InstanceNorm2d(32, affine=True)\n",
    "        self.deconv3 = ConvLayer(32*2, 3, kernel_size=9, stride=1)\n",
    "\n",
    "        self.relu = torch.nn.ReLU()\n",
    "    \n",
    "    def forward(self, X):\n",
    "        o1 = self.relu(self.conv1(X))\n",
    "        o2 = self.relu(self.conv2(o1))\n",
    "        o3 = self.relu(self.conv3(o2))\n",
    "\n",
    "        y = self.res1(o3)\n",
    "        y = self.res2(y)\n",
    "        y = self.res3(y)\n",
    "        y = self.res4(y)\n",
    "        y = self.res5(y)\n",
    "        y = self.res6(y)\n",
    "        y = self.res7(y)\n",
    "        y = self.res8(y)\n",
    "        y = self.res9(y)\n",
    "        y = self.res10(y)\n",
    "        y = self.res11(y)\n",
    "        y = self.res12(y)\n",
    "        y = self.res13(y)\n",
    "        y = self.res14(y)\n",
    "        y = self.res15(y)\n",
    "        y = self.res16(y)\n",
    "        \n",
    "        in1 = torch.cat( (y, o3), 1 )\n",
    "        y = self.relu(self.deconv1(in1))\n",
    "        in2 = torch.cat( (y, o2), 1 )\n",
    "        y = self.relu(self.deconv2(in2))\n",
    "        in3 = torch.cat( (y, o1), 1 )\n",
    "        y = self.deconv3(in3)\n",
    "        \n",
    "        return y\n",
    "\n",
    "class ConvLayer(torch.nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride):\n",
    "        super(ConvLayer, self).__init__()\n",
    "        reflection_padding = kernel_size // 2\n",
    "        self.reflection_pad = torch.nn.ReflectionPad2d(reflection_padding)\n",
    "        self.conv2d = torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.reflection_pad(x)\n",
    "        out = self.conv2d(out)\n",
    "        return out\n",
    "\n",
    "class ResidualBlock(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, channels):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.conv1 = ConvLayer(channels, channels, kernel_size=3, stride=1)\n",
    "        self.in1 = torch.nn.InstanceNorm2d(channels, affine=True)\n",
    "        self.conv2 = ConvLayer(channels, channels, kernel_size=3, stride=1)\n",
    "        self.in2 = torch.nn.InstanceNorm2d(channels, affine=True)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        out = self.relu(self.conv1(x))\n",
    "        out = self.conv2(out)\n",
    "        out = out + residual\n",
    "        return out\n",
    "\n",
    "\n",
    "class UpsampleConvLayer(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride, upsample=None):\n",
    "        super(UpsampleConvLayer, self).__init__()\n",
    "        self.upsample = upsample\n",
    "        reflection_padding = kernel_size // 2\n",
    "        self.reflection_pad = torch.nn.ReflectionPad2d(reflection_padding)\n",
    "        self.conv2d = torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x_in = x\n",
    "        if self.upsample:\n",
    "            x_in = torch.nn.functional.interpolate(x_in, mode='nearest', scale_factor=self.upsample)\n",
    "        out = self.reflection_pad(x_in)\n",
    "        out = self.conv2d(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ce0219d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for MFFNet:\n\tUnexpected key(s) in state_dict: \"att1.Wg.0.weight\", \"att1.Wg.0.bias\", \"att1.Wg.1.weight\", \"att1.Wg.1.bias\", \"att1.Wg.1.running_mean\", \"att1.Wg.1.running_var\", \"att1.Wg.1.num_batches_tracked\", \"att1.Wx.0.weight\", \"att1.Wx.0.bias\", \"att1.Wx.1.weight\", \"att1.Wx.1.bias\", \"att1.Wx.1.running_mean\", \"att1.Wx.1.running_var\", \"att1.Wx.1.num_batches_tracked\", \"att1.psi.0.weight\", \"att1.psi.0.bias\", \"att1.psi.1.weight\", \"att1.psi.1.bias\", \"att1.psi.1.running_mean\", \"att1.psi.1.running_var\", \"att1.psi.1.num_batches_tracked\", \"att2.Wg.0.weight\", \"att2.Wg.0.bias\", \"att2.Wg.1.weight\", \"att2.Wg.1.bias\", \"att2.Wg.1.running_mean\", \"att2.Wg.1.running_var\", \"att2.Wg.1.num_batches_tracked\", \"att2.Wx.0.weight\", \"att2.Wx.0.bias\", \"att2.Wx.1.weight\", \"att2.Wx.1.bias\", \"att2.Wx.1.running_mean\", \"att2.Wx.1.running_var\", \"att2.Wx.1.num_batches_tracked\", \"att2.psi.0.weight\", \"att2.psi.0.bias\", \"att2.psi.1.weight\", \"att2.psi.1.bias\", \"att2.psi.1.running_mean\", \"att2.psi.1.running_var\", \"att2.psi.1.num_batches_tracked\", \"att3.Wg.0.weight\", \"att3.Wg.0.bias\", \"att3.Wg.1.weight\", \"att3.Wg.1.bias\", \"att3.Wg.1.running_mean\", \"att3.Wg.1.running_var\", \"att3.Wg.1.num_batches_tracked\", \"att3.Wx.0.weight\", \"att3.Wx.0.bias\", \"att3.Wx.1.weight\", \"att3.Wx.1.bias\", \"att3.Wx.1.running_mean\", \"att3.Wx.1.running_var\", \"att3.Wx.1.num_batches_tracked\", \"att3.psi.0.weight\", \"att3.psi.0.bias\", \"att3.psi.1.weight\", \"att3.psi.1.bias\", \"att3.psi.1.running_mean\", \"att3.psi.1.running_var\", \"att3.psi.1.num_batches_tracked\". ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[1;32mIn [3]\u001b[0m, in \u001b[0;36m<cell line: 5>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(device)\n\u001b[0;32m      4\u001b[0m model_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMFF-net_all3_a_new\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m----> 5\u001b[0m \u001b[43mimageFilter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;132;43;01m%s\u001b[39;49;00m\u001b[38;5;124;43m.ckpt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;241;43m%\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m imageFilter \u001b[38;5;241m=\u001b[39m imageFilter\u001b[38;5;241m.\u001b[39mto(device)\u001b[38;5;241m.\u001b[39mfloat()\n",
      "File \u001b[1;32mc:\\users\\adams\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1497\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[1;34m(self, state_dict, strict)\u001b[0m\n\u001b[0;32m   1492\u001b[0m         error_msgs\u001b[38;5;241m.\u001b[39minsert(\n\u001b[0;32m   1493\u001b[0m             \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m   1494\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(k) \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)))\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m-> 1497\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m   1498\u001b[0m                        \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(error_msgs)))\n\u001b[0;32m   1499\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for MFFNet:\n\tUnexpected key(s) in state_dict: \"att1.Wg.0.weight\", \"att1.Wg.0.bias\", \"att1.Wg.1.weight\", \"att1.Wg.1.bias\", \"att1.Wg.1.running_mean\", \"att1.Wg.1.running_var\", \"att1.Wg.1.num_batches_tracked\", \"att1.Wx.0.weight\", \"att1.Wx.0.bias\", \"att1.Wx.1.weight\", \"att1.Wx.1.bias\", \"att1.Wx.1.running_mean\", \"att1.Wx.1.running_var\", \"att1.Wx.1.num_batches_tracked\", \"att1.psi.0.weight\", \"att1.psi.0.bias\", \"att1.psi.1.weight\", \"att1.psi.1.bias\", \"att1.psi.1.running_mean\", \"att1.psi.1.running_var\", \"att1.psi.1.num_batches_tracked\", \"att2.Wg.0.weight\", \"att2.Wg.0.bias\", \"att2.Wg.1.weight\", \"att2.Wg.1.bias\", \"att2.Wg.1.running_mean\", \"att2.Wg.1.running_var\", \"att2.Wg.1.num_batches_tracked\", \"att2.Wx.0.weight\", \"att2.Wx.0.bias\", \"att2.Wx.1.weight\", \"att2.Wx.1.bias\", \"att2.Wx.1.running_mean\", \"att2.Wx.1.running_var\", \"att2.Wx.1.num_batches_tracked\", \"att2.psi.0.weight\", \"att2.psi.0.bias\", \"att2.psi.1.weight\", \"att2.psi.1.bias\", \"att2.psi.1.running_mean\", \"att2.psi.1.running_var\", \"att2.psi.1.num_batches_tracked\", \"att3.Wg.0.weight\", \"att3.Wg.0.bias\", \"att3.Wg.1.weight\", \"att3.Wg.1.bias\", \"att3.Wg.1.running_mean\", \"att3.Wg.1.running_var\", \"att3.Wg.1.num_batches_tracked\", \"att3.Wx.0.weight\", \"att3.Wx.0.bias\", \"att3.Wx.1.weight\", \"att3.Wx.1.bias\", \"att3.Wx.1.running_mean\", \"att3.Wx.1.running_var\", \"att3.Wx.1.num_batches_tracked\", \"att3.psi.0.weight\", \"att3.psi.0.bias\", \"att3.psi.1.weight\", \"att3.psi.1.bias\", \"att3.psi.1.running_mean\", \"att3.psi.1.running_var\", \"att3.psi.1.num_batches_tracked\". "
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "imageFilter = MFFNet()\n",
    "print(device)\n",
    "model_name = 'MFF-net_all3_a_new'\n",
    "imageFilter.load_state_dict( torch.load('%s.ckpt'%(model_name)) )\n",
    "imageFilter = imageFilter.to(device).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f5f4b3fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "out_root exists\n"
     ]
    }
   ],
   "source": [
    "data_root = 'training_data'\n",
    "guide1_root = 'training_data'\n",
    "guide2_root = '380 train a'\n",
    "guide3_root = '640 train a'\n",
    "out_root = 'new model multiple attenuated no modulation'\n",
    "if not os.path.exists(out_root):\n",
    "    os.mkdir(out_root)\n",
    "else:\n",
    "    print(\"out_root exists\")\n",
    "\n",
    "for seq in range(1,1305):\n",
    "    # input rgb image is obtained from demosaicing the raw (no other manipulation)\n",
    "    # saved in 16-bit TIFF image\n",
    "    file = ('rgb_%s.png' % (seq) )\n",
    "    filename = os.path.join( data_root, file )\n",
    "    inputs = io.imread(filename) / 255\n",
    "    var = random.randint(100,2000)/10000\n",
    "    gauss = utils.generateGaussNoise(inputs, 0, var)\n",
    "    noisy_inputs = utils.validate_im(inputs + gauss)\n",
    "#     inputs = io.imread(filename)\n",
    "#     utils.matplotlib_imshow(inputs)\n",
    "#     plt.imshow(noisy_inputs)\n",
    "#     print(noisy_inputs.shape)\n",
    "#     save_image(torch.tensor(np.transpose(noisy_inputs,(2,0,1))), \"noisy_1337.png\")\n",
    "#     print(inputs.shape)\n",
    "#     inputs = np.transpose(inputs,(2,0,1))\n",
    "#     print(inputs.shape)\n",
    "\n",
    "#     file = ('guide_%s.bmp' % (seq) )\n",
    "#     filename = os.path.join( data_root, file )\n",
    "#     guided = io.imread(filename) / 255\n",
    "\n",
    "    guidedfile = ('rgb_%s.png' %(seq))\n",
    "    guidedfilename = os.path.join(guide1_root, guidedfile)\n",
    "    guide1img = io.imread(guidedfilename)\n",
    "\n",
    "    guide1 = (guide1img[:,:,0])\n",
    "    \n",
    "    guidedfile = ('training_%s_380_a.png' %(seq))\n",
    "    guidedfilename = os.path.join(guide2_root, guidedfile)\n",
    "    guide2img = io.imread(guidedfilename)\n",
    "\n",
    "    guide2 = guide2img[:,:,0]/2 + guide2img[:,:,1]/2\n",
    "    \n",
    "    \n",
    "    guidedfile = ('training_%s_640_a.png' %(seq))\n",
    "    guidedfilename = os.path.join(guide3_root, guidedfile)\n",
    "    guide3img = io.imread(guidedfilename)\n",
    "\n",
    "    guide3 = guide3img[:,:,0]/2 + guide3img[:,:,2]/2\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    #guided = utils.modulate(guided)\n",
    "#     plt.imshow(guided, cmap=\"gray\")\n",
    "#     plt.imshow(guided)\n",
    "#     save_image(torch.tensor(np.transpose(guided,(2,0,1))), \"rgb_1337.png\")\n",
    "    \n",
    "#     guided_3_channel = np.zeros_like(inputs)\n",
    "#     guided_3_channel[:,:,0] = guided\n",
    "#     guided_3_channel[:,:,1] = guided\n",
    "#     guided_3_channel[:,:,2] = guided\n",
    "#     plt.imshow(guided_3_channel)\n",
    "#     save_image(torch.tensor(np.transpose(guided_3_channel,(2,0,1))), \"rgb_1337.png\")\n",
    "#     inputs = (inputs*80)**0.4\n",
    "#     plt.imshow(inputs)\n",
    "    inputs = np.concatenate((guide1[:,:,None], guide2[:,:,None], guide3[:,:,None]), 2)\n",
    "    inputs = np.transpose(inputs,(2,0,1))\n",
    "    inputs = torch.from_numpy(inputs)\n",
    "    inputs = inputs[None,:,:,:].float()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        inputs = inputs.to(device) \n",
    "        outputs = imageFilter(inputs)\n",
    "    outputs[outputs>1] = 1\n",
    "    outputs[outputs<0] = 0\n",
    "\n",
    "#     # the parameter for color balance and brightness should be tuned for different scenes\n",
    "#     outputs[0,0,:,:] = outputs[0,0,:,:]*1.1*1.5\n",
    "#     outputs[0,1,:,:] = outputs[0,1,:,:]*1*1.5\n",
    "#     outputs[0,2,:,:] = outputs[0,2,:,:]*1.5*1.5\n",
    "\n",
    "    save_image(outputs[0,:,:,:], '%s/out_%s.png' % (out_root, seq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6dbd815",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
